% This file was created with Citavi 6.14.0.0

@book{.,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 title = {Deep Learning},
 file = {Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org):Attachments/Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf:application/pdf}
}


@unpublished{.b,
 author = {{van de Geer}, Sara},
 title = {Empirical process theory and applications},
 file = {empirical-processes:Attachments/empirical-processes.pdf:application/pdf}
}


@article{.c,
 title = {Freeman writing papers 2020},
 file = {Freeman writing papers 2020:Attachments/Freeman writing papers 2020.pdf:application/pdf}
}


@misc{.d,
 author = {Sarkar, Purnamrita},
 title = {Uniform law of large numbers- covering number: SDS 384 11: Theoretical statistics},
 file = {lecture14-ps:Attachments/lecture14-ps.pdf:application/pdf}
}


@article{.e,
 title = {r88},
 file = {r88:Attachments/r88.pdf:application/pdf}
}


@article{.f,
 title = {r95 (1)},
 file = {r95 (1):Attachments/r95 (1).pdf:application/pdf}
}


@article{.g,
 title = {r98},
 file = {r98:Attachments/r98.pdf:application/pdf}
}


@book{.h,
 title = {arx}
}


@book{.i,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
 title = {The Elements of Statistical Learning},
 file = {statistical{\_}learning:Attachments/statistical{\_}learning.pdf:application/pdf}
}


@proceedings{AAAI.2018,
 year = {2018},
 title = {AAAI}
}


@proceedings{AAAI.2020,
 year = {2020},
 title = {AAAI}
}


@proceedings{AAAI.2021,
 year = {2021},
 title = {AAAI}
}


@proceedings{AAAI.2023,
 year = {2023},
 title = {AAAI}
}


@article{Abadie.2010,
 abstract = {Journal of the American Statistical Association, Vol.105, No.490, 2010, 493-505},
 author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
 year = {2010},
 title = {Synthetic control methods for comparative case studies: Estimating the effect of California's tobacco control program},
 keywords = {Observational studies;Proposition 99;Tobacco control legislation;treatment effects},
 pages = {493--505},
 volume = {105},
 number = {490},
 journal = {Journal of the American Statistical Association},
 doi = {10.1198/jasa.2009.ap08746},
 file = {Synthetic Control Methods:Attachments/Synthetic Control Methods.pdf:application/pdf}
}


@article{Adler.2015,
 abstract = {N Engl J Med 2015.372:698-701},
 author = {Adler, Nancy E. and Stead, William W.},
 year = {2015},
 title = {Patients in context: {EHR} capture of social and behavioral determinants of health},
 keywords = {Choice Behavior;Consumer Health Information;Health Insurance Exchanges/economics;Health Literacy/economics;Humans;Internet;State Government;United States},
 pages = {695--698},
 volume = {372},
 number = {8},
 journal = {The New England Journal of Medicine},
 file = {nejmp1413945:Attachments/nejmp1413945.pdf:application/pdf}
}


@article{Ahsen.2017,
 author = {Ahsen, M. Eren and Ayvaci, Mehmet and Raghunathan, Srinivasan},
 year = {2019},
 title = {When algorithmic predictions use human-generated data: A bias-aware classification algorithm for breast cancer diagnosis},
 volume = {30},
 number = {1},
 journal = {Informations Systems Research},
 file = {SSRN-id3087467:Attachments/SSRN-id3087467.pdf:application/pdf}
}


@proceedings{AISTATS.2010,
 year = {2010},
 title = {AISTATS}
}


@proceedings{AISTATS.2018,
 year = {2018},
 title = {AISTATS}
}


@proceedings{AISTATS.2019,
 year = {2019},
 title = {AISTATS}
}


@proceedings{AISTATS.2020,
 year = {2020},
 title = {AISTATS}
}


@proceedings{AISTATS.2021,
 year = {2021},
 title = {AISTATS}
}


@proceedings{AISTATS.2022,
 year = {2022},
 title = {AISTATS}
}


@proceedings{AISTATS.2023,
 year = {2023},
 title = {AISTATS}
}


@inproceedings{Alaa.2017,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Bayesian inference of individualized treatment effects using multi-task {G}aussian processes},
 booktitle = {NeurIPS},
 year = {2017},
 file = {NIPS-2017-bayesian-inference-of-individualized-treatment-effects-using-multi-task-gaussian-processes-Paper:Attachments/NIPS-2017-bayesian-inference-of-individualized-treatment-effects-using-multi-task-gaussian-processes-Paper.pdf:application/pdf}
}


@inproceedings{Alaa.2018,
 abstract = {Limits of Estimating Heterogeneous Treatment Effects: Guidelines for Practical Algorithm DesignAhmed Alaa,~Mihaela SchaarEstimating heterogeneous treatm...},
 author = {Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Limits of estimating heterogeneous treatment effects: Guidelines for practical algorithm design},
 booktitle = {ICML},
 year = {2018},
 file = {Alaa, van der Schaar 2018 - Limits of estimating heterogeneous treatment:Attachments/Alaa, van der Schaar 2018 - Limits of estimating heterogeneous treatment.pdf:application/pdf;Ahmed Alaa, Mihaela Schaar 2018 - Limits of Estimating Heterogeneous Treatment:Attachments/Ahmed Alaa, Mihaela Schaar 2018 - Limits of Estimating Heterogeneous Treatment.pdf:application/pdf}
}


@inproceedings{Alaa.2019,
 abstract = {Proceedings of the International Conference on Machine Learning 2019},
 author = {Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Validating causal inference models via influence functions},
 keywords = {boring formatting information;ICML;Machine learning},
 booktitle = {ICML},
 year = {2019},
 file = {alaa19a:Attachments/alaa19a.pdf:application/pdf}
}


@article{Allam.2021,
 abstract = {In digital medicine, patient data typically record health events over time (eg, through electronic health records, wearables, or other sensing technologies) and thus form unique patient trajectories. Patient trajectories are highly predictive of the future course of diseases and therefore facilitate effective care. However, digital medicine often uses only limited patient data, consisting of health events from only a single or small number of time points while ignoring additional information encoded in patient trajectories. To analyze such rich longitudinal data, new artificial intelligence (AI) solutions are needed. In this paper, we provide an overview of the recent efforts to develop trajectory-aware AI solutions and provide suggestions for future directions. Specifically, we examine the implications for developing disease models from patient trajectories along the typical workflow in AI: problem definition, data processing, modeling, evaluation, and interpretation. We conclude with a discussion of how such AI solutions will allow the field to build robust models for personalized risk scoring, subtyping, and disease pathway discovery.},
 author = {Allam, Ahmed and Feuerriegel, Stefan and Rebhan, Michael and Krauthammer, Michael},
 year = {2021},
 title = {Analyzing patient trajectories with artificial intelligence},
 pages = {e29812},
 volume = {23},
 number = {12},
 journal = {Journal of Medical Internet Research},
 doi = {10.2196/29812}
}


@inproceedings{An.2021,
 author = {An, Yang and Gao, Rui},
 title = {Generalization bounds for (wasserstein) robust optimization},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-generalization-bounds-for-wasserstein-robust-optimization-Paper:Attachments/NeurIPS-2021-generalization-bounds-for-wasserstein-robust-optimization-Paper.pdf:application/pdf}
}


@article{Angrist.1990,
 author = {Angrist, Joshua D.},
 year = {1990},
 title = {Lifetime earnings and the vietnam era draft lotter: Evidence from social security administrative records},
 pages = {313--336},
 volume = {80},
 number = {3},
 journal = {The American Economic Review},
 file = {2006669:Attachments/2006669.pdf:application/pdf}
}


@article{Angrist.1991,
 author = {Angrist, Joshua D. and Krueger, Alan B.},
 year = {1991},
 title = {Does compulsory school attendance affect schooling and earnings?},
 pages = {979--1014},
 volume = {106},
 number = {4},
 journal = {The Quarterly Journal of Economics},
 file = {w3572:Attachments/w3572.pdf:application/pdf}
}


@article{Angrist.1996,
 author = {Angrist, Joshua D. and Imbens, Guido W. and Rubin, Donald B.},
 year = {1996},
 title = {Identification of causal effects using instrumental variables},
 pages = {444--455},
 volume = {91},
 number = {434},
 journal = {Journal of the American Statistical Association},
 file = {2291629 (1):Attachments/2291629 (1).pdf:application/pdf}
}


@book{Angrist.2008,
 author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
 year = {2008},
 title = {Most harmless econometrics},
 address = {Princeton},
 publisher = {{Princeton University Press}}
}


@misc{Annadani.21.02.2023,
 abstract = {We introduce a gradient-based approach for the problem of Bayesian optimal experimental design to learn causal models in a batch setting -- a critical component for causal discovery from finite data where interventions can be costly or risky. Existing methods rely on greedy approximations to construct a batch of experiments while using black-box methods to optimize over a single target-state pair to intervene with. In this work, we completely dispose of the black-box optimization techniques and greedy heuristics and instead propose a conceptually simple end-to-end gradient-based optimization procedure to acquire a set of optimal intervention target-state pairs. Such a procedure enables parameterization of the design space to efficiently optimize over a batch of multi-target-state interventions, a setting which has hitherto not been explored due to its complexity. We demonstrate that our proposed method outperforms baselines and existing acquisition strategies in both single-target and multi-target settings across a number of synthetic datasets.},
 author = {Annadani, Yashas and Tigas, Panagiotis and Ivanova, Desi R. and Jesson, Andrew and Gal, Yarin and Foster, Adam and Bauer, Stefan},
 date = {21.02.2023},
 title = {Differentiable Multi-Target Causal Bayesian Experimental Design},
 url = {http://arxiv.org/pdf/2302.10607v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;ICML;Machine learning;Statistics - Methodology},
 file = {2302.10607:Attachments/2302.10607.pdf:application/pdf}
}


@article{Arnsperger.1994,
 author = {Arnsperger, Christian},
 year = {1994},
 title = {Envy-freeness and distributive justice},
 pages = {155--186},
 volume = {8},
 number = {2},
 journal = {Journal of Economic Surveys},
 file = {j.1467-6419.1994.tb00098.x:Attachments/j.1467-6419.1994.tb00098.x.pdf:application/pdf}
}


@misc{Assaad.09.07.2021,
 abstract = {We examine interval estimation of the effect of a treatment T on an outcome Y given the existence of an unobserved confounder U. Using H\{\textquotedbl}older's inequality, we derive a set of bounds on the confounding bias |E[Y|T=t]-E[Y|do(T=t)]| based on the degree of unmeasured confounding (i.e., the strength of the connection U-{\textgreater}T, and the strength of U-{\textgreater}Y). These bounds are tight either when U is independent of T or when U is independent of Y given T (when there is no unobserved confounding). We focus on a special case of this bound depending on the total variation distance between the distributions p(U) and p(U|T=t), as well as the maximum (over all possible values of U) deviation of the conditional expected outcome E[Y|U=u,T=t] from the average expected outcome E[Y|T=t]. We discuss possible calibration strategies for this bound to get interval estimates for treatment effects, and experimentally validate the bound using synthetic and semi-synthetic datasets.},
 author = {Assaad, Serge and Zeng, Shuxi and Pfister, Henry and Li, Fan and Carin, Lawrence},
 date = {09.07.2021},
 title = {H{\"o}lder bounds for sensitivity analysis in causal reasoning},
 url = {http://arxiv.org/pdf/2107.04661v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;ICML;Machine learning;Statistics - Machine Learning},
 file = {2107.04661:Attachments/2107.04661.pdf:application/pdf}
}


@misc{Athey.,
 author = {Athey, Susan and Wager, Stefan},
 title = {Heterogeneous Treatment Effects},
 file = {CATE{\_}tutorial{\_}short (2):Attachments/CATE{\_}tutorial{\_}short (2).pdf:application/pdf}
}


@article{Athey.2017,
 author = {Athey, Susan and Imbens, Guido and Pham, Thai and Wager, Stefan},
 year = {2017},
 title = {Estimating average treatment effects: Supplementary analyses and remaining challenges},
 pages = {278--281},
 volume = {107},
 number = {5},
 issn = {0002-8282},
 journal = {American Economic Review},
 doi = {10.1257/aer.p20171042}
}


@misc{Athey.2020,
 abstract = {There has been an increase in interest in experimental evaluations to estimate causal effects, partly because their internal validity tends to be high. At the same time, as part of the big data revolution, large, detailed, and representative, administrative data sets have become more widely available. However, the credibility of estimates of causal effects based on such data sets alone can be low.  In this paper, we develop statistical methods for systematically combining experimental and observational data to obtain credible estimates of the causal effect of a binary treatment on a primary outcome that we only observe in the observational sample. Both the observational and experimental samples contain data about a treatment, observable individual characteristics, and a secondary (often short term) outcome. To estimate the effect of a treatment on the primary outcome while addressing the potential confounding in the observational sample, we propose a method that makes use of estimates of the relationship between the treatment and the secondary outcome from the experimental sample. If assignment to the treatment in the observational sample were unconfounded, we would expect the treatment effects on the secondary outcome in the two samples to be similar. We interpret differences in the estimated causal effects on the secondary outcome between the two samples as evidence of unobserved confounders in the observational sample, and develop control function methods for using those differences to adjust the estimates of the treatment effects on the primary outcome.  We illustrate these ideas by combining data on class size and third grade test scores from the Project STAR experiment with observational data on class size and both third and eighth grade test scores from the New York school system.},
 author = {Athey, Susan and Chetty, Raj and Imbens, Guido},
 date = {2020},
 title = {Combining experimental and observational data to estimate treatment  effects on long term outcomes},
 url = {http://arxiv.org/pdf/2006.09676v1},
 keywords = {Statistics - Methodology},
 file = {2006.09676:Attachments/2006.09676.pdf:application/pdf}
}


@article{Athey.2021,
 abstract = {In many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisfies application-specific constraints, such as budget, fairness, simplicity, or other functional form constraints. For example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. We propose a new approach to this problem motivated by the theory of semiparametrically efficient estimation. Our method can be used to optimize either binary treatments or infinitesimal nudges to continuous treatments, and can leverage observational data where causal effects are identified using a variety of strategies, including selection on observables and instrumental variables. Given a doubly robust estimator of the causal effect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.},
 author = {Athey, Susan and Wager, Stefan},
 year = {2021},
 title = {Policy learning with observational data},
 keywords = {Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 pages = {133--161},
 volume = {89},
 number = {1},
 journal = {Econometrica},
 file = {1702.02896:Attachments/1702.02896.pdf:application/pdf}
}


@inproceedings{Avin.2005,
 author = {Avin, Chen and Shpitser, Ilya and Pearl, Judea},
 title = {Identifiability of path-specific effects},
 booktitle = {IJCAI},
 year = {2005},
 file = {r321-ijcai05:Attachments/r321-ijcai05.pdf:application/pdf}
}


@inproceedings{Balazadeh.2022,
 abstract = {We consider the problem of partial identification, the estimation of bounds on the treatment effects from observational data. Although studied using discrete treatment variables or in specific causal graphs (e.g., instrumental variables), partial identification has been recently explored using tools from deep generative modeling. We propose a new method for partial identification of average treatment effects(ATEs) in general causal graphs using implicit generative models comprising continuous and discrete random variables. Since ATE with continuous treatment is generally non-regular, we leverage the partial derivatives of response functions to define a regular approximation of ATE, a quantity we call uniform average treatment derivative (UATD). We prove that our algorithm converges to tight bounds on ATE in linear structural causal models (SCMs). For nonlinear SCMs, we empirically show that using UATD leads to tighter and more stable bounds than methods that directly optimize the ATE.},
 author = {Balazadeh, Vahid and Syrgkanis, Vasilis and Krishnan, Rahul G.},
 title = {Partial identification of treatment effects with implicit generative models},
 keywords = {Computer Science - Learning},
 booktitle = {NeurIPS},
 year = {2022},
 file = {2210.08139:Attachments/2210.08139.pdf:application/pdf}
}


@inproceedings{Balke.1994,
 author = {Balke, Alexander and Pearl, Judea},
 title = {Counterfactual probabilities: Computational methods, bounds, and applications},
 booktitle = {UAI},
 year = {1994},
 file = {1302.6784:Attachments/1302.6784.pdf:application/pdf}
}


@article{Bang.2005,
 abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
 author = {Bang, Heejung and Robins, James M.},
 year = {2005},
 title = {Doubly robust estimation in missing data and causal inference models},
 pages = {962--973},
 volume = {61},
 number = {4},
 issn = {0006-341X},
 journal = {Biometrics},
 doi = {10.1111/j.1541-0420.2005.00377.x},
 file = {Biometrics - 2005 - Bang - Doubly Robust Estimation in Missing Data and Causal Inference Models:Attachments/Biometrics - 2005 - Bang - Doubly Robust Estimation in Missing Data and Causal Inference Models.pdf:application/pdf}
}


@article{Bareinboim.,
 author = {Bareinboim, Elias},
 title = {r96},
 file = {r96:Attachments/r96.pdf:application/pdf}
}


@incollection{Bareinboim.2015,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {{Elias Bareinboim} and {Andrew Forney} and {Judea Pearl}},
 title = {Bandits with Unobserved Confounders: A Causal Approach},
 booktitle = {NeurIPS},
 year = {2015},
 file = {NIPS-2015-bandits-with-unobserved-confounders-a-causal-approach-Paper:Attachments/NIPS-2015-bandits-with-unobserved-confounders-a-causal-approach-Paper.pdf:application/pdf}
}


@article{Bareinboim.2022,
 author = {Bareinboim, Elias and Correa, Juan D. and Ibeling, Daligur and Icard, Thomas},
 year = {2022},
 title = {On {P}earl's hierarchy and the foundations of causal inference},
 pages = {507--556},
 journal = {Probabilistic and Causal Inference: The Works of Judea Pearl},
 file = {r60:Attachments/r60.pdf:application/pdf}
}


@article{BargagliStoffi.2021,
 author = {Bargagli-Stoffi, Falco J. and de Witte, Kristof and Gnecco, Giorgio},
 year = {2021},
 title = {Heterogeneous causal eﬀects with imperfect compliance: A Bayesian machine learning approach},
 journal = {Annals of Applied Statistics},
 file = {2021-12-10 - dps 2113:Attachments/2021-12-10 - dps 2113.pdf:application/pdf}
}


@article{Barocas.2016,
 author = {Barocas, Solon and Selbst, Andrew D.},
 year = {2016},
 title = {Big data's disparate impact},
 pages = {671--732},
 volume = {104},
 journal = {California Law Review},
 file = {Big Data's Disparate Impact:Attachments/Big Data's Disparate Impact.pdf:application/pdf}
}


@article{Barocas.2017,
 author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
 year = {2017},
 title = {Fairness and machine learning},
 journal = {NeurIPS Tutorial},
 file = {fairmlbook:Attachments/fairmlbook.pdf:application/pdf}
}


@article{Bastani.2021,
 author = {Bastani, Hamsa},
 year = {2021},
 title = {Predicting with proxies: Transfer learning in high dimension},
 pages = {2964--2984},
 volume = {67},
 number = {5},
 issn = {0025-1909},
 journal = {Management Science},
 file = {proxies:Attachments/proxies.pdf:application/pdf}
}


@inproceedings{Bellot.2021,
 abstract = {Proceedings of the International Conference on Machine Learning 2021},
 author = {Bellot, Alexis and {van der Schaar}, Mihaela},
 title = {Policy analysis using synthetic controls in continuous-time},
 keywords = {ICML;Machine learning},
 booktitle = {ICML},
 year = {2021},
 file = {bellot21a:Attachments/bellot21a.pdf:application/pdf}
}


@misc{Bellot.29.05.2022,
 abstract = {We investigate the task of estimating the conditional average causal effect of treatment-dosage pairs from a combination of observational data and assumptions on the causal relationships in the underlying system. This has been a longstanding challenge for fields of study such as epidemiology or economics that require a treatment-dosage pair to make decisions but may not be able to run randomized trials to precisely quantify their effect and heterogeneity across individuals. In this paper, we extend (Shalit et al, 2017) to give new bounds on the counterfactual generalization error in the context of a continuous dosage parameter which relies on a different approach to defining counterfactuals and assignment bias adjustment. This result then guides the definition of new learning objectives that can be used to train representation learning algorithms for which we show empirically new state-of-the-art performance results across several benchmark datasets for this problem, including in comparison to doubly-robust estimation methods.},
 author = {Bellot, Alexis and Dhir, Anish and Prando, Giulia},
 date = {29.05.2022},
 title = {Generalization bounds and algorithms for estimating conditional average  treatment effect of dosage},
 url = {http://arxiv.org/pdf/2205.14692v1},
 keywords = {Computer Science - Learning},
 file = {2205.14692:Attachments/2205.14692.pdf:application/pdf}
}


@inproceedings{Bennett.2019,
 abstract = {Instrumental variable analysis is a powerful tool for estimating causal effects when randomization or full control of confounders is not possible. The application of standard methods such as 2SLS, GMM, and more recent variants are significantly impeded when the causal effects are complex, the instruments are high-dimensional, and/or the treatment is high-dimensional. In this paper, we propose the DeepGMM algorithm to overcome this. Our algorithm is based on a new variational reformulation of GMM with optimal inverse-covariance weighting that allows us to efficiently control very many moment conditions. We further develop practical techniques for optimization and model selection that make it particularly successful in practice. Our algorithm is also computationally tractable and can handle large-scale datasets. Numerical results show our algorithm matches the performance of the best tuned methods in standard settings and continues to work in high-dimensional settings where even recent methods break.},
 author = {Bennett, Andrew and Kallus, Nathan and Schnabel, Tobias},
 title = {Deep generalized method of moments for instrumental variable analysis},
 url = {http://arxiv.org/pdf/1905.12495v2},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2019},
 file = {1905.12495:Attachments/1905.12495.pdf:application/pdf}
}


@inproceedings{Bennett.2019b,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Bennett, Andrew and Kallus, Nathan},
 title = {Policy Evaluation with Latent Confounders via Optimal Balance},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-policy-evaluation-with-latent-confounders-via-optimal-balance-Paper:Attachments/NeurIPS-2019-policy-evaluation-with-latent-confounders-via-optimal-balance-Paper.pdf:application/pdf}
}


@inproceedings{Bennett.2020,
 abstract = {Recent work on policy learning from observational data has highlighted the importance of efficient policy evaluation and has proposed reductions to weighted (cost-sensitive) classification. But, efficient policy evaluation need not yield efficient estimation of policy parameters. We consider the estimation problem given by a weighted surrogate-loss classification reduction of policy learning with any score function, either direct, inverse-propensity weighted, or doubly robust. We show that, under a correct specification assumption, the weighted classification formulation need not be efficient for policy parameters. We draw a contrast to actual (possibly weighted) binary classification, where correct specification implies a parametric model, while for policy learning it only implies a semiparametric model. In light of this, we instead propose an estimation approach based on generalized method of moments, which is efficient for the policy parameters. We propose a particular method based on recent developments on solving moment problems using neural networks and demonstrate the efficiency and regret benefits of this method empirically.},
 author = {Bennett, Andrew and Kallus, Nathan},
 title = {Efficient policy learning from surrogate-loss classification reductions},
 url = {http://arxiv.org/pdf/2002.05153v1},
 keywords = {Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 booktitle = {ICML},
 year = {2020},
 file = {2002.05153:Attachments/2002.05153.pdf:application/pdf}
}


@article{Berrevoets.2021,
 abstract = {Choosing the best treatment-plan for each individual patient requires accurate forecasts of their outcome trajectories as a function of the treatment, over time. While large observational data sets constitute rich sources of information to learn from, they also contain biases as treatments are rarely assigned randomly in practice. To provide accurate and unbiased forecasts, we introduce the Disentangled Counterfactual Recurrent Network (DCRN), a novel sequence-to-sequence architecture that estimates treatment outcomes over time by learning representations of patient histories that are disentangled into three separate latent factors: a treatment factor, influencing only treatment selection; an outcome factor, influencing only the outcome; and a confounding factor, influencing both. With an architecture that is completely inspired by the causal structure of treatment influence over time, we advance forecast accuracy and disease understanding, as our architecture allows for practitioners to infer which patient features influence which part in a patient's trajectory, contrasting other approaches in this domain. We demonstrate that DCRN outperforms current state-of-the-art methods in forecasting treatment responses, on both real and simulated data.},
 author = {Berrevoets, Jeroen and Curth, Alicia and Bica, Ioana and McKinney, Eoin and {van der Schaar}, Mihaela},
 year = {2021},
 title = {Disentangled counterfactual recurrent networks for treatment effect  inference over time},
 keywords = {Computer Science - Learning},
 volume = {arXiv:2112.03811},
 journal = {arXiv preprint},
 file = {2112.03811:Attachments/2112.03811.pdf:application/pdf}
}


@inproceedings{Berrevoets.2023,
 abstract = {Missing data is a systemic problem in practical scenarios that causes noise and bias when estimating treatment effects. This makes treatment effect estimation from data with missingness a particularly tricky endeavour. A key reason for this is that standard assumptions on missingness are rendered insufficient due to the presence of an additional variable, treatment, besides the individual and the outcome. Having a treatment variable introduces additional complexity with respect to why some variables are missing that is not fully explored by previous work. In our work we identify a new missingness mechanism, which we term mixed confounded missingness (MCM), where some missingness determines treatment selection and other missingness is determined by treatment selection. Given MCM, we show that naively imputing all data leads to poor performing treatment effects models, as the act of imputation effectively removes information necessary to provide unbiased estimates. However, no imputation at all also leads to biased estimates, as missingness determined by treatment divides the population in distinct subpopulations, where estimates across these populations will be biased. Our solution is selective imputation, where we use insights from MCM to inform precisely which variables should be imputed and which should not. We empirically demonstrate how various learners benefit from selective imputation compared to other solutions for missing data.},
 author = {Berrevoets, Jeroen and Imrie, Fergus and Kyono, Trent and Jordon, James and {van der Schaar}, Mihaela},
 title = {To Impute or not to Impute? Missing data in treatment effect estimation},
 booktitle = {AISTATS},
 year = {2023},
 file = {Berrevoets, Imrie et al. 04.02.2022 - To Impute or not:Attachments/Berrevoets, Imrie et al. 04.02.2022 - To Impute or not.pdf:application/pdf}
}


@article{Bertsimas.2011,
 abstract = {TeX output 2011.03.04:1223},
 author = {Bertsimas, Dimitris and Farias, Vivek F. and Trichakis, Nikolaos},
 year = {2011},
 title = {The price of fairness},
 pages = {17--31},
 volume = {59},
 number = {1},
 journal = {Operations Research},
 doi = {10.1287/opre.1100.0865},
 file = {The Price of Fairness:Attachments/The Price of Fairness.pdf:application/pdf}
}


@article{Bertsimas.2012,
 author = {Bertsimas, Dimitris and Farias, Vivek F. and Trichakis, Nikolaos},
 year = {2012},
 title = {On the efficiency-fairness trade-off},
 pages = {2234--2250},
 volume = {58},
 number = {12},
 issn = {0025-1909},
 journal = {Management Science},
 file = {mnsc.1120.1549:Attachments/mnsc.1120.1549.pdf:application/pdf}
}


@article{Bertsimas.2013,
 author = {Bertsimas, Dimitris and Farias, Vivek F. and Trichakis, Nikolaos},
 year = {2013},
 title = {Fairness, efficiency, and flexibility in organ allocation for kidney transplantation},
 pages = {73--87},
 volume = {61},
 number = {1},
 journal = {Operations Research},
 doi = {10.1287/opre.1120.1138},
 file = {opre.1120.1138:Attachments/opre.1120.1138.pdf:application/pdf}
}


@inproceedings{Bhattacharya.2020,
 author = {Bhattacharya, Rohit and Malinsky, Daniel and Shpitser, Ilya},
 title = {Causal inference under interference and network uncertainty},
 booktitle = {UAI},
 year = {2020},
 file = {bhattacharya20a:Attachments/bhattacharya20a.pdf:application/pdf}
}


@article{Bhutta.2022,
 author = {Bhutta, Neil and Hizmo, Aurel and Ringo, Daniel},
 year = {2022},
 title = {How much does racial bias affect mortgage lending? {E}vidence from human and algorithmic credit decisions},
 keywords = {automated underwriting;credit score;discrimination;Fair lending;G28;mortgage lendingG21;R30;R51},
 pages = {1--44},
 volume = {2022},
 number = {-067},
 journal = {Finance and Economics Discussion Series},
 doi = {10.17016/FEDS.2022.067},
 file = {SSRN-id4276067:Attachments/SSRN-id4276067.pdf:application/pdf}
}


@inproceedings{Bibaut.2021,
 author = {Bibaut, Aurelien and Dimakopoulou, Maria and Kallus, Nathan and Chambaz, Antoine and {van der Laan}, Mark},
 title = {Post-contextual-bandit-inference},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-post-contextual-bandit-inference-Paper:Attachments/NeurIPS-2021-post-contextual-bandit-inference-Paper.pdf:application/pdf}
}


@inproceedings{Bica.2020,
 abstract = {Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association between patient history and treatment assignments and thus can be reliably used for making counterfactual predictions. On a simulated model of tumour growth, with varying degree of time-dependent confounding, we show how our model achieves lower error in estimating counterfactuals and in choosing the correct treatment and timing of treatment than current state-of-the-art methods.},
 author = {Bica, Ioana and Alaa, Ahmed M. and Jordon, James and {van der Schaar}, Mihaela},
 title = {Estimating counterfactual treatment outcomes over time through adversarially balanced representations},
 booktitle = {ICLR},
 year = {2020}
}


@inproceedings{Bica.2020b,
 abstract = {Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden ConfoundersIoana Bica,~Ahmed Alaa,~Mihaela Van Der Schaa...},
 author = {Bica, Ioana and Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Time series deconfounder: Estimating treatment effects over time in the presence of hidden confounders},
 booktitle = {ICML},
 year = {2020},
 file = {Ioana Bica, Ahmed Alaa et al. 2020 - Time Series Deconfounder:Attachments/Ioana Bica, Ahmed Alaa et al. 2020 - Time Series Deconfounder.pdf:application/pdf}
}


@inproceedings{Bica.2020c,
 abstract = {While much attention has been given to the problem of estimating the effect of discrete interventions from observational data, relatively little work has been done in the setting of continuous-valued interventions, such as treatments associated with a dosage parameter. In this paper, we tackle this problem by building on a modification of the generative adversarial networks (GANs) framework. Our model, SCIGAN, is flexible and capable of simultaneously estimating counterfactual outcomes for several different continuous interventions. The key idea is to use a significantly modified GAN model to learn to generate counterfactual outcomes, which can then be used to learn an inference model, using standard supervised methods, capable of estimating these counterfactuals for a new sample. To address the challenges presented by shifting to continuous interventions, we propose a novel architecture for our discriminator - we build a hierarchical discriminator that leverages the structure of the continuous intervention setting. Moreover, we provide theoretical results to support our use of the GAN framework and of the hierarchical discriminator. In the experiments section, we introduce a new semi-synthetic data simulation for use in the continuous intervention setting and demonstrate improvements over the existing benchmark models.},
 author = {Bica, Ioana and Jordon, James and {van der Schaar}, Mihaela},
 title = {Estimating the effects of continuous-valued interventions using generative adversarial networks},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2020},
 file = {2002.12326:Attachments/2002.12326.pdf:application/pdf}
}


@inproceedings{Bica.2021,
 author = {Bica, Ioana and Jarrett, Daniel and {van der Schaar}, Mihaela},
 title = {Invariant causal imitation learning for generalizable policies},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-invariant-causal-imitation-learning-for-generalizable-policies-Paper:Attachments/NeurIPS-2021-invariant-causal-imitation-learning-for-generalizable-policies-Paper.pdf:application/pdf}
}


@inproceedings{Bica.2022,
 abstract = {Consider the problem of improving the estimation of conditional average treatment effects (CATE) for a target domain of interest by leveraging related information from a source domain with a different feature space. This heterogeneous transfer learning problem for CATE estimation is ubiquitous in areas such as healthcare where we may wish to evaluate the effectiveness of a treatment for a new patient population for which different clinical covariates and limited data are available. In this paper, we address this problem by introducing several building blocks that use representation learning to handle the heterogeneous feature spaces and a flexible multi-task architecture with shared and private layers to transfer information between potential outcome functions across domains. Then, we show how these building blocks can be used to recover transfer learning equivalents of the standard CATE learners. On a new semi-synthetic data simulation benchmark for heterogeneous transfer learning we not only demonstrate performance improvements of our heterogeneous transfer causal effect learners across datasets, but also provide insights into the differences between these learners from a transfer perspective.},
 author = {Bica, Ioana and {van der Schaar}, Mihaela},
 title = {Transfer learning on heterogeneous feature spaces for treatment effects estimation},
 keywords = {Computer Science - Learning;Statistics - Methodology},
 booktitle = {NeurIPS},
 year = {2022},
 file = {2210.06183:Attachments/2210.06183.pdf:application/pdf}
}


@book{Bishop.2006,
 author = {Bishop, Christopher M.},
 year = {2006},
 title = {Pattern recognition and machine learning},
 keywords = {Machine learning;Pattern perception},
 address = {New York},
 publisher = {Springer},
 isbn = {0387310738},
 series = {Information science and statistics},
 file = {Bishop - Pattern Recognition And Machine Learning - Springer  2006:Attachments/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:application/pdf}
}


@article{Blackwell.2014,
 author = {Blackwell, Matthew},
 year = {2014},
 title = {A selection bias approach to sensitivity analysis for causal effects},
 pages = {169--182},
 volume = {22},
 number = {2},
 journal = {Political Analysis},
 doi = {10.1093/pan/mpt006},
 file = {causalsens:Attachments/causalsens.pdf:application/pdf}
}


@article{Bloom.1997,
 author = {Bloom, Howard S. and Orr, Larry L. and Bell, Stephen H. and Cave, George and Doolittle, Fred and Lin, Winston and Bos, Johannes M.},
 year = {1997},
 title = {The benefits and costs of {JTPA} title II-A programs: Key Findings from the National Job Training Partnership Act Study},
 pages = {549--586},
 volume = {32},
 number = {32},
 journal = {Journal of Human Resources},
 file = {146183:Attachments/146183.pdf:application/pdf}
}


@article{Bongers.2021,
 abstract = {Structural causal models (SCMs), also known as (nonparametric) structural equation models (SEMs), are widely used for causal modeling purposes. In particular, acyclic SCMs, also known as recursive SEMs, form a well-studied subclass of SCMs that generalize causal Bayesian networks to allow for latent confounders. In this paper, we investigate SCMs in a more general setting, allowing for the presence of both latent confounders and cycles. We show that in the presence of cycles, many of the convenient properties of acyclic SCMs do not hold in general: they do not always have a solution; they do not always induce unique observational, interventional and counterfactual distributions; a marginalization does not always exist, and if it exists the marginal model does not always respect the latent projection; they do not always satisfy a Markov property; and their graphs are not always consistent with their causal semantics. We prove that for SCMs in general each of these properties does hold under certain solvability conditions. Our work generalizes results for SCMs with cycles that were only known for certain special cases so far. We introduce the class of simple SCMs that extends the class of acyclic SCMs to the cyclic setting, while preserving many of the convenient properties of acyclic SCMs. With this paper we aim to provide the foundations for a general theory of statistical causal modeling with SCMs.},
 author = {Bongers, Stephan and Forr{\'e}, Patrick and Peters, Jonas and Mooij, Joris M.},
 year = {2021},
 title = {Foundations of structural causal models with cycles and latent variables},
 keywords = {62A09;68T30;68T37;causal graph;Computer Science - Artificial Intelligence;Computer Science - Learning;counterfactuals;cycles;interventions;marginalization;Markov properties;solvability;Statistics - Methodology;structural causal models},
 pages = {2885--2915},
 volume = {49},
 number = {5},
 issn = {0090-5364},
 journal = {The Annals of Statistics},
 doi = {10.1214/21-AOS2064},
 file = {1611.06221 (1):Attachments/1611.06221 (1).pdf:application/pdf}
}


@article{Bonvini.2022,
 abstract = {We introduce several methods for assessing sensitivity to unmeasured confounding in marginal structural models; importantly we allow treatments to be discrete or continuous, static or time-varying. We consider three sensitivity models: a propensity-based model, an outcome-based model, and a subset confounding model, in which only a fraction of the population is subject to unmeasured confounding. In each case we develop efficient estimators and confidence intervals for bounds on the causal parameters.},
 author = {Bonvini, Matteo and Kennedy, Edward and Ventura, Valerie and Wasserman, Larry},
 year = {2022},
 title = {Sensitivity analysis for marginal structural models},
 url = {http://arxiv.org/pdf/2210.04681v2},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 volume = {arXiv:2210.04681},
 journal = {arXiv preprint},
 file = {2210.04681:Attachments/2210.04681.pdf:application/pdf}
}


@article{Boruvka.2018,
 abstract = {In mobile health interventions aimed at behavior change and maintenance, treatments are provided in real time to manage current or impending high risk situations or promote healthy behaviors in near real time. Currently there is great scientific interest in developing data analysis approaches to guide the development of mobile interventions. In particular data from mobile health studies might be used to examine effect moderators-individual characteristics, time-varying context or past treatment response that moderate the effect of current treatment on a subsequent response. This paper introduces a formal definition for moderated effects in terms of potential outcomes, a definition that is particularly suited to mobile interventions, where treatment occasions are numerous, individuals are not always available for treatment, and potential moderators might be influenced by past treatment. Methods for estimating moderated effects are developed and compared. The proposed approach is illustrated using BASICS-Mobile, a smartphone-based intervention designed to curb heavy drinking and smoking among college students.},
 author = {Boruvka, Audrey and Almirall, Daniel and Witkiewitz, Katie and Murphy, Susan A.},
 year = {2018},
 title = {Assessing time-varying causal effect moderation in mobile health},
 keywords = {Effect modification;mHealth;Structural nested mean model},
 pages = {1112--1121},
 volume = {113},
 number = {523},
 journal = {Journal of the American Statistical Association},
 file = {Assessing Time-Varying Causal Effect Moderation in Mobile:Attachments/Assessing Time-Varying Causal Effect Moderation in Mobile.pdf:application/pdf;Boruvka, Almirall et al. 2018 - Assessing time-varying causal effect moderation:Attachments/Boruvka, Almirall et al. 2018 - Assessing time-varying causal effect moderation.pdf:application/pdf}
}


@inproceedings{Brouwer.2022,
 abstract = {Referred to as the third rung of the causal inference ladder, counterfactual queries typically ask the {\textquotedbl}What if ?{\textquotedbl} question retrospectively. The standard approach to estimate counterfactuals resides in using a structural equation model that accurately reflects the underlying data generating process. However, such models are seldom available in practice and one usually wishes to infer them from observational data alone. Unfortunately, the correct structural equation model is in general not identifiable from the observed factual distribution. Nevertheless, in this work, we show that under the assumption that the main latent contributors to the treatment responses are categorical, the counterfactuals can be still reliably predicted. Building upon this assumption, we introduce CounterFactual Query Prediction (CFQP), a novel method to infer counterfactuals from continuous observations when the background variables are categorical. We show that our method significantly outperforms previously available deep-learning-based counterfactual methods, both theoretically and empirically on time series and image data. Our code is available at https://github.com/edebrouwer/cfqp.},
 author = {Brouwer, Edward De},
 title = {Deep counterfactual estimation with categorical background variables},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Methodology},
 booktitle = {NeurIPS},
 year = {2022},
 file = {2210.05811:Attachments/2210.05811.pdf:application/pdf}
}


@misc{Buchholz.04.06.2023,
 abstract = {We study the problem of learning causal representations from unknown, latent interventions in a general setting, where the latent distribution is Gaussian but the mixing function is completely general. We prove strong identifiability results given unknown single-node interventions, i.e., without having access to the intervention targets. This generalizes prior works which have focused on weaker classes, such as linear maps or paired counterfactual data. This is also the first instance of causal identifiability from non-paired interventions for deep neural network embeddings. Our proof relies on carefully uncovering the high-dimensional geometric structure present in the data distribution after a non-linear density transformation, which we capture by analyzing quadratic forms of precision matrices of the latent distributions. Finally, we propose a contrastive algorithm to identify the latent variables in practice and evaluate its performance on various tasks.},
 author = {Buchholz, Simon and Rajendran, Goutham and Rosenfeld, Elan and Aragam, Bryon and Sch{\"o}lkopf, Bernhard and Ravikumar, Pradeep},
 date = {04.06.2023},
 title = {Learning Linear Causal Representations from Interventions under General  Nonlinear Mixing},
 url = {http://arxiv.org/pdf/2306.02235v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Methodology;Statistics - Theory},
 file = {2306.02235:Attachments/2306.02235.pdf:application/pdf}
}


@inproceedings{Bui.2022,
 abstract = {It is well-known that deep neural networks (DNNs) are susceptible to adversarial attacks, exposing a severe fragility of deep learning systems. As the result, adversarial training (AT) method, by incorporating adversarial examples during training, represents a natural and effective approach to strengthen the robustness of a DNN-based classifier. However, most AT-based methods, notably PGD-AT and TRADES, typically seek a pointwise adversary that generates the worst-case adversarial example by independently perturbing each data sample, as a way to {\textquotedbl}probe{\textquotedbl} the vulnerability of the classifier. Arguably, there are unexplored benefits in considering such adversarial effects from an entire distribution. To this end, this paper presents a unified framework that connects Wasserstein distributional robustness with current state-of-the-art AT methods. We introduce a new Wasserstein cost function and a new series of risk functions, with which we show that standard AT methods are special cases of their counterparts in our framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distributional robustness AT-based algorithms. Extensive experiments show that our distributional robustness AT algorithms robustify further their standard AT counterparts in various settings.},
 author = {Bui, Tuan Anh and {Le Trung} and Tran, Quan and Zhao, He and Phung, Dinh},
 title = {A unified wasserstein distributional robustness framework for adversarial training},
 url = {http://arxiv.org/pdf/2202.13437v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning},
 booktitle = {ICLR},
 year = {2022},
 file = {NeurIPS-2020-f-gail-learning-f-divergence-for-generative-adversarial-imitation-learning-Paper:Attachments/NeurIPS-2020-f-gail-learning-f-divergence-for-generative-adversarial-imitation-learning-Paper.pdf:application/pdf}
}


@article{Celli.2022,
 abstract = {Journal of Economic Surveys 2022.36:214-234},
 author = {Celli, Viviana},
 year = {2022},
 title = {Causal mediation analysis in economics: Objectives, assumptions, models},
 pages = {214--234},
 volume = {36},
 number = {1},
 journal = {Journal of Economic Surveys},
 doi = {10.1111/joes.12452},
 file = {Journal of Economic Surveys - 2021 - Celli - Causal mediation analysis in economics  Objectives  assumptions  models:Attachments/Journal of Economic Surveys - 2021 - Celli - Causal mediation analysis in economics  Objectives  assumptions  models.pdf:application/pdf}
}


@book{Chakraborty.2013,
 author = {Chakraborty, Bibhas and Moodie, Erica E.M.},
 year = {2013},
 title = {Statistical Methods for Dynamic Treatment Regimes},
 address = {New York, NY},
 publisher = {{Springer New York}},
 isbn = {978-1-4614-7427-2},
 doi = {10.1007/978-1-4614-7428-9},
 file = {Statistical Methods:Attachments/Statistical Methods.pdf:application/pdf}
}


@inproceedings{Chang.2022,
 author = {Chang, Jonathan D. and Wang, Kaiwen and Kallus, Nathan and Sun, Wen},
 title = {Learning bellman complete representations for offline policy evaluation},
 keywords = {Bellman Completeness;Offline RL;Off-Policy Evaluation;Reinforcement Learning;representation learning},
 booktitle = {ICML},
 year = {2022},
 file = {chang22b:Attachments/chang22b.pdf:application/pdf}
}


@article{Chao.2023,
 abstract = {We consider the problem of answering observational, interventional, and counterfactual queries in a causally sufficient setting where only observational data and the causal graph are available. Utilizing the recent developments in diffusion models, we introduce diffusion-based causal models (DCM) to learn causal mechanisms, that generate unique latent encodings to allow for direct sampling under interventions as well as abduction for counterfactuals. We utilize DCM to model structural equations, seeing that diffusion models serve as a natural candidate here since they encode each node to a latent representation, a proxy for the exogenous noise, and offer flexible and accurate modeling to provide reliable causal statements and estimates. Our empirical evaluations demonstrate significant improvements over existing state-of-the-art methods for answering causal queries. Our theoretical results provide a methodology for analyzing the counterfactual error for general encoder/decoder models which could be of independent interest.},
 author = {Chao, Patrick and Bl{\"o}baum, Patrick and Kasiviswanathan, Shiva Prasad},
 year = {2023},
 title = {Interventional and counterfactual inference with diffusion models},
 url = {http://arxiv.org/pdf/2302.00860v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 volume = {arXiv:2302.00860},
 journal = {arXiv preprint},
 file = {2302.00860 (1):Attachments/2302.00860 (1).pdf:application/pdf}
}


@article{Charpentier.2023,
 abstract = {Many problems ask a question that can be formulated as a causal question: {\textquotedbl}what would have happened if...?{\textquotedbl} For example, {\textquotedbl}would the person have had surgery if he or she had been Black?{\textquotedbl} To address this kind of questions, calculating an average treatment effect (ATE) is often uninformative, because one would like to know how much impact a variable (such as skin color) has on a specific individual, characterized by certain covariates. Trying to calculate a conditional ATE (CATE) seems more appropriate. In causal inference, the propensity score approach assumes that the treatment is influenced by x, a collection of covariates. Here, we will have the dual view: doing an intervention, or changing the treatment (even just hypothetically, in a thought experiment, for example by asking what would have happened if a person had been Black) can have an impact on the values of x. We will see here that optimal transport allows us to change certain characteristics that are influenced by the variable we are trying to quantify the effect of. We propose here a mutatis mutandis version of the CATE, which will be done simply in dimension one by saying that the CATE must be computed relative to a level of probability, associated to the proportion of x (a single covariate) in the control population, and by looking for the equivalent quantile in the test population. In higher dimension, it will be necessary to go through transport, and an application will be proposed on the impact of some variables on the probability of having an unnatural birth (the fact that the mother smokes, or that the mother is Black).},
 author = {Charpentier, Arthur and Flachaire, Emmanuel and Gallic, Ewen},
 year = {2023},
 title = {Optimal transport for counterfactual estimation: A method for causal inference},
 url = {http://arxiv.org/pdf/2301.07755v1},
 volume = {arXiv:2301.07755},
 journal = {arXiv preprint},
 file = {2301.07755:Attachments/2301.07755.pdf:application/pdf}
}


@inproceedings{Chen.2000,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Chen, Scott Shaobing and Gopinath, Ramesh A.},
 title = {Gaussianization},
 booktitle = {NeurIPS},
 year = {2000},
 file = {NIPS-2000-gaussianization-Paper:Attachments/NIPS-2000-gaussianization-Paper.pdf:application/pdf}
}


@article{Chen.2023,
 abstract = {We consider identification and inference for the average treatment effect and heterogeneous treatment effect conditional on observable covariates in the presence of unmeasured confounding. Since point identification of average treatment effect and heterogeneous treatment effect is not achievable without strong assumptions, we obtain bounds on both average and heterogeneous treatment effects by leveraging differential effects, a tool that allows for using a second treatment to learn the effect of the first treatment. The differential effect is the effect of using one treatment in lieu of the other, and it could be identified in some observational studies in which treatments are not randomly assigned to units, where differences in outcomes may be due to biased assignments rather than treatment effects. With differential effects, we develop a flexible and easy-to-implement semi-parametric framework to estimate bounds and establish asymptotic properties over the support for conducting statistical inference. We provide conditions under which causal estimands are point identifiable as well in the proposed framework. The proposed method is examined by a simulation study and two case studies using datasets from National Health and Nutrition Examination Survey and Youth Risk Behavior Surveillance System.},
 author = {Chen, Kan and Wang, Bingkai and Small, Dylan S.},
 year = {2023},
 title = {A differential effect approach to partial identification of treatment effects},
 url = {http://arxiv.org/pdf/2303.06332v1},
 keywords = {Statistics - Methodology},
 volume = {arXiv:2303.06332},
 journal = {arXiv preprint},
 file = {2303.06332:Attachments/2303.06332.pdf:application/pdf}
}


@article{Chernozhukov.2013,
 abstract = {Econometrica 2013.81:2205-2268},
 author = {Chernozhukov, Victor and Fern{\'a}ndez-Val, Ivan and Melly, Blaise},
 year = {2013},
 title = {Inference on counterfactual distributions},
 keywords = {Counterfactual distribution;decomposition analysis;distribution regression;duration/transformation regression;exchangeable bootstrap;Hadamard differentiability of the counterfactual operator;policy analysis;quantile regression;unconditional quantile and distribution effects},
 pages = {2205--2268},
 volume = {81},
 number = {6},
 journal = {Econometrica},
 doi = {10.3982/ECTA10582},
 file = {Econometrica - 2013 - Chernozhukov - Inference on Counterfactual Distributions:Attachments/Econometrica - 2013 - Chernozhukov - Inference on Counterfactual Distributions.pdf:application/pdf}
}


@article{Chernozhukov.2018,
 author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James M.},
 year = {2018},
 title = {Double/debiased machine learning for treatment and structural parameters},
 pages = {C1-C68},
 volume = {21},
 number = {1},
 issn = {1368-4221},
 journal = {The Econometrics Journal},
 doi = {10.1111/ectj.12097},
 file = {Double-debiased machine learning for treatment and structural parameters:Attachments/Double-debiased machine learning for treatment and structural parameters.pdf:application/pdf}
}


@article{Chernozhukov.2022,
 abstract = {Many economic and causal parameters depend on nonparametric or high dimensional first steps. We give a general construction of locally robust/orthogonal moment functions for GMM, where moment conditions have zero derivative with respect to first steps. We show that orthogonal moment functions can be constructed by adding to identifying moments the nonparametric influence function for the effect of the first step on identifying moments. Orthogonal moments reduce model selection and regularization bias, as is very important in many applications, especially for machine learning first steps.  We give debiased machine learning estimators of functionals of high dimensional conditional quantiles and of dynamic discrete choice parameters with high dimensional state variables. We show that adding to identifying moments the nonparametric influence function provides a general construction of orthogonal moments, including regularity conditions, and show that the nonparametric influence function is robust to additional unknown functions on which it depends. We give a general approach to estimating the unknown functions in the nonparametric influence function and use it to automatically debias estimators of functionals of high dimensional conditional location learners. We give a variety of new doubly robust moment equations and characterize double robustness. We give general and simple regularity conditions and apply these for asymptotic inference on functionals of high dimensional regression quantiles and dynamic discrete choice parameters with high dimensional state variables.},
 author = {Chernozhukov, Victor and Escanciano, Juan Carlos and Ichimura, Hidehiko and Newey, Whitney K. and Robins, James M.},
 year = {2022},
 title = {Locally robust semiparametric estimation},
 url = {http://arxiv.org/pdf/1608.00033v4},
 keywords = {Mathematics - Statistics;Semiparametric inference;Statistics - Theory;Stochastic Equicontinuity;Two-step estimation},
 pages = {1501--1535},
 volume = {90},
 number = {4},
 journal = {Econometrica},
 file = {1608.00033:Attachments/1608.00033.pdf:application/pdf}
}


@article{Chesson.1976,
 author = {Chesson, Jean},
 year = {1976},
 title = {A non-central multivariate hypergeometric distribution arising from biased sampling with application to selective predation},
 pages = {795--797},
 volume = {13},
 number = {4},
 issn = {0021-9002},
 journal = {Journal of Applied Probability},
 doi = {10.2307/3212535}
}


@proceedings{CHIL.2020,
 year = {2020},
 title = {CHIL}
}


@article{Chipman.2010,
 abstract = {The Annals of Applied Statistics , 2010, Vol.4, No.1, 266-298},
 author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
 year = {2010},
 title = {{BART}: Bayesian additive regression trees},
 keywords = {Bayesian backfitting;boosting;CART;classification;ensemble;MCMC;nonparametric regression;probit model;random basis;regularizatio;sum-of-trees model;variable selection;weak learner},
 pages = {266--298},
 volume = {4},
 number = {1},
 journal = {The Annals of Applied Statistics},
 doi = {10.1214/09-AOAS285},
 file = {09-AOAS285:Attachments/09-AOAS285.pdf:application/pdf}
}


@article{Chouldechova.2020,
 abstract = {The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.},
 author = {Chouldechova, Alexandra and Roth, Aaron},
 year = {2020},
 title = {A snapshot of the frontiers of fairness in machine learning},
 url = {http://arxiv.org/pdf/1810.08810v1},
 keywords = {Computer Science - Computer Science and Game Theory;Computer Science - Data Structures and Algorithms;Computer Science - Learning;Statistics - Machine Learning},
 pages = {82--89},
 volume = {63},
 number = {5},
 journal = {Communications of the ACM},
 file = {1810.08810:Attachments/1810.08810.pdf:application/pdf}
}


@proceedings{CIKM.2021,
 year = {2021},
 title = {CIKM}
}


@article{Cinelli.2020,
 author = {Cinelli, Carlos and Forney, Andrew and Pearl, Judea},
 year = {2020},
 title = {A Crash Course in Good and Bad Controls},
 journal = {SSRN Electronic Journal},
 doi = {10.2139/ssrn.3689437},
 file = {SSRN-id3689437:Attachments/SSRN-id3689437.pdf:application/pdf}
}


@proceedings{CLeaR.2022,
 year = {2022},
 title = {CLeaR}
}


@proceedings{CLeaR.2023,
 year = {2023},
 title = {CLeaR}
}


@article{Cohen.2022,
 abstract = {Management Science 0.0},
 author = {Cohen, Maxime C. and Elmachtoub, Adam N. and Lei, Xiao},
 year = {2022},
 title = {Price Discrimination with Fairness Constraints},
 keywords = {fairness;personalization;price discrimination;social welfare},
 pages = {8536--8552},
 volume = {68},
 number = {12},
 issn = {0025-1909},
 journal = {Management Science},
 doi = {10.1287/mnsc.2022.4317},
 file = {mnsc.2022.4317:Attachments/mnsc.2022.4317.pdf:application/pdf}
}


@misc{Colangelo.07.04.2020,
 abstract = {We propose a nonparametric inference method for causal effects of continuous treatment variables, under unconfoundedness and nonparametric or high-dimensional nuisance parameters. Our double debiased machine learning (DML) estimators for the average dose-response function (or the average structural function) and the partial effects are asymptotically normal with nonparametric convergence rates. The nuisance estimators for the conditional expectation function and the conditional density can be nonparametric or ML methods. Utilizing a kernel-based doubly robust moment function and cross-fitting, we give high-level conditions under which the nuisance estimators do not affect the first-order large sample distribution of the DML estimators. We further provide sufficient low-level conditions for kernel, series, and deep neural networks. We propose a data-driven bandwidth to consistently estimate the optimal bandwidth that minimizes the asymptotic mean squared error. We justify the use of kernel to localize the continuous treatment at a given value by the Gateaux derivative. We implement various ML methods in Monte Carlo simulations and an empirical application on a job training program evaluation.},
 author = {Colangelo, Kyle and Lee, Ying-Ying},
 date = {07.04.2020},
 title = {Double Debiased Machine Learning Nonparametric Inference with Continuous  Treatments},
 url = {http://arxiv.org/pdf/2004.03036v7},
 file = {Econometrics Journal - 2017 - Chernozhukov - Double debiased machine learning for treatment and structural parameters:Attachments/Econometrics Journal - 2017 - Chernozhukov - Double debiased machine learning for treatment and structural parameters.pdf:application/pdf}
}


@misc{ComptrolleroftheCurrency.2010,
 author = {{Comptroller of the Currency}},
 year = {2010},
 title = {Fair lending: comptroller's handbook},
 url = {https://www.occ.treas.gov/publications-and-resources/publications/comptrollers-handbook/files/fair-lending/index-fair-lending.html}
}


@misc{ComsumerFinancialProtectionBureau.2013,
 author = {{Comsumer Financial Protection Bureau}},
 year = {2013},
 title = {{CFPB} and {DOJ} order ally to pay {\$}80 million to consumers harmed by discriminatory auto loan pricing},
 url = {https://www.consumerfinance.gov/about-us/newsroom/cfpb-and-doj-order-ally-to-pay-80-million-to-consumers-harmed-by-discriminatory-auto-loan-pricing/}
}


@inproceedings{CorbettDavies.2017,
 author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
 title = {Algorithmic decision making and the cost of fairness},
 pages = {797--806},
 booktitle = {KDD},
 year = {2017},
 file = {1701.08230:Attachments/1701.08230.pdf:application/pdf}
}


@article{CorbettDavies.2023,
 author = {Corbett-Davies, Sam and Gaebler, Johann D. and Nilforoshan, Hamed and Shroff, Ravi and Goel, Sharad},
 year = {2023},
 title = {The measure and mismeasure of fairness: A critical review of fair machine learning},
 journal = {arXiv preprint},
 file = {fair-ml:Attachments/fair-ml.pdf:application/pdf}
}


@article{Cornfield.1959,
 author = {Cornfield, James and Haenszel, William and Hammond, E. Cuyler and Lilienfeld, Abraham M. and Shimkin, Michael B. and Wynder, Ernst L.},
 year = {1959},
 title = {Smoking and lung cancer: Recent evidence and a discussion of some questions},
 pages = {173--203},
 volume = {22},
 number = {1},
 journal = {Journal of the National Cancer Institute},
 file = {dyp289:Attachments/dyp289.pdf:application/pdf}
}


@inproceedings{Coston.2020,
 abstract = {Algorithms are commonly used to predict outcomes under a particular decision or intervention, such as predicting whether an offender will succeed on parole if placed under minimal supervision. Generally, to learn such counterfactual prediction models from observational data on historical decisions and corresponding outcomes, one must measure all factors that jointly affect the outcomes and the decision taken. Motivated by decision support applications, we study the counterfactual prediction task in the setting where all relevant factors are captured in the historical data, but it is either undesirable or impermissible to use some such factors in the prediction model. We refer to this setting as runtime confounding. We propose a doubly-robust procedure for learning counterfactual prediction models in this setting. Our theoretical analysis and experimental results suggest that our method often outperforms competing approaches. We also present a validation procedure for evaluating the performance of counterfactual prediction methods.},
 author = {Coston, Amanda and Kennedy, Edward H. and Chouldechova, Alexandra},
 title = {Counterfactual predictions under runtime confounding},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 booktitle = {NeurIPS},
 year = {2020},
 file = {2006.16916:Attachments/2006.16916.pdf:application/pdf}
}


@article{Cowgill.2020,
 author = {Cowgill, Bo and Tucker, Catherine},
 year = {2020},
 title = {Algorithmic fairness and economics},
 journal = {Columbia Business School Research Paper},
 file = {SSRN-id3361280:Attachments/SSRN-id3361280.pdf:application/pdf}
}


@inproceedings{Crabbe.2021,
 author = {Crabb{\'e}, Jonathan and Qian, Zhaozhi and Imrie, Fergus and {van der Schaar}, Mihaela},
 title = {Explaining latent representations with a corpus of examples},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-explaining-latent-representations-with-a-corpus-of-examples-Paper:Attachments/NeurIPS-2021-explaining-latent-representations-with-a-corpus-of-examples-Paper.pdf:application/pdf}
}


@inproceedings{Creager.2019,
 abstract = {Proceedings of the International Conference on Machine Learning 2019},
 author = {Creager, Elliot and Madras, David and Jacobsen, Joern-Henrik and Weiss, Marissa A. and Swersky, Kevin and Pitassi, Tohiann and Zemel, Richard},
 title = {Flexibly Fair Representation Learning by Disentanglement},
 keywords = {ICML;Machine learning},
 booktitle = {ICML},
 year = {2019},
 file = {creager19a:Attachments/creager19a.pdf:application/pdf}
}


@article{Cui.2020,
 abstract = {Skepticism about the assumption of no unmeasured confounding, also known as exchangeability, is often warranted in making causal inferences from observational data; because exchangeability hinges on an investigator's ability to accurately measure covariates that capture all potential sources of confounding. In practice, the most one can hope for is that covariate measurements are at best proxies of the true underlying confounding mechanism operating in a given observational study. In this paper, we consider the framework of proximal causal inference introduced by Tchetgen Tchetgen et al. (2020), which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability on the basis of measured covariates fails. We make a number of contributions to proximal inference including (i) an alternative set of conditions for nonparametric proximal identification of the average treatment effect; (ii) general semiparametric theory for proximal estimation of the average treatment effect including efficiency bounds for key semiparametric models of interest; (iii) a characterization of proximal doubly robust and locally efficient estimators of the average treatment effect. Moreover, we provide analogous identification and efficiency results for the average treatment effect on the treated. Our approach is illustrated via simulation studies and a data application on evaluating the effectiveness of right heart catheterization in the intensive care unit of critically ill patients.},
 author = {Cui, Yifan and Pu, Hongming and Shi, Xu and Miao, Wang and Tchetgen, Eric Tchetgen},
 year = {2020},
 title = {Semiparametric proximal causal inference},
 url = {http://arxiv.org/pdf/2011.08411v1},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 journal = {arXiv preprint},
 file = {2011.08411:Attachments/2011.08411.pdf:application/pdf}
}


@article{Cui.2021,
 abstract = {There is a fast-growing literature on estimating optimal treatment regimes based on randomized trials or observational studies under a key identifying condition of no unmeasured confounding. Because confounding by unmeasured factors cannot generally be ruled out with certainty in observational studies or randomized trials subject to noncompliance, we propose a general instrumental variable approach to learning optimal treatment regimes under endogeneity. Specifically, we establish identification of both value function {\$}E[Y{\_}{\mathcal{D}(L)}]{\$} for a given regime {\$}\mathcal{D}{\$} and optimal regimes {\$}\text{argmax}{\_}{\mathcal{D}} E[Y{\_}{\mathcal{D}(L)}]{\$} with the aid of a binary instrumental variable, when no unmeasured confounding fails to hold. We also construct novel multiply robust classification-based estimators. Furthermore, we propose to identify and estimate optimal treatment regimes among those who would comply to the assigned treatment under a standard monotonicity assumption. In this latter case, we establish the somewhat surprising result that complier optimal regimes can be consistently estimated without directly collecting compliance information and therefore without the complier average treatment effect itself being identified. Our approach is illustrated via extensive simulation studies and a data application on the effect of child rearing on labor participation.},
 author = {Cui, Yifan and Tchetgen, Eric Tchetgen},
 year = {2021},
 title = {A semiparametric instrumental variable approach to optimal treatment  regimes under endogeneity},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Methodology;Statistics - Theory},
 pages = {126--137},
 volume = {116},
 number = {553},
 journal = {Journal of the American Statistical Association},
 file = {A semiparametric instrumental variable approach to optimal treatment regimes under endogeneity:Attachments/A semiparametric instrumental variable approach to optimal treatment regimes under endogeneity.pdf:application/pdf}
}


@article{Cui.2022,
 author = {Cui, Yifan and Kosorok, Michael R. and Sverdrup, Erik and Wager, Stefan and Zhu, Ruoqing},
 year = {2022},
 title = {Estimating heterogeneous treatment effects with right-censored data via causal survival forests},
 file = {2001.09887:Attachments/2001.09887.pdf:application/pdf}
}


@article{Curth.2020,
 abstract = {We aim to construct a class of learning algorithms that are of practical value to applied researchers in fields such as biostatistics, epidemiology and econometrics, where the need to learn from incompletely observed information is ubiquitous. We propose a new framework for statistical machine learning of target functions arising as identifiable functionals from statistical models, which we call `IF-learning' due to its reliance on influence functions (IFs). This framework is problem- and model-agnostic and can be used to estimate a broad variety of target parameters of interest in applied statistics: we can consider any target function for which an IF of a population-averaged version exists in analytic form. Throughout, we put particular focus on so-called coarsening at random/doubly robust problems with partially unobserved information. This includes problems such as treatment effect estimation and inference in the presence of missing outcome data. Within this framework, we propose two general learning algorithms that build on the idea of nonparametric plug-in bias removal via IFs: the 'IF-learner' which uses pseudo-outcomes motivated by uncentered IFs for regression in large samples and outputs entire target functions without confidence bands, and the 'Group-IF-learner', which outputs only approximations to a function but can give confidence estimates if sufficient information on coarsening mechanisms is available. We apply both in a simulation study on inferring treatment effects.},
 author = {Curth, Alicia and Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 year = {2020},
 title = {Estimating structural target functions using machine learning and  influence functions},
 url = {http://arxiv.org/pdf/2008.06461v3},
 keywords = {Statistics - Machine Learning;Statistics - Methodology},
 volume = {arXiv:2008.06461},
 journal = {arXiv preprint},
 file = {2008.06461:Attachments/2008.06461.pdf:application/pdf}
}


@inproceedings{Curth.2021,
 abstract = {The need to evaluate treatment effectiveness is ubiquitous in most of empirical science, and interest in flexibly investigating effect heterogeneity is growing rapidly. To do so, a multitude of model-agnostic, nonparametric meta-learners have been proposed in recent years. Such learners decompose the treatment effect estimation problem into separate sub-problems, each solvable using standard supervised learning methods. Choosing between different meta-learners in a data-driven manner is difficult, as it requires access to counterfactual information. Therefore, with the ultimate goal of building better understanding of the conditions under which some learners can be expected to perform better than others a priori, we theoretically analyze four broad meta-learning strategies which rely on plug-in estimation and pseudo-outcome regression. We highlight how this theoretical reasoning can be used to guide principled algorithm design and translate our analyses into practice by considering a variety of neural network architectures as base-learners for the discussed meta-learning strategies. In a simulation study, we showcase the relative strengths of the learners under different data-generating processes.},
 author = {Curth, Alicia and {van der Schaar}, Mihaela},
 title = {Nonparametric estimation of heterogeneous treatment effects: From theory  to learning algorithms},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {AISTATS},
 year = {2021},
 file = {2101.10943:Attachments/2101.10943.pdf:application/pdf}
}


@inproceedings{Curth.2021b,
 author = {Curth, Alicia and Lee, Changhee and {van der Schaar}, Mihaela},
 title = {Surv{ITE}: Learning heterogeneous treatment effects from time-to-event data},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-survite-learning-heterogeneous-treatment-effects-from-time-to-event-data-Paper:Attachments/NeurIPS-2021-survite-learning-heterogeneous-treatment-effects-from-time-to-event-data-Paper.pdf:application/pdf}
}


@inproceedings{Curth.2021c,
 author = {Curth, Alicia and {van der Schaar}, Mihaela},
 title = {On inductive biases for heterogeneous treatment effect estimation},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-on-inductive-biases-for-heterogeneous-treatment-effect-estimation-Paper:Attachments/NeurIPS-2021-on-inductive-biases-for-heterogeneous-treatment-effect-estimation-Paper.pdf:application/pdf}
}


@article{Curth.2021d,
 abstract = {The machine learning toolbox for estimation of heterogeneous treatment effects from observational data is expanding rapidly, yet many of its algorithms have been evaluated only on a very limited set of semi-synthetic benchmark datasets. In this paper, we show that even in arguably the simplest setting -- estimation under ignorability assumptions -- the results of such empirical evaluations can be misleading if (i) the assumptions underlying the data-generating mechanisms in benchmark datasets and (ii) their interplay with baseline algorithms are inadequately discussed. We consider two popular machine learning benchmark datasets for evaluation of heterogeneous treatment effect estimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems with their current use and highlight that the inherent characteristics of the benchmark datasets favor some algorithms over others -- a fact that is rarely acknowledged but of immense relevance for interpretation of empirical results. We close by discussing implications and possible next steps.},
 author = {Curth, Alicia and {van der Schaar}, Mihaela},
 title = {Doing great at estimating {CATE}? On the neglected assumptions in benchmark comparisons of treatment effect estimators},
 url = {http://arxiv.org/pdf/2107.13346v1},
 keywords = {Computer Science - Learning;ICML;Machine learning;Statistics - Methodology},
 journal = {arXiv preprint},
 file = {2107.13346:Attachments/2107.13346.pdf:application/pdf}
}


@misc{Curth.2022,
 abstract = {We study the problem of adaptively identifying patient subpopulations that benefit from a given treatment during a confirmatory clinical trial. This type of adaptive clinical trial, often referred to as adaptive enrichment design, has been thoroughly studied in biostatistics with a focus on a limited number of subgroups (typically two) which make up (sub)populations, and a small number of interim analysis points. In this paper, we aim to relax classical restrictions on such designs and investigate how to incorporate ideas from the recent machine learning literature on adaptive and online experimentation to make trials more flexible and efficient. We find that the unique characteristics of the subpopulation selection problem -- most importantly that (i) one is usually interested in finding subpopulations with any treatment benefit (and not necessarily the single subgroup with largest effect) given a limited budget and that (ii) effectiveness only has to be demonstrated across the subpopulation on average -- give rise to interesting challenges and new desiderata when designing algorithmic solutions. Building on these findings, we propose AdaGGI and AdaGCPI, two meta-algorithms for subpopulation construction, which focus on identifying good subgroups and good composite subpopulations, respectively. We empirically investigate their performance across a range of simulation scenarios and derive insights into their (dis)advantages across different settings.},
 author = {Curth, Alicia and H{\"u}y{\"u}k, Alihan and {van der Schaar}, Mihaela},
 date = {2022},
 title = {Adaptively identifying patient populations with treatment benefit in clinical trials},
 url = {http://arxiv.org/pdf/2208.05844v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 file = {2208.05844:Attachments/2208.05844.pdf:application/pdf}
}


@inproceedings{Dai.2018,
 abstract = {When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov's smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm's sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.},
 author = {Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
 title = {{SBEED}: Convergent Reinforcement Learning with Nonlinear Function  Approximation},
 keywords = {Computer Science - Learning;Markov Decision Processes;Reinforcement Learning},
 booktitle = {ICML},
 year = {2018},
 file = {dai18c:Attachments/dai18c.pdf:application/pdf}
}


@inproceedings{DAmour.2019,
 abstract = {Unobserved confounding is a central barrier to drawing causal inferences from observational data. Several authors have recently proposed that this barrier can be overcome in the case where one attempts to infer the effects of several variables simultaneously. In this paper, we present two simple, analytical counterexamples that challenge the general claims that are central to these approaches. In addition, we show that nonparametric identification is impossible in this setting. We discuss practical implications, and suggest alternatives to the methods that have been proposed so far in this line of work: using proxy variables and shifting focus to sensitivity analysis.},
 author = {D'Amour, Alexander},
 title = {On multi-cause causal inference with unobserved confounding: Counterexamples, impossibility, and alternatives},
 url = {http://arxiv.org/pdf/1902.10286v4},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {AISTATS},
 year = {2019},
 file = {1902.10286:Attachments/1902.10286.pdf:application/pdf}
}


@article{Daniel.2013,
 abstract = {Longitudinal studies, where data are repeatedly collected on subjects over a period, are common in medical research. When estimating the effect of a time-varying treatment or exposure on an outcome of interest measured at a later time, standard methods fail to give consistent estimators in the presence of time-varying confounders if those confounders are themselves affected by the treatment. Robins and colleagues have proposed several alternative methods that, provided certain assumptions hold, avoid the problems associated with standard approaches. They include the g-computation formula, inverse probability weighted estimation of marginal structural models and g-estimation of structural nested models. In this tutorial, we give a description of each of these methods, exploring the links and differences between them and the reasons for choosing one over the others in different settings.},
 author = {Daniel, Rhian. M. and Cousens, Simon N. and de Stavola, Bianca L. and Kenward, Mike G. and Sterne, Jonathan A. C.},
 year = {2013},
 title = {Methods for dealing with time-dependent confounding},
 keywords = {Data Interpretation, Statistical;Humans;Longitudinal Studies;Models, Statistical},
 pages = {1584--1618},
 volume = {32},
 number = {9},
 journal = {Statistics in Medicine},
 doi = {10.1002/sim.5686},
 file = {daniel2012:Attachments/daniel2012.pdf:application/pdf}
}


@inproceedings{Daskalakis.2018,
 abstract = {We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.},
 author = {Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
 title = {Training {GAN}s with optimism},
 url = {http://arxiv.org/pdf/1711.00141v2},
 keywords = {Computer Science - Computer Science and Game Theory;Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {ICLR},
 year = {2018},
 file = {1711.00141:Attachments/1711.00141.pdf:application/pdf}
}


@misc{Dastin.2018,
 author = {Dastin, Jeffrey},
 year = {2018},
 title = {Amazon scraps secret {AI} recruiting tool that showed bias against women},
 url = {https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}
}


@article{DeArteaga.2022,
 abstract = {The extensive adoption of business analytics (BA) has brought financial gains and increased efficiencies. However, these advances have simultaneously drawn attention to rising legal and ethical challenges when BA inform decisions with fairness implications. As a response to these concerns, the emerging study of algorithmic fairness deals with algorithmic outputs that may result in disparate outcomes or other forms of injustices for subgroups of the population, especially those who have been historically marginalized. Fairness is relevant on the basis of legal compliance, social responsibility, and utility; if not adequately and systematically addressed, unfair BA systems may lead to societal harms and may also threaten an organization's own survival, its competitiveness, and overall performance. This paper offers a forward-looking, BA-focused review of algorithmic fairness. We first review the state-of-the-art research on sources and measures of bias, as well as bias mitigation algorithms. We then provide a detailed discussion of the utility-fairness relationship, emphasizing that the frequent assumption of a trade-off between these two constructs is often mistaken or short-sighted. Finally, we chart a path forward by identifying opportunities for business scholars to address impactful, open challenges that are key to the effective and responsible deployment of BA.},
 author = {De-Arteaga, Maria and Feuerriegel, Stefan and Saar-Tsechansky, Maytal},
 year = {2022},
 title = {Algorithmic fairness in business analytics: Directions for research and practice},
 keywords = {Computer Science - Artificial Intelligence},
 pages = {3749--3770},
 volume = {31},
 number = {10},
 journal = {Production and Operations Management},
 file = {2207.10991:Attachments/2207.10991.pdf:application/pdf}
}


@inproceedings{Ding.2020,
 author = {Ding, Yi and Toulis, Panos},
 title = {Dynamical systems theory for causal inference with application to synthetic control methods},
 booktitle = {AISTATS},
 year = {2020},
 file = {ding20a:Attachments/ding20a.pdf:application/pdf}
}


@article{Dippel.2019,
 author = {Dippel, Christian and Gold, Robert and Heblich, Stephan and Pinto, Rodrigo},
 year = {2019},
 title = {Mediation analysis in {IV} settings with a single instrument},
 journal = {arXiv preprint},
 file = {Mediation Analysis in IV Settings with a Single Instrument:Attachments/Mediation Analysis in IV Settings with a Single Instrument.pdf:application/pdf}
}


@inproceedings{Dolatabadi.2020,
 author = {Dolatabadi, Haid M. and Erfani, Sarah and Leckie, Christopher},
 title = {Invertible generartive modeling using linear rational splines},
 booktitle = {AISTATS},
 year = {2020},
 file = {dolatabadi20a:Attachments/dolatabadi20a.pdf:application/pdf}
}


@article{Donoho.1983,
 author = {Donoho, David L. and Huber, Peter J.},
 year = {1983},
 title = {The notion of breakdown point},
 pages = {157--184},
 journal = {A Festschrift for Erich L. Lehmann}
}


@article{Dorn.2022,
 abstract = {Inverse propensity weighting (IPW) is a popular method for estimating treatment effects from observational data. However, its correctness relies on the untestable (and frequently implausible) assumption that all confounders have been measured. This paper introduces a robust sensitivity analysis for IPW that estimates the range of treatment effects compatible with a given amount of unobserved confounding. The estimated range converges to the narrowest possible interval (under the given assumptions) that must contain the true treatment effect. Our proposal is a refinement of the influential sensitivity analysis by Zhao, Small, and Bhattacharya (2019), which we show gives bounds that are too wide even asymptotically. This analysis is based on new partial identification results for Tan (2006)'s marginal sensitivity model.},
 author = {Dorn, Jacob and Guo, Kevin},
 year = {2022},
 title = {Sharp sensitivity analysis for inverse propensity weighting via quantile balancing},
 url = {http://arxiv.org/pdf/2102.04543v2},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 journal = {Journal of the American Statistical Association},
 file = {2102.04543:Attachments/2102.04543.pdf:application/pdf}
}


@article{Dorn.2022b,
 abstract = {We consider the problem of constructing bounds on the average treatment effect (ATE) when unmeasured confounders exist but have bounded influence. Specifically, we assume that omitted confounders could not change the odds of treatment for any unit by more than a fixed factor. We derive the sharp partial identification bounds implied by this assumption by leveraging distributionally robust optimization, and we propose estimators of these bounds with several novel robustness properties. The first is double sharpness: our estimators consistently estimate the sharp ATE bounds when one of two nuisance parameters is misspecified and achieve semiparametric efficiency when all nuisance parameters are suitably consistent. The second is double validity: even when most nuisance parameters are misspecified, our estimators still provide valid but possibly conservative bounds for the ATE and our Wald confidence intervals remain valid even when our estimators are not asymptotically normal. As a result, our estimators provide a highly credible method for sensitivity analysis of causal inferences.},
 author = {Dorn, Jacob and Guo, Kevin and Kallus, Nathan},
 year = {2022},
 title = {Doubly-valid/ doubly-sharp sensitivity analysis for causal inference with unmeasured confounding},
 url = {http://arxiv.org/pdf/2112.11449v2},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Statistics - Machine Learning;Statistics - Methodology},
 volume = {arXiv:2112.11449},
 journal = {arXiv preprint},
 file = {2112.11449 (2):Attachments/2112.11449 (2).pdf:application/pdf}
}


@article{Duarte.2023,
 abstract = {When causal quantities cannot be point identified, researchers often pursue partial identification to quantify the range of possible values. However, the peculiarities of applied research conditions can make this analytically intractable. We present a general and automated approach to causal inference in discrete settings. We show causal questions with discrete data reduce to polynomial programming problems, and we present an algorithm to automatically bound causal effects using efficient dual relaxation and spatial branch-and-bound techniques. The user declares an estimand, states assumptions, and provides data (however incomplete or mismeasured). The algorithm then searches over admissible data-generating processes and outputs the most precise possible range consistent with available information -- i.e., sharp bounds -- including a point-identified solution if one exists. Because this search can be computationally intensive, our procedure reports and continually refines non-sharp ranges that are guaranteed to contain the truth at all times, even when the algorithm is not run to completion. Moreover, it offers an additional guarantee we refer to as {\$}$\backslash$epsilon{\$}-sharpness, characterizing the worst-case looseness of the incomplete bounds. Analytically validated simulations show the algorithm accommodates classic obstacles, including confounding, selection, measurement error, noncompliance, and nonresponse.},
 author = {Duarte, Guilherme and Finkelstein, Noam and Knox, Dean and Mummolo, Jonathan and Shpitser, Ilya},
 year = {2023},
 title = {An automated approach to causal inference in discrete settings},
 keywords = {Computer Science - Learning;Statistics - Computation;Statistics - Machine Learning;Statistics - Methodology},
 journal = {Journal of the American Statistical Association},
 file = {2109.13471:Attachments/2109.13471.pdf:application/pdf}
}


@unpublished{Ducchi.2017,
 author = {Duchi, John},
 year = {2017},
 title = {VC-dimension, covering, and packing: Notes for Statistics 300b},
 file = {vc-dimension:Attachments/vc-dimension.pdf:application/pdf}
}


@article{Duchi.2021,
 abstract = {A common goal in statistics and machine learning is to learn models that can perform well against distributional shifts, such as latent heterogeneous subpopulations, unknown covariate shifts, or unmodeled temporal effects. We develop and analyze a distributionally robust stochastic optimization (DRO) framework that learns a model providing good performance against perturbations to the data-generating distribution. We give a convex formulation for the problem, providing several convergence guarantees. We prove finite-sample minimax upper and lower bounds, showing that distributional robustness sometimes comes at a cost in convergence rates. We give limit theorems for the learned parameters, where we fully specify the limiting distribution so that confidence intervals can be computed. On real tasks including generalizing to unknown subpopulations, fine-grained recognition, and providing good tail performance, the distributionally robust approach often exhibits improved performance.},
 author = {Duchi, John and Namkoong, Hongseok},
 year = {2021},
 title = {Learning models with uniform performance via distributionally robust optimization},
 url = {http://arxiv.org/pdf/1810.08750v6},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 volume = {49},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 file = {1810.08750:Attachments/1810.08750.pdf:application/pdf}
}


@inproceedings{Durkan.2019,
 abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
 author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
 title = {Neural spline flows},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2019},
 file = {1906.04032:Attachments/1906.04032.pdf:application/pdf}
}


@article{DurkP.Kingma.,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {{Durk P. Kingma} and {Tim Salimans} and {Rafal Jozefowicz} and {Xi Chen} and {Ilya Sutskever} and {Max Welling}},
 title = {Improved Variational Inference with Inverse Autoregressive Flow},
 file = {NIPS-2016-improved-variational-inference-with-inverse-autoregressive-flow-Paper:Attachments/NIPS-2016-improved-variational-inference-with-inverse-autoregressive-flow-Paper.pdf:application/pdf}
}


@inproceedings{Dwork.2012,
 author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Tohiann and Reingold, Omer and Zemel, Richard},
 title = {Fairness through awareness},
 keywords = {Computational complexity and cryptography;Computer Applications;Computer science;Computer Systems Organization;Computing Methodologies;Data;Design and analysis of algorithms;Information Systems;Mathematics of Computing;Numerical analysis;Software;Theory of Computation},
 booktitle = {ITCS},
 year = {2012},
 file = {2090236.2090255:Attachments/2090236.2090255.pdf:application/pdf}
}


@inproceedings{Ensign.2018,
 author = {Ensign, Danielle and Friedler, Sorelle A. and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
 title = {Runaway feedback loops in predictive modelling},
 booktitle = {FAccT},
 year = {2018},
 file = {ensign18a:Attachments/ensign18a.pdf:application/pdf}
}


@article{Erzurumluoglu.2020,
 abstract = {Smoking is a major heritable and modifiable risk factor for many diseases, including cancer, common respiratory disorders and cardiovascular diseases. Fourteen genetic loci have previously been associated with smoking behaviour-related traits. We tested up to 235,116 single nucleotide variants (SNVs) on the exome-array for association with smoking initiation, cigarettes per day, pack-years, and smoking cessation in a fixed effects meta-analysis of up to 61 studies (up to~346,813 participants). In a subset of 112,811 participants, a further one million SNVs were also genotyped and tested for association with the four smoking behaviour traits. SNV-trait associations with P {\textless} 5 $\times$ 10-8 in either analysis were taken forward for replication in up to 275,596 independent participants from UK Biobank. Lastly, a meta-analysis of the discovery and replication studies was performed. Sixteen SNVs were associated with at least one of the smoking behaviour traits (P {\textless} 5 $\times$ 10-8) in the discovery samples. Ten novel SNVs, including rs12616219 near TMEM182, were followed-up and five of them (rs462779 in REV3L, rs12780116 in CNNM2, rs1190736 in GPR101, rs11539157 in PJA1, and rs12616219 near TMEM182) replicated at a Bonferroni significance threshold (P {\textless} 4.5 $\times$ 10-3) with consistent direction of effect. A further 35 SNVs were associated with smoking behaviour traits in the discovery plus replication meta-analysis (up to 622,409 participants) including a rare SNV, rs150493199, in CCDC141 and two low-frequency SNVs in CEP350 and HDGFRP2. Functional follow-up implied that decreased expression of REV3L may lower the probability of smoking initiation. The novel loci will facilitate understanding the genetic aetiology of smoking behaviour and may lead to the identification of potential drug targets for smoking prevention and/or cessation.},
 author = {Erzurumluoglu, A. Mesut and {et al.}},
 year = {2020},
 title = {Meta-analysis of up to 622,409 individuals identifies 40 novel smoking behaviour associated genetic loci},
 keywords = {Biological Specimen Banks;Databases, Factual;Europe/ethnology;Exome;Female;Genetic Loci;Humans;Male;Polymorphism, Single Nucleotide/genetics;Smoking/genetics;United Kingdom},
 pages = {2392--2409},
 volume = {25},
 number = {10},
 journal = {Molecular psychiatry},
 doi = {10.1038/s41380-018-0313-0},
 file = {s41380-018-0313-0:Attachments/s41380-018-0313-0.pdf:application/pdf}
}


@proceedings{FAccT.2018,
 year = {2018},
 title = {FAccT}
}


@proceedings{FAccT.2021,
 year = {2021},
 title = {FAccT}
}


@article{Fang.2022,
 author = {Fang, Ethan X. and Wang, Zhaoran and Wang, Lan},
 year = {2022},
 title = {Fairness-oriented learning for optimal individualized treatment rules},
 keywords = {Individualized Treatment Regime;Nonconvex Problem;Quantile},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2021.2008402},
 file = {Fairness Oriented Learning for Optimal Individualized Treatment Rules:Attachments/Fairness Oriented Learning for Optimal Individualized Treatment Rules.pdf:application/pdf}
}


@article{Farbmacher.2022,
 abstract = {This paper combines causal mediation analysis with double machine learning to control for observed confounders in a data-driven way under a selection-on-observables assumption in a high-dimensional setting. We consider the average indirect effect of a binary treatment operating through an intermediate variable (or mediator) on the causal path between the treatment and the outcome, as well as the unmediated direct effect. Estimation is based on efficient score functions, which possess a multiple robustness property w.r.t. misspecifications of the outcome, mediator, and treatment models. This property is key for selecting these models by double machine learning, which is combined with data splitting to prevent overfitting in the estimation of the effects of interest. We demonstrate that the direct and indirect effect estimators are asymptotically normal and root-n consistent under specific regularity conditions and investigate the finite sample properties of the suggested methods in a simulation study when considering lasso as machine learner. We also provide an empirical application to the U.S. National Longitudinal Survey of Youth, assessing the indirect effect of health insurance coverage on general health operating via routine checkups as mediator, as well as the direct effect. We find a moderate short term effect of health insurance coverage on general health which is, however, not mediated by routine checkups.},
 author = {Farbmacher, Helmut and Huber, Martin and Laff{\'e}rs, Luk{\'a}{\v{s}} and Langen, Henrika and Spindler, Martin},
 year = {2022},
 title = {Causal mediation analysis with double machine learning},
 pages = {277--300},
 volume = {25},
 number = {2},
 issn = {1368-4221},
 journal = {The Econometrics Journal},
 file = {2002.12710:Attachments/2002.12710.pdf:application/pdf}
}


@article{FernandezLoria.2022,
 abstract = {This study presents a systematic comparison of methods for individual treatment assignment, a general problem that arises in many applications and has received significant attention from economists, computer scientists, and social scientists. We group the various methods proposed in the literature into three general classes of algorithms (or metalearners): learning models to predict outcomes (the O-learner), learning models to predict causal effects (the E-learner), and learning models to predict optimal treatment assignments (the A-learner). We compare the metalearners in terms of (1) their level of generality and (2) the objective function they use to learn models from data; we then discuss the implications that these characteristics have for modeling and decision making. Notably, we demonstrate analytically and empirically that optimizing for the prediction of outcomes or causal effects is not the same as optimizing for treatment assignments, suggesting that in general the A-learner should lead to better treatment assignments than the other metalearners. We demonstrate the practical implications of our findings in the context of choosing, for each user, the best algorithm for playlist generation in order to optimize engagement. This is the first comparison of the three different metalearners on a real-world application at scale (based on more than half a billion individual treatment assignments). In addition to supporting our analytical findings, the results show how large A/B tests can provide substantial value for learning treatment assignment policies, rather than simply choosing the variant that performs best on average.},
 author = {Fern{\'a}ndez-Lor{\'i}a, Carlos and Provost, Foster and Anderton, Jesse and Carterette, Benjamin and Chandar, Praveen},
 year = {2022},
 title = {A comparison of methods for treatment assignment with an application to playlist generation},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 journal = {Informations Systems Research},
 file = {2004.11532:Attachments/2004.11532.pdf:application/pdf}
}


@misc{Feuerriegel.2021,
 author = {Feuerriegel, Stefan},
 title = {How to write scientific papers},
 file = {LMU{\_}Managing your research process{\_}v1:Attachments/LMU{\_}Managing your research process{\_}v1.pdf:application/pdf}
}


@article{Finkelstein.2012,
 abstract = {In 2008, a group of uninsured low-income adults in Oregon was selected by lottery to be given the chance to apply for Medicaid. This lottery provides an opportunity to gauge the effects of expanding access to public health insurance on the health care use, financial strain, and health of low-income adults using a randomized controlled design. In the year after random assignment, the treatment group selected by the lottery was about 25 percentage points more likely to have insurance than the control group that was not selected. We find that in this first year, the treatment group had substantively and statistically significantly higher health care utilization (including primary and preventive care as well as hospitalizations), lower out-of-pocket medical expenditures and medical debt (including fewer bills sent to collection), and better self-reported physical and mental health than the control group.},
 author = {Finkelstein, Amy and Taubman, Sarah and Wright, Bill and Bernstein, Mira and Gruber, Jonathan and Newhouse, Joseph P. and Allen, Heidi and Baicker, Katherine},
 year = {2012},
 title = {The oregon health insurance experiment: Evidence from the first year},
 pages = {1057--1106},
 volume = {127},
 number = {3},
 journal = {The Quarterly Journal of Economics},
 doi = {10.1093/qje/qjs020},
 file = {qje{\_}qjs020:Attachments/qje{\_}qjs020.pdf:application/pdf}
}


@article{Fisher.2021,
 abstract = {Estimators based on influence functions (IFs) have been shown to be effective in many settings, especially when combined with machine learning techniques. By focusing on estimating a specific target of interest (e.g., the average effect of a treatment), rather than on estimating the full underlying data generating distribution, IF-based estimators are often able to achieve asymptotically optimal mean-squared error. Still, many researchers find IF-based estimators to be opaque or overly technical, which makes their use less prevalent and their benefits less available. To help foster understanding and trust in IF-based estimators, we present tangible, visual illustrations of when and how IF-based estimators can outperform standard ``plug-in'' estimators. The figures we show are based on connections between IFs, gradients, linear approximations, and Newton-Raphson.},
 author = {Fisher, Aaron and Kennedy, Edward H.},
 year = {2021},
 title = {Visually communicating and teaching Intuition for influence functions},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 pages = {162--172},
 volume = {75},
 number = {2},
 journal = {The American Statistician},
 file = {1810.03260:Attachments/1810.03260.pdf:application/pdf}
}


@article{Foster.2019,
 abstract = {We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target parameter and one for the nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate settings in which the target parameter belongs to a complex nonparametric class. We provide conditions on the metric entropy of the nuisance and target classes such that oracle rates---rates of the same order as if we knew the nuisance parameter---are achieved. We also derive new rates for specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data.},
 author = {Foster, Dylan J. and Syrgkanis, Vasilis},
 year = {2019},
 title = {Orthogonal statistical learning},
 url = {http://arxiv.org/pdf/1901.09036v3},
 volume = {arXiv:1901.09036},
 journal = {arXiv preprint},
 file = {Foster, Syrgkanis 25.01.2019 - Orthogonal Statistical Learning:Attachments/Foster, Syrgkanis 25.01.2019 - Orthogonal Statistical Learning.pdf:application/pdf}
}


@inproceedings{Frauen.2023,
 abstract = {In medical practice, treatments are selected based on the expected causal effects on patient outcomes. Here, the gold standard for estimating causal effects are randomized controlled trials; however, such trials are costly and sometimes even unethical. Instead, medical practice is increasingly interested in estimating causal effects among patient subgroups from electronic health records, that is, observational data. In this paper, we aim at estimating the average causal effect (ACE) from observational data (patient trajectories) that are collected over time. For this, we propose DeepACE: an end-to-end deep learning model. DeepACE leverages the iterative G-computation formula to adjust for the bias induced by time-varying confounders. Moreover, we develop a novel sequential targeting procedure which ensures that DeepACE has favorable theoretical properties, i.e., is doubly robust and asymptotically efficient. To the best of our knowledge, this is the first work that proposes an end-to-end deep learning model for estimating time-varying ACEs. We compare DeepACE in an extensive number of experiments, confirming that it achieves state-of-the-art performance. We further provide a case study for patients suffering from low back pain to demonstrate that DeepACE generates important and meaningful findings for clinical practice. Our work enables medical practitioners to develop effective treatment recommendations tailored to patient subgroups.},
 author = {Frauen, Dennis and Hatt, Tobias and Melnychuk, Valentyn and Feuerriegel, Stefan},
 title = {Estimating average causal effects from patient trajectories},
 url = {´},
 booktitle = {AAAI},
 year = {2023},
 file = {Frauen, Hatt et al. 02.03.2022 - Estimating average causal effects:Attachments/Frauen, Hatt et al. 02.03.2022 - Estimating average causal effects.pdf:application/pdf}
}


@inproceedings{Frauen.2023b,
 abstract = {Estimating individual treatment effects (ITEs) from observational data is relevant in many fields such as personalized medicine. However, in practice, the treatment assignment is usually confounded by unobserved variables and thus introduces bias. A remedy to remove the bias is the use of instrumental variables (IVs). Such settings are widespread in medicine (e.g., trials where compliance is used as binary IV). In this paper, we propose a novel, multiply robust machine learning framework, called MRIV, for estimating ITEs using binary IVs and thus yield an unbiased ITE estimator. Different from previous work for binary IVs, our framework estimates the ITE directly via a pseudo outcome regression. (1) We provide a theoretical analysis where we show that our framework yields multiply robust convergence rates: our ITE estimator achieves fast convergence even if several nuisance estimators converge slowly. (2) We further show that our framework asymptotically outperforms state-of-the-art plug-in IV methods for ITE estimation. (3) We build upon our theoretical results and propose a tailored deep neural network architecture called MRIV-Net for ITE estimation using binary IVs. Across various computational experiments, we demonstrate empirically that our MRIV-Net achieves state-of-the-art performance. To the best of our knowledge, our MRIV is the first machine learning framework for estimating ITEs in the binary IV setting shown to be multiply robust.},
 author = {Frauen, Dennis and Feuerriegel, Stefan},
 title = {Estimating individual treatment effects under unobserved confounding  using binary instruments},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 booktitle = {ICLR},
 year = {2023},
 file = {2208.08544:Attachments/2208.08544.pdf:application/pdf}
}


@article{Frauen.2023c,
 abstract = {Causal inference from observational data is crucial for many disciplines such as medicine and economics. However, sharp bounds for causal effects under relaxations of the unconfoundedness assumption (causal sensitivity analysis) are subject to ongoing research. So far, works with sharp bounds are restricted to fairly simple settings (e.g., a single binary treatment). In this paper, we propose a unified framework for causal sensitivity analysis under unobserved confounding in various settings. For this, we propose a flexible generalization of the marginal sensitivity model (MSM) and then derive sharp bounds for a large class of causal effects. This includes (conditional) average treatment effects, effects for mediation analysis and path analysis, and distributional effects. Furthermore, our sensitivity model is applicable to discrete, continuous, and time-varying treatments. It allows us to interpret the partial identification problem under unobserved confounding as a distribution shift in the latent confounders while evaluating the causal effect of interest. In the special case of a single binary treatment, our bounds for (conditional) average treatment effects coincide with recent optimality results for causal sensitivity analysis. Finally, we propose a scalable algorithm to estimate our sharp bounds from observational data.},
 author = {Frauen, Dennis and Melnychuk, Valentyn and Feuerriegel, Stefan},
 year = {2023},
 title = {Sharp bounds for generalized causal sensitivity analysis},
 url = {http://arxiv.org/pdf/2305.16988},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning},
 volume = {arxiv::2305.16988},
 journal = {arXiv preprint},
 file = {2305.16988:Attachments/2305.16988.pdf:application/pdf}
}


@article{Friedler.2021,
 abstract = {What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the {\textquotedbl}observed{\textquotedbl} space) and outputs (the {\textquotedbl}decision{\textquotedbl} space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction.  We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations.},
 author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
 year = {2021},
 title = {The (im)possibility of fairness: Different value systems require different mechanisms for fair decision making},
 keywords = {Computer Science - Computers and Society;Statistics - Machine Learning},
 pages = {136--143},
 volume = {64},
 number = {4},
 journal = {Communications of the ACM},
 file = {1609.07236:Attachments/1609.07236.pdf:application/pdf}
}


@article{Frölich.2017,
 abstract = {IZA Discussion Paper No. 8280},
 author = {Fr{\"o}lich, Markus and Huber, Martin},
 year = {2017},
 title = {Direct and indirect treatment effects--causal chains and mediation analysis with instrumental variables},
 keywords = {direct effect;indirect effect;instrument;treatment effects},
 pages = {1645--1666},
 volume = {79},
 number = {5},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 doi = {10.1920/wp.cem.2014.3114},
 file = {Direct and Indirect Treatment Effects - Causal Chains and Mediation Analysis with Instrumental Variables:Attachments/Direct and Indirect Treatment Effects - Causal Chains and Mediation Analysis with Instrumental Variables.pdf:application/pdf}
}


@article{Fu.2020,
 author = {Fu, Runshan and Huang, Yan and Singh, Param Vir},
 year = {2020},
 title = {AI and algorithmic bias: Source, detection, mitigation and implications},
 pages = {39--63},
 journal = {Tutorials in Operiations Research},
 file = {SSRN-id3681517:Attachments/SSRN-id3681517.pdf:application/pdf}
}


@article{Fu.2022,
 author = {Fu, Runshan and Aseri, Manmohan and Singh, Param Vir and Srinivasan, Kannan},
 year = {2022},
 title = {``Un''Fair machine learning algorithms},
 pages = {4173--4195},
 volume = {68},
 number = {6},
 issn = {0025-1909},
 journal = {Management Science},
 file = {fuaserisinghsrinivasan{\_}updated2:Attachments/fuaserisinghsrinivasan{\_}updated2.pdf:application/pdf}
}


@inproceedings{Gal.2016,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Gal, Yarin and Ghahramani, Zoubin},
 title = {A theoretically grounded application of dropout in recurrent neural networks},
 booktitle = {NeurIPS},
 year = {2016},
 file = {NIPS-2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks-Paper:Attachments/NIPS-2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks-Paper.pdf:application/pdf}
}


@inproceedings{Ganin.2015,
 abstract = {Proceedings of the International Conference on Machine Learning 2015},
 author = {Ganin, Yaroslav and Lempitsky, Victor},
 title = {Unsupervised domain adaptation by backpropagation},
 keywords = {Deep Learning;Gradient Reversal;Unsupervised Domain Adaptation},
 booktitle = {ICML},
 year = {2015},
 file = {ganin15:Attachments/ganin15.pdf:application/pdf}
}


@misc{Ghassami.2021,
 abstract = {Proximal causal inference was recently proposed as a framework to identify causal effects from observational data in the presence of hidden confounders for which proxies are available. In this paper, we extend the proximal causal approach to settings where identification of causal effects hinges upon a set of mediators which unfortunately are not directly observed, however proxies of the hidden mediators are measured. Specifically, we establish (i) a new hidden front-door criterion which extends the classical front-door result to allow for hidden mediators for which proxies are available; (ii) We extend causal mediation analysis to identify direct and indirect causal effects under unconfoundedness conditions in a setting where the mediator in view is hidden, but error prone proxies of the latter are available. We view (i) and (ii) as important steps towards the practical application of front-door criteria and mediation analysis as mediators are almost always error prone and thus, the most one can hope for in practice is that our measurements are at best proxies of mediating mechanisms. Finally, we show that identification of certain causal effects remains possible even in settings where challenges in (i) and (ii) might co-exist.},
 author = {Ghassami, AmirEmad and Shpitser, Ilya and Tchetgen, Eric Tchetgen},
 date = {2021},
 title = {Proximal causal inference with hidden mediators: Front-door and related mediation problems},
 url = {http://arxiv.org/pdf/2111.02927v1},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 file = {2111.02927:Attachments/2111.02927.pdf:application/pdf}
}


@inproceedings{Ghassami.2022,
 author = {Ghassami, AmirEmad and Ying, Andrew and Shpitser, Ilya and Tchetgen, Eric J. Tchetgen},
 title = {Minimax kernel machine learning for a class of doubly robust functionals with applications to proximal causal inference},
 booktitle = {AISTATS},
 year = {2022},
 file = {ghassami22a:Attachments/ghassami22a.pdf:application/pdf}
}


@misc{Ghassami.2022b,
 abstract = {We consider the task of identifying and estimating the causal effect of a treatment variable on a long-term outcome variable using data from an observational domain and an experimental domain. The observational domain is subject to unobserved confounding. Furthermore, subjects in the experiment are only followed for a short period of time; hence, long-term effects of treatment are unobserved but short-term effects will be observed. Therefore, data from neither domain alone suffices for causal inference about the effect of the treatment on the long-term outcome, and must be pooled in a principled way, instead. Athey et al. (2020) proposed a method for systematically combining such data for identifying the downstream causal effect in view. Their approach is based on the assumptions of internal and external validity of the experimental data, and an extra novel assumption called latent unconfoundedness. In this paper, we first review their proposed approach, and then we propose three alternative approaches for data fusion for the purpose of identifying and estimating average treatment effect as well as the effect of treatment on the treated. Our first approach is based on assuming equi-confounding bias for the short-term and long-term outcomes. Our second approach is based on a relaxed version of the equi-confounding bias assumption, where we assume the existence of an observed confounder such that the short-term and long-term potential outcome variables have the same partial additive association with that confounder. Our third approach is based on the proximal causal inference framework, in which we assume the existence of an extra variable in the system which is a proxy of the latent confounder of the treatment-outcome relation. We propose influence function-based estimation strategies for each of our data fusion frameworks and study the robustness properties of the proposed estimators.},
 author = {Ghassami, AmirEmad and Yang, Alan and Richardson, David and Shpitser, Ilya and Tchetgen, Eric Tchetgen},
 date = {2022},
 title = {Combining experimental and observational data for identification and estimation of long-term causal effects},
 url = {http://arxiv.org/pdf/2201.10743v3},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Methodology;Statistics - Theory},
 file = {2201.10743:Attachments/2201.10743.pdf:application/pdf}
}


@article{Glass.2013,
 abstract = {Causal inference has a central role in public health; the determination that an association is causal indicates the possibility for intervention. We review and comment on the long-used guidelines for interpreting evidence as supporting a causal association and contrast them with the potential outcomes framework that encourages thinking in terms of causes that are interventions. We argue that in public health this framework is more suitable, providing an estimate of an action's consequences rather than the less precise notion of a risk factor's causal effect. A variety of modern statistical methods adopt this approach. When an intervention cannot be specified, causal relations can still exist, but how to intervene to change the outcome will be unclear. In application, the often-complex structure of causal processes needs to be acknowledged and appropriate data collected to study them. These newer approaches need to be brought to bear on the increasingly complex public health challenges of our globalized world.},
 author = {Glass, Thomas A. and Goodman, Steven N. and Hern{\'a}n, Miguel A. and Samet, Jonathan M.},
 year = {2013},
 title = {Causal inference in public health},
 pages = {61--75},
 volume = {34},
 journal = {Annual Review of Public Health}
}


@inproceedings{Greenfeld.2020,
 abstract = {We investigate the use of a non-parametric independence measure, the Hilbert-Schmidt Independence Criterion (HSIC), as a loss-function for learning robust regression and classification models. This loss-function encourages learning models where the distribution of the residuals between the label and the model prediction is statistically independent of the distribution of the instances themselves. This loss-function was first proposed by Mooij et al. (2009) in the context of learning causal graphs. We adapt it to the task of learning for unsupervised covariate shift: learning on a source domain without access to any instances or labels from the unknown target domain, but with the assumption that {\$}p(y|x){\$} (the conditional probability of labels given instances) remains the same in the target domain. We show that the proposed loss is expected to give rise to models that generalize well on a class of target domains characterised by the complexity of their description within a reproducing kernel Hilbert space. Experiments on unsupervised covariate shift tasks demonstrate that models learned with the proposed loss-function outperform models learned with standard loss functions, achieving state-of-the-art results on a challenging cell-microscopy unsupervised covariate shift task.},
 author = {Greenfeld, Daniel and Shalit, Uri},
 title = {Robust learning with the hilbert-schmidt independence criterion},
 url = {http://arxiv.org/pdf/1910.00270v4},
 keywords = {Computer Science - Learning;ICML;Machine learning;Statistics - Machine Learning},
 booktitle = {ICML},
 year = {2020},
 file = {1910.00270 (1):Attachments/1910.00270 (1).pdf:application/pdf}
}


@inproceedings{Gresele.2022,
 author = {Gresele, Luigi and von K{\"u}gelgen, Julius and K{\"u}bler, Jonas M. and Kirschbaum, Elke and Sch{\"o}lkopf, Bernhard and Janzig, Dominik},
 title = {Causal Inference Through the Structural Causal Marginal Problem},
 keywords = {ICML;Machine learning},
 booktitle = {ICML},
 year = {2022},
 file = {gresele22a:Attachments/gresele22a.pdf:application/pdf}
}


@inproceedings{Gretton.2007,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon Hui and Song, Le and Sch{\"o}lkopf, Bernhard and Smola, Alexander J.},
 title = {A kernel statistical test of independence},
 booktitle = {NIPS},
 year = {2007},
 file = {NIPS-2007-a-kernel-statistical-test-of-independence-Paper:Attachments/NIPS-2007-a-kernel-statistical-test-of-independence-Paper.pdf:application/pdf}
}


@article{Gueyffier.1997,
 abstract = {BACKGROUND

Trials of drug therapy for hypertension have shown that such therapy has a clear overall benefit in preventing cardiovascular disease. Although these trials have included slightly more women than men, it is still not clear whether treatment benefit is similar for both sexes.

OBJECTIVE

To quantify the average treatment effect in both sexes and to determine whether available data show significant differences in treatment effect between women and men.

DESIGN

Subgroup meta-analysis of individual patient data according to sex. Analysis was based on seven trials from the INDANA (INdividual Data ANalysis of Antihypertensive intervention trials) database and was adjusted for possible confounders.

PATIENTS

20,802 women and 19,975 men recruited between 1972 and 1990.

INTERVENTIONS

Primarily beta-blockers and thiazide diuretics.

RESULTS

In women, treatment effect was statistically significant for stroke (fatal strokes and all strokes) and for major cardiovascular events. In men, it was statistically significant for all categories of events (total and specific mortality, all coronary events, all strokes, and major cardiovascular events). The odds ratios for any category of event did not differ significantly between men and women. In absolute terms, the benefit in women was seen primarily for strokes; in men, treatment prevented as many coronary events as strokes. Graphical analyses suggest that these results could be completely explained by the difference in untreated risk.

CONCLUSIONS

In terms of relative risk, treatment benefit did not differ between women and men. The absolute risk reduction attributable to treatment seemed to depend on untreated risk. These findings underline the need to predict accurately the untreated cardiovascular risk of an individual person in order to rationalize and individualize antihypertensive treatment.},
 author = {Gueyffier, Francois and Boutitie, Florent and Boissel, Jean P. and Pocock, Stuart and Coope, John and Cutler, Jeffrey and Ekbom, Tord and Fagard, Robert and Friedman, Lawrence and Perry, Mitchell and Prineas, Ronald and Schron, Eleanor},
 year = {1997},
 title = {Effect of antihypertensive drug treatment on cardiovascular outcomes in women and men. A meta-analysis of individual patient data from randomized, controlled trials. The INDANA Investigators},
 pages = {761--767},
 volume = {126},
 number = {10},
 issn = {0003-4819},
 journal = {Annals of Internal Medicine},
 doi = {10.7326/0003-4819-126-10-199705150-00002}
}


@article{Gunsilius.2020,
 abstract = {Partial identification approaches are a flexible and robust alternative to standard point-identification approaches in general instrumental variable models. However, this flexibility comes at the cost of a ``curse of cardinality'': the number of restrictions on the identified set grows exponentially with the number of points in the support of the endogenous treatment. This article proposes a novel path-sampling approach to this challenge. It is designed for partially identifying causal effects of interest in the most complex models with continuous endogenous treatments. A stochastic process representation allows to seamlessly incorporate assumptions on individual behavior into the model. Some potential applications include dose-response estimation in randomized trials with imperfect compliance, the evaluation of social programs, welfare estimation in demand models, and continuous choice models. As a demonstration, the method provides informative nonparametric bounds on household expenditures under the assumption that expenditure is continuous. The mathematical contribution is an approach to approximately solving infinite dimensional linear programs on path spaces via sampling.},
 author = {Gunsilius, Florian},
 year = {2020},
 title = {A path-sampling method to partially identify causal effects in instrumental variable models},
 url = {http://arxiv.org/pdf/1910.09502v2},
 keywords = {Mathematics - Statistics;Statistics - Theory},
 volume = {arXiv:1910.09502},
 journal = {arXiv preprint},
 file = {1910.09502:Attachments/1910.09502.pdf:application/pdf}
}


@inproceedings{Guo.2022,
 abstract = {Causal inference from observational datasets often relies on measuring and adjusting for covariates. In practice, measurements of the covariates can often be noisy and/or biased, or only measurements of their proxies may be available. Directly adjusting for these imperfect measurements of the covariates can lead to biased causal estimates. Moreover, without additional assumptions, the causal effects are not point-identifiable due to the noise in these measurements. To this end, we study the partial identification of causal effects given noisy covariates, under a user-specified assumption on the noise level. The key observation is that we can formulate the identification of the average treatment effects (ATE) as a robust optimization problem. This formulation leads to an efficient robust optimization algorithm that bounds the ATE with noisy covariates. We show that this robust optimization approach can extend a wide range of causal adjustment methods to perform partial identification, including backdoor adjustment, inverse propensity score weighting, double machine learning, and front door adjustment. Across synthetic and real datasets, we find that this approach provides ATE bounds with a higher coverage probability than existing methods.},
 author = {Guo, Wenshuo and Yin, Mingzhang and Wang, Yixin and Jordan, Michael I.},
 title = {Partial identification with noisy covariates: A robust optimization approach},
 keywords = {Computer Science - Learning;Statistics - Methodology},
 booktitle = {CLeaR},
 year = {2022},
 file = {2202.10665:Attachments/2202.10665.pdf:application/pdf}
}


@misc{Guo.25.02.2022,
 abstract = {The intersection of causal inference and machine learning for decision-making is rapidly expanding, but the default decision criterion remains an \textit{average} of individual causal outcomes across a population. In practice, various operational restrictions ensure that a decision-maker's utility is not realized as an \textit{average} but rather as an \textit{output} of a downstream decision-making problem (such as matching, assignment, network flow, minimizing predictive risk). In this work, we develop a new framework for off-policy evaluation with a \textit{policy-dependent} linear optimization response: causal outcomes introduce stochasticity in objective function coefficients. In this framework, a decision-maker's utility depends on the policy-dependent optimization, which introduces a fundamental challenge of \textit{optimization} bias even for the case of policy evaluation. We construct unbiased estimators for the policy-dependent estimand by a perturbation method. We also discuss the asymptotic variance properties for a set of plug-in regression estimators adjusted to be compatible with that perturbation method. Lastly, attaining unbiased policy evaluation allows for policy optimization, and we provide a general algorithm for optimizing causal interventions. We corroborate our theoretical results with numerical simulations.},
 author = {Guo, Wenshuo and Jordan, Michael I. and Zhou, Angela},
 date = {25.02.2022},
 title = {Off-policy evaluation with policy-dependent optimization response},
 url = {http://arxiv.org/pdf/2202.12958v1},
 keywords = {Computer Science - Learning},
 file = {2202.12958:Attachments/2202.12958.pdf:application/pdf}
}


@misc{Gurovich.2022,
 abstract = {We study the problem of performing classification in a manner that is fair for sensitive groups, such as race and gender. This problem is tackled through the lens of disentangled and locally fair representations. We learn a locally fair representation, such that, under the learned representation, the neighborhood of each sample is balanced in terms of the sensitive attribute. For instance, when a decision is made to hire an individual, we ensure that the $K$ most similar hired individuals are racially balanced. Crucially, we ensure that similar individuals are found based on attributes not correlated to their race. To this end, we disentangle the embedding space into two representations. The first of which is correlated with the sensitive attribute while the second is not. We apply our local fairness objective only to the second, uncorrelated, representation. Through a set of experiments, we demonstrate the necessity of both disentangled and local fairness for obtaining fair and accurate representations. We evaluate our method on real-world settings such as predicting income and re-incarceration rate and demonstrate the advantage of our method.},
 author = {Gurovich, Yaron and Benaim, Sagie and Wolf, Lior},
 date = {2022},
 title = {On disentangled and locally fair representations},
 url = {http://arxiv.org/pdf/2205.02673v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning},
 file = {2205.02673:Attachments/2205.02673.pdf:application/pdf}
}


@inproceedings{Gutmann.2010,
 author = {Gutmann, Michael and Hyv{\"a}rinen, Aapo},
 title = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
 booktitle = {AISTATS},
 year = {2010},
 file = {gutmann10a:Attachments/gutmann10a.pdf:application/pdf}
}


@article{Hamburg.2010,
 author = {Hamburg, Margaret A. and Collins, Francis S.},
 year = {2010},
 title = {The path to personalized medicine},
 pages = {301--304},
 volume = {363},
 number = {4},
 journal = {The New England Journal of Medicine},
 doi = {10.1056/NEJMp1006304}
}


@inproceedings{Hardt.2016,
 abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.  In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.  We illustrate our notion using a case study of FICO credit scores.},
 author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
 title = {Equality of opportunity in supervised learning},
 keywords = {Computer Science - Learning},
 booktitle = {NeurIPS},
 year = {2016},
 file = {NIPS-2016-equality-of-opportunity-in-supervised-learning-Paper:Attachments/NIPS-2016-equality-of-opportunity-in-supervised-learning-Paper.pdf:application/pdf}
}


@inproceedings{Hartford.2017,
 abstract = {Deep IV: A Flexible Approach for Counterfactual PredictionJason Hartford,~Greg Lewis,~Kevin Leyton-Brown,~Matt TaddyCounterfactual prediction ...},
 author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
 title = {Deep {IV}: A flexible approach for counterfactual prediction},
 booktitle = {ICML},
 year = {2017},
 file = {Jason Hartford, Greg Lewis et al. 2017 - Deep IV A Flexible Approach:Attachments/Jason Hartford, Greg Lewis et al. 2017 - Deep IV A Flexible Approach.pdf:application/pdf}
}


@inproceedings{Hassanpour.2020,
 author = {Hassanpour, Negar and Greiner, Russell},
 title = {Learning disentangled representations for counterfactual regression},
 booktitle = {ICLR},
 year = {2020},
 file = {learning{\_}disentangled{\_}represen:Attachments/learning{\_}disentangled{\_}represen.pdf:application/pdf}
}


@inproceedings{Hatt.2021,
 abstract = {Decision-making often requires accurate estimation of treatment effects from observational data. This is challenging as outcomes of alternative decisions are not observed and have to be estimated. Previous methods estimate outcomes based on unconfoundedness but neglect any constraints that unconfoundedness imposes on the outcomes. In this paper, we propose a novel regularization framework for estimating average treatment effects that exploits unconfoundedness. To this end, we formalize unconfoundedness as an orthogonality constraint, which ensures that the outcomes are orthogonal to the treatment assignment. This orthogonality constraint is then included in the loss function via a regularization. Based on our regularization framework, we develop deep orthogonal networks for unconfounded treatments (DONUT), which learn outcomes that are orthogonal to the treatment assignment. Using a variety of benchmark datasets for estimating average treatment effects, we demonstrate that DONUT outperforms the state-of-the-art substantially.},
 author = {Hatt, Tobias and Feuerriegel, Stefan},
 title = {Estimating average treatment effects via orthogonal regularization},
 booktitle = {CIKM},
 year = {2021},
 file = {Hatt, Feuerriegel 21 01 2021 - Estimating Average Treatment Effects:Attachments/Hatt, Feuerriegel 21 01 2021 - Estimating Average Treatment Effects.pdf:application/pdf}
}


@article{Hatt.2021b,
 abstract = {Using observational data to estimate the effect of a treatment is a powerful tool for decision-making when randomized experiments are infeasible or costly. However, observational data often yields biased estimates of treatment effects, since treatment assignment can be confounded by unobserved variables. A remedy is offered by deconfounding methods that adjust for such unobserved confounders. In this paper, we develop the Sequential Deconfounder, a method that enables estimating individualized treatment effects over time in presence of unobserved confounders. This is the first deconfounding method that can be used in a general sequential setting (i.e., with one or more treatments assigned at each timestep). The Sequential Deconfounder uses a novel Gaussian process latent variable model to infer substitutes for the unobserved confounders, which are then used in conjunction with an outcome model to estimate treatment effects over time. We prove that using our method yields unbiased estimates of individualized treatment responses over time. Using simulated and real medical data, we demonstrate the efficacy of our method in deconfounding the estimation of treatment responses over time.},
 author = {Hatt, Tobias and Feuerriegel, Stefan},
 year = {2021},
 title = {Sequential deconfounding for causal inference with unobserved  confounders},
 url = {http://arxiv.org/pdf/2104.09323v2},
 journal = {arXiv preprint}
}


@article{Hatt.2022,
 abstract = {Estimating heterogeneous treatment effects is an important problem across many domains. In order to accurately estimate such treatment effects, one typically relies on data from observational studies or randomized experiments. Currently, most existing works rely exclusively on observational data, which is often confounded and, hence, yields biased estimates. While observational data is confounded, randomized data is unconfounded, but its sample size is usually too small to learn heterogeneous treatment effects. In this paper, we propose to estimate heterogeneous treatment effects by combining large amounts of observational data and small amounts of randomized data via representation learning. In particular, we introduce a two-step framework: first, we use observational data to learn a shared structure (in form of a representation); and then, we use randomized data to learn the data-specific structures. We analyze the finite sample properties of our framework and compare them to several natural baselines. As such, we derive conditions for when combining observational and randomized data is beneficial, and for when it is not. Based on this, we introduce a sample-efficient algorithm, called CorNet. We use extensive simulation studies to verify the theoretical properties of CorNet and multiple real-world datasets to demonstrate our method's superiority compared to existing methods.},
 author = {Hatt, Tobias and Berrevoets, Jeroen and Curth, Alicia and Feuerriegel, Stefan and {van der Schaar}, Mihaela},
 year = {2022},
 title = {Combining observational and randomized data for estimating heterogeneous treatment effects},
 url = {http://arxiv.org/pdf/2202.12891v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 journal = {arXiv preprint},
 file = {2202.12891 (1):Attachments/2202.12891 (1).pdf:application/pdf}
}


@inproceedings{Hatt.2022b,
 abstract = {Learning personalized decision policies that generalize to the target population is of great relevance. Since training data is often not representative of the target population, standard policy learning methods may yield policies that do not generalize target population. To address this challenge, we propose a novel framework for learning policies that generalize to the target population. For this, we characterize the difference between the training data and the target population as a sample selection bias using a selection variable. Over an uncertainty set around this selection variable, we optimize the minimax value of a policy to achieve the best worst-case policy value on the target population. In order to solve the minimax problem, we derive an efficient algorithm based on a convex-concave procedure and prove convergence for parametrized spaces of policies such as logistic policies. We prove that, if the uncertainty set is well-specified, our policies generalize to the target population as they can not do worse than on the training data. Using simulated data and a clinical trial, we demonstrate that, compared to standard policy learning methods, our framework improves the generalizability of policies substantially.},
 author = {Hatt, Tobias and Tschernutter, Daniel and Feuerriegel, Stefan},
 title = {Generalizing off-policy learning under sample selection bias},
 url = {http://arxiv.org/pdf/2112.01387v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {UAI},
 year = {2022},
 file = {2112.01387:Attachments/2112.01387.pdf:application/pdf}
}


@article{HeinzeDeml.2018,
 author = {Heinze-Deml, Christina and Peters, Jonas and Meinshausen, Nicolai},
 year = {2018},
 title = {Invariant causal prediction for nonlinear models},
 volume = {6},
 number = {2},
 issn = {2193-3677},
 journal = {Journal of Causal Inference},
 doi = {10.1515/jci-2017-0016},
 file = {10.1515{\_}jci-2017-0016:Attachments/10.1515{\_}jci-2017-0016.pdf:application/pdf}
}


@article{HeinzeDeml.2018b,
 abstract = {Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that not only represent the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and compare their empirical performance under various scenarios.},
 author = {Heinze-Deml, Christina and Maathuis, Marloes H. and Meinshausen, Nicolai},
 year = {2018},
 title = {Causal structure learning},
 url = {http://arxiv.org/pdf/1706.09141v1},
 keywords = {Statistics - Methodology},
 pages = {371--391},
 volume = {5},
 journal = {Annual Review of Statistics and Its Applications},
 file = {1706.09141:Attachments/1706.09141.pdf:application/pdf}
}


@article{Heng.2021,
 author = {Heng, Siyu and Small, Dylan S.},
 year = {2021},
 title = {Sharpening the Rosenbaum sensitivity bounds to adress concerns about interactions between observed and unobserved covariates},
 pages = {2331--2353},
 volume = {31},
 number = {Online special issue},
 journal = {Statistica Sinica},
 file = {becker-caliendo-2007-sensitivity-analysis-for-average-treatment-effects:Attachments/becker-caliendo-2007-sensitivity-analysis-for-average-treatment-effects.pdf:application/pdf}
}


@article{Hitsch.2023,
 author = {Hitsch, G{\"u}nter J. and Misra, Sanjog and Zhang, Walter},
 year = {2023},
 title = {Heterogeneous treatment effects and optimal targeting policy evaluation},
 journal = {SSRN Journal},
 doi = {10.2139/ssrn.3111957},
 file = {SSRN-id3111957:Attachments/SSRN-id3111957.pdf:application/pdf}
}


@article{Hochreiter.1997,
 abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
 author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
 year = {1997},
 title = {Long short-term memory},
 pages = {1735--1780},
 volume = {9},
 number = {8},
 issn = {0899-7667},
 journal = {Neural Computation},
 doi = {10.1162/neco.1997.9.8.1735}
}


@article{Hsu.2013,
 abstract = {Biometrics 2013.69:803-811},
 author = {Hsu, Jesse Y. and Small, Dylan S.},
 year = {2013},
 title = {Calibrating sensitivity analyses to observed covariates in observational studies},
 pages = {803--811},
 volume = {69},
 number = {4},
 issn = {0006-341X},
 journal = {Biometrics},
 file = {Biometrics - 2013 - Hsu - Calibrating Sensitivity Analyses to Observed Covariates in Observational Studies:Attachments/Biometrics - 2013 - Hsu - Calibrating Sensitivity Analyses to Observed Covariates in Observational Studies.pdf:application/pdf}
}


@article{Hu.2022,
 abstract = {We study a nonparametric contextual bandit problem where the expected reward functions belong to a H\{\textquotedbl}older class with smoothness parameter {\$}$\backslash$beta{\$}. We show how this interpolates between two extremes that were previously studied in isolation: non-differentiable bandits ({\$}$\backslash$beta\leq1{\$}), where rate-optimal regret is achieved by running separate non-contextual bandits in different context regions, and parametric-response bandits (satisfying {\$}$\backslash$beta=$\backslash$infty{\$}), where rate-optimal regret can be achieved with minimal or no exploration due to infinite extrapolatability. We develop a novel algorithm that carefully adjusts to all smoothness settings and we prove its regret is rate-optimal by establishing matching upper and lower bounds, recovering the existing results at the two extremes. In this sense, our work bridges the gap between the existing literature on parametric and non-differentiable contextual bandit problems and between bandit algorithms that exclusively use global or local information, shedding light on the crucial interplay of complexity and regret in contextual bandits.},
 author = {Hu, Yichun and Kallus, Nathan and Mao, Xiaojie},
 year = {2022},
 title = {Smooth contextual bandits: Bridging the Parametric and  Non-differentiable Regret Regimes},
 url = {http://arxiv.org/pdf/1909.02553v4},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Mathematics - Optimization and Control;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 journal = {Operations Research},
 file = {1909.02553:Attachments/1909.02553.pdf:application/pdf}
}


@article{Huang.2020,
 author = {Huang, Biwei and Zhang, Kun and Zhang, Jiji and Ramsey, Joseph and Sanchez-Romero, Ruben and Glymour, Clark and Sch{\"o}lkopf, Bernhard},
 year = {2020},
 title = {Causal discovery from heterogeneous/nonstationary data},
 volume = {21},
 journal = {Journal of Machine Learning Research},
 file = {19-232:Attachments/19-232.pdf:application/pdf}
}


@article{Hudgens.2008,
 abstract = {A fundamental assumption usually made in causal inference is that of no interference between individuals (or units); that is, the potential outcomes of one individual are assumed to be unaffected by the treatment assignment of other individuals. However, in many settings, this assumption obviously does not hold. For example, in the dependent happenings of infectious diseases, whether one person becomes infected depends on who else in the population is vaccinated. In this article, we consider a population of groups of individuals where interference is possible between individuals within the same group. We propose estimands for direct, indirect, total, and overall causal effects of treatment strategies in this setting. Relations among the estimands are established; for example, the total causal effect is shown to equal the sum of direct and indirect causal effects. Using an experimental design with a two-stage randomization procedure (first at the group level, then at the individual level within groups), unbiased estimators of the proposed estimands are presented. Variances of the estimators are also developed. The methodology is illustrated in two different settings where interference is likely: assessing causal effects of housing vouchers and of vaccines.},
 author = {Hudgens, Michael G. and Halloran, M. Elizabeth},
 year = {2008},
 title = {Toward causal inference with interference},
 pages = {832--842},
 volume = {103},
 number = {482},
 journal = {Journal of the American Statistical Association},
 doi = {10.1198/016214508000000292},
 file = {nihms-73860:Attachments/nihms-73860.pdf:application/pdf}
}


@proceedings{ICDM.2020,
 year = {2020},
 title = {ICDM},
 doi = {10.1109/ICDM50108.2020}
}


@proceedings{ICLR.2015,
 year = {2015},
 title = {ICLR}
}


@proceedings{ICLR.2018,
 year = {2018},
 title = {ICLR}
}


@proceedings{ICLR.2020,
 year = {2020},
 title = {ICLR}
}


@proceedings{ICLR.2021,
 year = {2021},
 title = {ICLR}
}


@proceedings{ICLR.2022,
 year = {2022},
 title = {ICLR}
}


@proceedings{ICLR.2023,
 year = {2023},
 title = {ICLR}
}


@proceedings{ICML.2013,
 year = {2013},
 title = {ICML}
}


@proceedings{ICML.2015,
 year = {2015},
 title = {ICML}
}


@proceedings{ICML.2016,
 year = {2016},
 title = {ICML}
}


@proceedings{ICML.2017,
 year = {2017},
 title = {ICML}
}


@proceedings{ICML.2018,
 year = {2018},
 title = {ICML}
}


@proceedings{ICML.2019,
 year = {2019},
 title = {ICML}
}


@proceedings{ICML.2020,
 year = {2020},
 title = {ICML}
}


@proceedings{ICML.2021,
 year = {2021},
 title = {ICML}
}


@proceedings{ICML.2022,
 year = {2022},
 title = {ICML}
}


@proceedings{ICML.2023,
 year = {2023},
 title = {ICML}
}


@proceedings{IJCAI.2005,
 year = {2005},
 title = {IJCAI}
}


@article{Imbens.1994,
 author = {Imbens, Guido W. and Angrist, Joshua D.},
 year = {1994},
 title = {Identification and estimation of local average treatment effects},
 pages = {467--475},
 volume = {62},
 number = {2},
 journal = {Econometrica},
 file = {2951620 (1):Attachments/2951620 (1).pdf:application/pdf}
}


@article{Imbens.2003,
 author = {Imbens, Guido W.},
 year = {2003},
 title = {Sensitivity to exogeneity assumptions in program evaluation},
 pages = {128--132},
 volume = {93},
 number = {2},
 issn = {0002-8282},
 journal = {American Economic Review},
 file = {sensitivity{\_}to{\_}exogeneity{\_}assumptions{\_}in{\_}program{\_}evaluation:Attachments/sensitivity{\_}to{\_}exogeneity{\_}assumptions{\_}in{\_}program{\_}evaluation.pdf:application/pdf}
}


@article{Imbens.2022,
 abstract = {We study the identification and estimation of long-term treatment effects when both experimental and observational data are available. Since the long-term outcome is observed only after a long delay, it is not measured in the experimental data, but only recorded in the observational data. However, both types of data include observations of some short-term outcomes. In this paper, we uniquely tackle the challenge of persistent unmeasured confounders, i.e., some unmeasured confounders that can simultaneously affect the treatment, short-term outcomes and the long-term outcome, noting that they invalidate identification strategies in previous literature. To address this challenge, we exploit the sequential structure of multiple short-term outcomes, and develop three novel identification strategies for the average long-term treatment effect. We further propose three corresponding estimators and prove their asymptotic consistency and asymptotic normality. We finally apply our methods to estimate the effect of a job training program on long-term employment using semi-synthetic data. We numerically show that our proposals outperform existing methods that fail to handle persistent confounders.},
 author = {Imbens, Guido and Kallus, Nathan and Mao, Xiaojie and Wang, Yuhao},
 year = {2022},
 title = {Long-term Causal Inference Under Persistent Confounding via Data  Combination},
 url = {http://arxiv.org/pdf/2202.07234v1},
 keywords = {Statistics - Machine Learning;Statistics - Methodology},
 journal = {arXiv preprint},
 file = {2202.07234:Attachments/2202.07234.pdf:application/pdf}
}


@proceedings{ITCS.2012,
 year = {2012},
 title = {ITCS}
}


@misc{Iwata.19.05.2023,
 abstract = {This article proposes a meta-learning method for estimating the conditional average treatment effect (CATE) from a few observational data. The proposed method learns how to estimate CATEs from multiple tasks and uses the knowledge for unseen tasks. In the proposed method, based on the meta-learner framework, we decompose the CATE estimation problem into sub-problems. For each sub-problem, we formulate our estimation models using neural networks with task-shared and task-specific parameters. With our formulation, we can obtain optimal task-specific parameters in a closed form that are differentiable with respect to task-shared parameters, making it possible to perform effective meta-learning. The task-shared parameters are trained such that the expected CATE estimation performance in few-shot settings is improved by minimizing the difference between a CATE estimated with a large amount of data and one estimated with just a few data. Our experimental results demonstrate that our method outperforms the existing meta-learning approaches and CATE estimation methods.},
 author = {Iwata, Tomoharu and Chikahara, Yoichi},
 date = {19.05.2023},
 title = {Meta-learning for heterogeneous treatment effect estimation with  closed-form solvers},
 url = {http://arxiv.org/pdf/2305.11353v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Machine Learning},
 file = {2305.11353:Attachments/2305.11353.pdf:application/pdf}
}


@inproceedings{Jabbari.2017,
 abstract = {We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness},
 author = {Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Roth, Aaron},
 title = {Fairness in reinforcement learning},
 keywords = {Computer Science - Learning},
 booktitle = {ICML},
 year = {2017},
 file = {jabbari17a:Attachments/jabbari17a.pdf:application/pdf}
}


@misc{Javidian.27.02.2021,
 abstract = {One of the most critical problems in transfer learning is the task of domain adaptation, where the goal is to apply an algorithm trained in one or more source domains to a different (but related) target domain. This paper deals with domain adaptation in the presence of covariate shift while invariances exist across domains. One of the main limitations of existing causal inference methods for solving this problem is scalability. To overcome this difficulty, we propose SCTL, an algorithm that avoids an exhaustive search and identifies invariant causal features across source and target domains based on Markov blanket discovery. SCTL does not require having prior knowledge of the causal structure, the type of interventions, or the intervention targets. There is an intrinsic locality associated with SCTL that makes it practically scalable and robust because local causal discovery increases the power of computational independence tests and makes the task of domain adaptation computationally tractable. We show the scalability and robustness of SCTL for domain adaptation using synthetic and real data sets in low-dimensional and high-dimensional settings.},
 author = {Javidian, Mohammad Ali and Pandey, Om and Jamshidi, Pooyan},
 date = {27.02.2021},
 title = {Scalable Causal Domain Adaptation},
 url = {http://arxiv.org/pdf/2103.00139v3},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Machine Learning},
 file = {2103.00139:Attachments/2103.00139.pdf:application/pdf}
}


@article{Jerolon.2020,
 abstract = {Mediation analysis aims at disentangling the effects of a treatment on an outcome through alternative causal mechanisms and has become a popular practice in biomedical and social science applications. The causal framework based on counterfactuals is currently the standard approach to mediation, with important methodological advances introduced in the literature in the last decade, especially for simple mediation, that is with one mediator at the time. Among a variety of alternative approaches, Imai et al. showed theoretical results and developed an R package to deal with simple mediation as well as with multiple mediation involving multiple mediators conditionally independent given the treatment and baseline covariates. This approach does not allow to consider the often encountered situation in which an unobserved common cause induces a spurious correlation between the mediators. In this context, which we refer to as mediation with uncausally related mediators, we show that, under appropriate hypothesis, the natural direct and joint indirect effects are non-parametrically identifiable. Moreover, we adopt the quasi-Bayesian algorithm developed by Imai et al. and propose a procedure based on the simulation of counterfactual distributions to estimate not only the direct and joint indirect effects but also the indirect effects through individual mediators. We study the properties of the proposed estimators through simulations. As an illustration, we apply our method on a real data set from a large cohort to assess the effect of hormone replacement treatment on breast cancer risk through three mediators, namely dense mammographic area, nondense area and body mass index.},
 author = {J{\'e}rolon, Allan and Baglietto, Laura and Birmel{\'e}, Etienne and Alarcon, Flora and Perduca, Vittorio},
 year = {2020},
 title = {Causal mediation analysis in presence of multiple mediators uncausally related},
 keywords = {Bayes Theorem;Causality;Cohort Studies;correlated mediators;direct and indirect effects;Humans;independent mediators;Mediation Analysis;Models, Statistical;multiple mediators;simulation of counterfactuals},
 pages = {191--221},
 volume = {17},
 number = {2},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/ijb-2019-0088},
 file = {10.1515{\_}ijb-2019-0088:Attachments/10.1515{\_}ijb-2019-0088.pdf:application/pdf}
}


@inproceedings{Jesson.2020,
 author = {Jesson, Andrew and Mindermann, S{\"o}ren and Shalit, Uri and Gal, Yarin},
 title = {Identifying causal effect inference failure with uncertainty-aware models},
 booktitle = {NeurIPS},
 year = {2020},
 file = {NeurIPS-2020-identifying-causal-effect-inference-failure-with-uncertainty-aware-models-Paper (4):Attachments/NeurIPS-2020-identifying-causal-effect-inference-failure-with-uncertainty-aware-models-Paper (4).pdf:application/pdf}
}


@inproceedings{Jesson.2021,
 author = {Jesson, Andrew and Mindermann, S{\"o}ren and Gal, Yarin and Shalit, Uri},
 title = {Quantifying ignorance in individual-level causal-effect estimates under hidden confounding},
 booktitle = {ICML},
 year = {2021},
 file = {jesson21a:Attachments/jesson21a.pdf:application/pdf}
}


@inproceedings{Jesson.2022,
 abstract = {Estimating the effects of continuous-valued interventions from observational data is critically important in fields such as climate science, healthcare, and economics. Recent work focuses on designing neural-network architectures and regularization functions to allow for scalable estimation of average and individual-level dose response curves from high-dimensional, large-sample data. Such methodologies assume ignorability (all confounding variables are observed) and positivity (all levels of treatment can be observed for every unit described by a given covariate value), which are especially challenged in the continuous treatment regime. Developing scalable sensitivity and uncertainty analyses that allow us to understand the ignorance induced in our estimates when these assumptions are relaxed receives less attention. Here, we develop a continuous treatment-effect marginal sensitivity model (CMSM) and derive bounds that agree with both the observed data and a researcher-defined level of hidden confounding. We introduce a scalable algorithm to derive the bounds and uncertainty-aware deep models to efficiently estimate these bounds for high-dimensional, large-sample observational data. We validate our methods using both synthetic and real-world experiments. For the latter, we work in concert with climate scientists interested in evaluating the climatological impacts of human emissions on cloud properties using satellite observations from the past 15 years: a finite-data problem known to be complicated by the presence of a multitude of unobserved confounders.},
 author = {Jesson, Andrew and Douglas, Alyson and Manshausen, Peter and Meinshausen, Nicolai and Stier, Philip and Gal, Yarin and Shalit, Uri},
 title = {Scalable sensitivity and uncertainty analysis for causal-effect estimates of continuous-valued interventions},
 url = {http://arxiv.org/pdf/2204.10022v2},
 keywords = {causal effects;Climate;Clouds;Computer Science - Learning;ICML;Machine learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2022},
 file = {2204.10022 (2):Attachments/2204.10022 (2).pdf:application/pdf}
}


@inproceedings{Jiang.2019,
 author = {Jiang, Jiechuan and Lu, Zongqing},
 title = {Learning fairness in multi-agent systems},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-learning-fairness-in-multi-agent-systems-Paper:Attachments/NeurIPS-2019-learning-fairness-in-multi-agent-systems-Paper.pdf:application/pdf}
}


@article{Jin.2022,
 abstract = {This paper introduces the $f$-sensitivity model, a new sensitivity model that characterizes the violation of unconfoundedness in causal inference. It assumes the selection bias due to unmeasured confounding is bounded {\textquotedbl}on average{\textquotedbl}; compared with the widely used point-wise sensitivity models in the literature, it is able to capture the strength of unmeasured confounding by not only its magnitude but also the chance of encountering such a magnitude.  We propose a framework for sensitivity analysis under our new model based on a distributional robustness perspective. We first show that the bounds on counterfactual means under the f-sensitivity model are optimal solutions to a new class of distributionally robust optimization (DRO) programs, whose dual forms are essentially risk minimization problems. We then construct point estimators for these bounds by applying a novel debiasing technique to the output of the corresponding empirical risk minimization (ERM) problems. Our estimators are shown to converge to valid bounds on counterfactual means if any nuisance component can be estimated consistently, and to the exact bounds when the ERM step is additionally consistent. We further establish asymptotic normality and Wald-type inference for these estimators under slower-than-root-n convergence rates of the estimated nuisance components. Finally, the performance of our method is demonstrated with numerical experiments.},
 author = {Jin, Ying and Ren, Zhimei and Zhou, Zhengyuan},
 year = {2022},
 title = {Sensitivity analysis under the $f$-sensitivity models: A distributional  robustness perspective},
 url = {http://arxiv.org/pdf/2203.04373v2},
 keywords = {Statistics - Methodology},
 volume = {arXiv:2203.04373},
 journal = {arXiv preprint},
 file = {2203.04373:Attachments/2203.04373.pdf:application/pdf}
}


@article{Jin.2023,
 abstract = {We propose a model-free framework for sensitivity analysis of individual treatment effects (ITEs), building upon ideas from conformal inference. For any unit, our procedure reports the \textgreek{G}-value, a number which quantifies the minimum strength of confounding needed to explain away the evidence for ITE. Our approach rests on the reliable predictive inference of counterfactuals and ITEs in situations where the training data are confounded. Under the marginal sensitivity model of [Z. Tan, J. Am. Stat. Assoc. 101, 1619-1637 (2006)], we characterize the shift between the distribution of the observations and that of the counterfactuals. We first develop a general method for predictive inference of test samples from a shifted distribution; we then leverage this to construct covariate-dependent prediction sets for counterfactuals. No matter the value of the shift, these prediction sets (resp. approximately) achieve marginal coverage if the propensity score is known exactly (resp. estimated). We describe a distinct procedure also attaining coverage, however, conditional on the training data. In the latter case, we prove a sharpness result showing that for certain classes of prediction problems, the prediction intervals cannot possibly be tightened. We verify the validity and performance of the methods via simulation studies and apply them to analyze real datasets.},
 author = {Jin, Ying and Ren, Zhimei and Cand{\`e}s, Emmanuel J.},
 year = {2023},
 title = {Sensitivity analysis of individual treatment effects: A robust conformal inference approach},
 volume = {120},
 number = {6},
 journal = {Proceedings of the National Academy of Sciences (PNAS)},
 file = {pnas.2214889120:Attachments/pnas.2214889120.pdf:application/pdf}
}


@inproceedings{Johansson.2016,
 abstract = {Proceedings of the International Conference on Machine Learning 2016},
 author = {Johansson, Fredrik D. and Shalit, Uri and Sonntag, David},
 title = {Learning representations for counterfactual inference},
 keywords = {causal inference;domain adaptation;representation learning},
 booktitle = {ICML},
 year = {2016},
 file = {Learning Representations for Counterfactual Inference:Attachments/Learning Representations for Counterfactual Inference.pdf:application/pdf}
}


@article{Johnson.2016,
 abstract = {MIMIC-III ('Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
 author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark, Roger G.},
 year = {2016},
 title = {{MIMIC-III}, a freely accessible critical care database},
 pages = {160035},
 volume = {3},
 number = {1},
 issn = {2052-4463},
 journal = {Scientific Data},
 doi = {10.1038/sdata.2016.35},
 file = {Johnson, Pollard et al. 2016 - MIMIC-III, a freely accessible critical:Attachments/Johnson, Pollard et al. 2016 - MIMIC-III, a freely accessible critical.pdf:application/pdf}
}


@misc{Jrgensen.01.06.2023,
 abstract = {Many fairness criteria constrain the policy or choice of predictors. In this work, we propose a different framework for thinking about fairness: Instead of constraining the policy or choice of predictors, we consider which utility a policy is optimizing for. We define value of information fairness and propose to not use utilities that do not satisfy this criterion. We describe how to modify a utility to satisfy this fairness criterion and discuss the consequences this might have on the corresponding optimal policies.},
 author = {J{\o}rgensen, Frederik Hytting and Weichwald, Sebastian and Peters, Jonas},
 date = {01.06.2023},
 title = {Unfair Utilities and First Steps Towards Improving Them},
 url = {http://arxiv.org/pdf/2306.00636v1},
 keywords = {Computer Science - Computers and Society;Computer Science - Learning;Statistics - Machine Learning},
 file = {2306.00636:Attachments/2306.00636.pdf:application/pdf}
}


@article{Kaddour.2022,
 abstract = {Causal Machine Learning (CausalML) is an umbrella term for machine learning methods that formalize the data-generation process as a structural causal model (SCM). This perspective enables us to reason about the effects of changes to this process (interventions) and what would have happened in hindsight (counterfactuals). We categorize work in CausalML into five groups according to the problems they address: (1) causal supervised learning, (2) causal generative modeling, (3) causal explanations, (4) causal fairness, and (5) causal reinforcement learning. We systematically compare the methods in each category and point out open problems. Further, we review data-modality-specific applications in computer vision, natural language processing, and graph representation learning. Finally, we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field, including recommendations for future work.},
 author = {Kaddour, Jean and Lynch, Aengus and Liu, Qi and Kusner, Matt J. and Silva, Ricardo},
 year = {2022},
 title = {Causal machine learning: A survey and open problems},
 keywords = {Computer Science - Learning;Statistics - Methodology},
 volume = {arxiv:2206.15475},
 journal = {arXiv preprint},
 file = {2206.15475:Attachments/2206.15475.pdf:application/pdf}
}


@inproceedings{Kallus.2018,
 abstract = {We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success.},
 author = {Kallus, Nathan},
 title = {Balanced policy evaluation and learning},
 booktitle = {NeurIPS},
 year = {2018},
 file = {Kallus 2018 - Balanced Policy Evaluation and Learning:Attachments/Kallus 2018 - Balanced Policy Evaluation and Learning.pdf:application/pdf}
}


@inproceedings{Kallus.2018b,
 abstract = {Valid causal inference in observational studies often requires controlling for confounders. However, in practice measurements of confounders may be noisy, and can lead to biased estimates of causal effects. We show that we can reduce the bias caused by measurement noise using a large number of noisy measurements of the underlying confounders. We propose the use of matrix factorization to infer the confounders from noisy covariates, a flexible and principled framework that adapts to missing values, accommodates a wide variety of data types, and can augment many causal inference methods. We bound the error for the induced average treatment effect estimator and show it is consistent in a linear regression setting, using Exponential Family Matrix Completion preprocessing. We demonstrate the effectiveness of the proposed procedure in numerical experiments with both synthetic data and real clinical data.},
 author = {Kallus, Nathan and Mao, Xiaojie and Udell, Madeleine},
 title = {Causal inference with noisy and missing covariates via matrix factorization},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2018},
 file = {1806.00811v1:Attachments/1806.00811v1.pdf:application/pdf}
}


@inproceedings{Kallus.2018c,
 author = {Kallus, Nathan and Zhou, Angela},
 title = {Confounding-robust policy improvement},
 booktitle = {NeurIPS},
 year = {2018},
 file = {neurips{\_}2018{\_}appendix:Attachments/neurips{\_}2018{\_}appendix.pdf:application/pdf;NeurIPS-2018-confounding-robust-policy-improvement-Paper:Attachments/NeurIPS-2018-confounding-robust-policy-improvement-Paper.pdf:application/pdf}
}


@inproceedings{Kallus.2018d,
 author = {Kallus, Nathan and Zhou, Angela},
 title = {Policy evaluation and optimization with continuous treatments},
 booktitle = {AISTATS},
 year = {2018},
 file = {kallus18a:Attachments/kallus18a.pdf:application/pdf}
}


@inproceedings{Kallus.2018e,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Kallus, Nathan and {Pulim Aahlad Manas} and Shalit, Uri},
 title = {Removing Hidden Confounding by Experimental Grounding},
 booktitle = {NeurIPS},
 year = {2018},
 file = {NeurIPS-2018-removing-hidden-confounding-by-experimental-grounding-Paper:Attachments/NeurIPS-2018-removing-hidden-confounding-by-experimental-grounding-Paper.pdf:application/pdf}
}


@inproceedings{Kallus.2018f,
 author = {Kallus, Nathan and Zhou, Angela},
 title = {Residual unfairness in fair machine learning from prejudiced data},
 booktitle = {ICML},
 year = {2018},
 file = {kallus18a:Attachments/kallus18a.pdf:application/pdf}
}


@inproceedings{Kallus.2019,
 author = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
 title = {Interval estimation of individual-level causal effects under unobserved confounding},
 booktitle = {AISTATS},
 year = {2019},
 file = {kallus19a:Attachments/kallus19a.pdf:application/pdf}
}


@article{Kallus.2019b,
 abstract = {The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, to construct models that predict the protected class from proxy variables, such as surname and geolocation. We show that even with such data, a variety of common disparity measures are generally unidentifiable, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the tightest-possible set of all possible true disparities that are consistent with the data (and possibly any assumptions). We further provide optimization-based algorithms for computing and visualizing these sets and statistical tools to assess sampling uncertainty. Together, these enable reliable and robust assessments of disparities -- an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.},
 author = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
 year = {2021},
 title = {Assessing algorithmic fairness with unobserved protected class using data combination},
 url = {http://arxiv.org/pdf/1906.00285v2},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Statistics - Machine Learning},
 pages = {1591--2376},
 volume = {68},
 number = {3},
 issn = {0025-1909},
 journal = {Management Science},
 file = {1906.00285:Attachments/1906.00285.pdf:application/pdf}
}


@misc{Kallus.2021,
 abstract = {We study the estimation of causal parameters when not all confounders are observed and instead negative controls are available. Recent work has shown how these can enable identification and efficient estimation via two so-called bridge functions. In this paper, we tackle the primary challenge to causal inference using negative controls: the identification and estimation of these bridge functions. Previous work has relied on uniqueness and completeness assumptions on these functions that may be implausible in practice and also focused on their parametric estimation. Instead, we provide a new identification strategy that avoids both uniqueness and completeness. And, we provide a new estimators for these functions based on minimax learning formulations. These estimators accommodate general function classes such as reproducing Hilbert spaces and neural networks. We study finite-sample convergence results both for estimating bridge function themselves and for the final estimation of the causal parameter. We do this under a variety of combinations of assumptions that include realizability and closedness conditions on the hypothesis and critic classes employed in the minimax estimator. Depending on how much we are willing to assume, we obtain different convergence rates. In some cases, we show the estimate for the causal parameter may converge even when our bridge function estimators do not converge to any valid bridge function. And, in other cases, we show we can obtain semiparametric efficiency.},
 author = {Kallus, Nathan and Mao, Xiaojie and Uehara, Masatoshi},
 date = {2021},
 title = {Causal Inference Under Unmeasured Confounding With Negative Controls: A  Minimax Learning Approach},
 url = {http://arxiv.org/pdf/2103.14029v2},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 file = {2103.14029:Attachments/2103.14029.pdf:application/pdf}
}


@article{Kallus.2021b,
 author = {Kallus, Nathan},
 year = {2021},
 title = {More efficient policy learning via optimal retargeting},
 keywords = {Efficient policy learning;Individualized treatment regimes;Optimization;Overlap},
 pages = {646--658},
 volume = {116},
 number = {534},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2020.1788948},
 file = {01621459.2020:Attachments/01621459.2020.pdf:application/pdf}
}


@inproceedings{Kallus.2021c,
 abstract = {We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a {\textquotedbl}triple bottom line{\textquotedbl}: personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.},
 author = {Kallus, Nathan and Zhou, Angela},
 title = {Fairness, welfare, and equity in personalized pricing},
 keywords = {Computer Science - Computers and Society;Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {FAccT},
 year = {2021},
 file = {2012.11066:Attachments/2012.11066.pdf:application/pdf}
}


@inproceedings{Kallus.2022,
 abstract = {Off-policy evaluation and learning (OPE/L) use offline observational data to make better decisions, which is crucial in applications where experimentation is necessarily limited. OPE/L is nonetheless sensitive to discrepancies between the data-generating environment and that where policies are deployed. Recent work proposed distributionally robust OPE/L (DROPE/L) to remedy this, but the proposal relies on inverse-propensity weighting, whose regret rates may deteriorate if propensities are estimated and whose variance is suboptimal even if not. For vanilla OPE/L, this is solved by doubly robust (DR) methods, but they do not naturally extend to the more complex DROPE/L, which involves a worst-case expectation. In this paper, we propose the first DR algorithms for DROPE/L with KL-divergence uncertainty sets. For evaluation, we propose Localized Doubly Robust DROPE (LDR$^2$OPE) and prove its semiparametric efficiency under weak product rates conditions. Notably, thanks to a localization technique, LDR$^2$OPE only requires fitting a small number of regressions, just like DR methods for vanilla OPE. For learning, we propose Continuum Doubly Robust DROPL (CDR$^2$OPL) and show that, under a product rate condition involving a continuum of regressions, it enjoys a fast regret rate of {\$}\mathcal{O}(N{\^{}}{-1/2}){\$} even when unknown propensities are nonparametrically estimated. We further extend our results to general $f$-divergence uncertainty sets. We illustrate the advantage of our algorithms in simulations.},
 author = {Kallus, Nathan and Mao, Xiaojie and Wang, Kaiwen and Zhou, Zhengyuan},
 title = {Doubly robust distributionally roust off-policy evaluation and learning},
 url = {http://arxiv.org/pdf/2202.09667v1},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 booktitle = {ICML},
 year = {2022},
 file = {2202.09667:Attachments/2202.09667.pdf:application/pdf}
}


@misc{Kallus.2022b,
 abstract = {The conditional average treatment effect (CATE) is the best point prediction of individual causal effects given individual baseline covariates and can help personalize treatments. However, as CATE only reflects the (conditional) average, it can wash out potential risks and tail events, which are crucially relevant to treatment choice. In aggregate analyses, this is usually addressed by measuring distributional treatment effect (DTE), such as differences in quantiles or tail expectations between treatment groups. Hypothetically, one can similarly fit covariate-conditional quantile regressions in each treatment group and take their difference, but this would not be robust to misspecification or provide agnostic best-in-class predictions. We provide a new robust and model-agnostic methodology for learning the conditional DTE (CDTE) for a wide class of problems that includes conditional quantile treatment effects, conditional super-quantile treatment effects, and conditional treatment effects on coherent risk measures given by $f$-divergences. Our method is based on constructing a special pseudo-outcome and regressing it on baseline covariates using any given regression learner. Our method is model-agnostic in the sense that it can provide the best projection of CDTE onto the regression model class. Our method is robust in the sense that even if we learn these nuisances nonparametrically at very slow rates, we can still learn CDTEs at rates that depend on the class complexity and even conduct inferences on linear projections of CDTEs. We investigate the performance of our proposal in simulation studies, and we demonstrate its use in a case study of 401(k) eligibility effects on wealth.},
 author = {Kallus, Nathan and Oprescu, Miruna},
 date = {2022},
 title = {Robust and agnostic learning of conditional distributional treatment  effects},
 url = {http://arxiv.org/pdf/2205.11486v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 file = {2205.11486:Attachments/2205.11486.pdf:application/pdf}
}


@misc{Karlsson.2022,
 abstract = {A common assumption in causal inference from observational data is the assumption of no hidden confounding. Yet it is, in general, impossible to verify the presence of hidden confounding factors from a single dataset. However, under the assumption of independent causal mechanisms underlying the data generative process, we demonstrate a way to detect unobserved confounders when having multiple observational datasets coming from different environments. We present a theory for testable conditional independencies that are only violated during hidden confounding and examine cases where we break its assumptions: degenerate {\&} dependent mechanisms, and faithfulness violations. Additionally, we propose a procedure to test these independencies and study its empirical finite-sample behavior using simulation studies.},
 author = {Karlsson, Rickard K. A. and Krijthe, Jesse H.},
 date = {2022},
 title = {Combining observational datasets from multiple environments to detect  hidden confounding},
 url = {http://arxiv.org/pdf/2205.13935v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 file = {2205.13935:Attachments/2205.13935.pdf:application/pdf}
}


@article{Katok.2014,
 author = {Katok, Elena and Olsen, Tava and Pavlov, Valery},
 year = {2014},
 title = {Wholesale pricing under mild and privately known concerns for fairness},
 pages = {285--302},
 volume = {23},
 number = {2},
 journal = {Production and Operations Management},
 file = {WP{\_}post:Attachments/WP{\_}post.pdf:application/pdf}
}


@proceedings{KDD.2017,
 year = {2017},
 title = {KDD}
}


@proceedings{KDD.2018,
 year = {2018},
 title = {KDD}
}


@article{Kelley.2022,
 abstract = {M{\&}SOM 2022.24:3039-3059},
 author = {Kelley, Stephanie and Ovchinnikov, Anton and Hardoon, David R. and Heinrich, Adrienne},
 year = {2022},
 title = {Antidiscrimination laws, artificial intelligence, and gender bias: A case study in nonmortgage fintech lending},
 keywords = {artificial intelligence;Bias;discrimination;ethics;fintech;gender;law;Machine learning},
 volume = {24},
 number = {6},
 journal = {Manufacturing {\&} Science Operations Management},
 file = {msom.2022.1108:Attachments/msom.2022.1108.pdf:application/pdf}
}


@article{Kennedy.2016,
 abstract = {In this paper we review important aspects of semiparametric theory and empirical processes that arise in causal inference problems. We begin with a brief introduction to the general problem of causal inference, and go on to discuss estimation and inference for causal effects under semiparametric models, which allow parts of the data-generating process to be unrestricted if they are not of particular interest (i.e., nuisance functions). These models are very useful in causal problems because the outcome process is often complex and difficult to model, and there may only be information available about the treatment process (at best). Semiparametric theory gives a framework for benchmarking efficiency and constructing estimators in such settings. In the second part of the paper we discuss empirical process theory, which provides powerful tools for understanding the asymptotic behavior of semiparametric estimators that depend on flexible nonparametric estimators of nuisance functions. These tools are crucial for incorporating machine learning and other modern methods into causal inference analyses. We conclude by examining related extensions and future directions for work in semiparametric causal inference.},
 author = {Kennedy, Edward H.},
 year = {2016},
 title = {Semiparametric theory and empirical processes in causal inference},
 keywords = {Mathematics - Statistics;Statistics - Theory},
 pages = {141--167},
 journal = {Statistical Causal Inferences and their Applications in Public Health Research},
 file = {1510.04740:Attachments/1510.04740.pdf:application/pdf}
}


@article{Kennedy.2017,
 abstract = {Continuous treatments (e.g., doses) arise often in practice, but many available causal effect estimators are limited by either requiring parametric models for the effect curve, or by not allowing doubly robust covariate adjustment. We develop a novel kernel smoothing approach that requires only mild smoothness assumptions on the effect curve, and still allows for misspecification of either the treatment density or outcome regression. We derive asymptotic properties and give a procedure for data-driven bandwidth selection. The methods are illustrated via simulation and in a study of the effect of nurse staffing on hospital readmissions penalties.},
 author = {Kennedy, Edward H. and Ma, Zongming and McHugh, Matthew D. and Small, Dylan S.},
 year = {2017},
 title = {Nonparametric methods for doubly robust estimation of continuous treatment effects},
 pages = {1229--1245},
 volume = {79},
 number = {4},
 issn = {1369-7412},
 journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
 doi = {10.1111/rssb.12212},
 file = {Journal of the Royal Statistical Society  Series B  Statistical Methodology - 2016 - Kennedy - Non-parametric methods for:Attachments/Journal of the Royal Statistical Society  Series B  Statistical Methodology - 2016 - Kennedy - Non-parametric methods for.pdf:application/pdf}
}


@misc{Kennedy.2018,
 author = {Kennedy, Edward H.},
 title = {Nonparametric efficiency theory and machine learning in causal inference},
 file = {tutorial:Attachments/tutorial.pdf:application/pdf}
}


@article{Kennedy.2019,
 abstract = {Instrumental variables are commonly used to estimate effects of a treatment afflicted by unmeasured confounding, and in practice instruments are often continuous (e.g., measures of distance, or treatment preference). However, available methods for continuous instruments have important limitations: they either require restrictive parametric assumptions for identification, or else rely on modeling both the outcome and treatment process well (and require modeling effect modification by all adjustment covariates). In this work we develop the first semiparametric doubly robust estimators of the local instrumental variable effect curve, i.e., the effect among those who would take treatment for instrument values above some threshold and not below. In addition to being robust to misspecification of either the instrument or treatment/outcome processes, our approach also incorporates information about the instrument mechanism and allows for flexible data-adaptive estimation of effect modification. We discuss asymptotic properties under weak conditions, and use the methods to study infant mortality effects of neonatal intensive care units with high versus low technical capacity, using travel time as an instrument.},
 author = {Kennedy, Edward H. and Lorch, Scott A. and Small, Dylan S.},
 year = {2019},
 title = {Robust causal inference with continuous instruments using the local instrumental variable curve},
 url = {http://arxiv.org/pdf/1607.02566v3},
 keywords = {Statistics - Methodology},
 pages = {121--143},
 volume = {81},
 number = {1},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 file = {1607.02566:Attachments/1607.02566.pdf:application/pdf}
}


@article{Kennedy.2022,
 abstract = {In this review we cover the basics of efficient nonparametric parameter estimation (also called functional estimation), with a focus on parameters that arise in causal inference problems. We review both efficiency bounds (i.e., what is the best possible performance for estimating a given parameter?) and the analysis of particular estimators (i.e., what is this estimator's error, and does it attain the efficiency bound?) under weak assumptions. We emphasize minimax-style efficiency bounds, worked examples, and practical shortcuts for easing derivations. We gloss over most technical details, in the interest of highlighting important concepts and providing intuition for main ideas.},
 author = {Kennedy, Edward H.},
 year = {2022},
 title = {Semiparametric doubly robust targeted double machine learning: A review},
 url = {http://arxiv.org/pdf/2203.06469v1},
 keywords = {Statistics - Methodology},
 journal = {arXiv preprint},
 file = {2203.06469:Attachments/2203.06469.pdf:application/pdf}
}


@article{Kennedy.2022b,
 abstract = {Estimation of heterogeneous causal effects - i.e., how effects of policies and treatments vary across subjects - is a fundamental task in causal inference, playing a crucial role in optimal treatment allocation, generalizability, subgroup effects, and more. Many flexible methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but questions surrounding optimality have remained largely unanswered. In particular, a minimax theory of optimality has yet to be developed, with the minimax rate of convergence and construction of rate-optimal estimators remaining open problems. In this paper we derive the minimax rate for CATE estimation, in a nonparametric model where distributional components are Holder-smooth, and present a new local polynomial estimator, giving high-level conditions under which it is minimax optimal. More specifically, our minimax lower bound is derived via a localized version of the method of fuzzy hypotheses, combining lower bound constructions for nonparametric regression and functional estimation. Our proposed estimator can be viewed as a local polynomial R-Learner, based on a localized modification of higher-order influence function methods; it is shown to be minimax optimal under a condition on how accurately the covariate distribution is estimated. The minimax rate we find exhibits several interesting features, including a non-standard elbow phenomenon and an unusual interpolation between nonparametric regression and functional estimation rates. The latter quantifies how the CATE, as an estimand, can be viewed as a regression/functional hybrid. We conclude with some discussion of a few remaining open problems.},
 author = {Kennedy, Edward H. and Balakrishnan, Sivaraman and Wasserman, Larry},
 year = {2022},
 title = {Minimax rates for heterogeneous causal effect estimation},
 url = {http://arxiv.org/pdf/2203.00837v1},
 keywords = {Mathematics - Statistics;Statistics - Theory},
 journal = {arXiv preprint},
 file = {2203.00837:Attachments/2203.00837.pdf:application/pdf}
}


@misc{Kennedy.2022c,
 author = {Kennedy, Edward H.},
 title = {Machine learning {\&} nonparametric efficiency  in causal inference},
 file = {tutorial (1):Attachments/tutorial (1).pdf:application/pdf}
}


@article{Kennedy.2022d,
 abstract = {Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations.},
 author = {Kennedy, Edward H.},
 year = {2022},
 title = {Towards optimal doubly robust estimation of heterogeneous causal effects},
 keywords = {Mathematics - Statistics;Statistics - Theory},
 journal = {arXiv preprint},
 file = {2004.14497 (2):Attachments/2004.14497 (2).pdf:application/pdf}
}


@article{Kennedy.2023,
 abstract = {Causal effects are often characterized with averages, which can give an incomplete picture of the underlying counterfactual distributions. Here we consider estimating the entire counterfactual density and generic functionals thereof. We focus on two kinds of target parameters. The first is a density approximation, defined by a projection onto a finite-dimensional model using a generalized distance metric, which includes f-divergences as well as $L_p$ norms. The second is the distance between counterfactual densities, which can be used as a more nuanced effect measure than the mean difference, and as a tool for model selection. We study nonparametric efficiency bounds for these targets, giving results for smooth but otherwise generic models and distances. Importantly, we show how these bounds connect to means of particular non-trivial functions of counterfactuals, linking the problems of density and mean estimation. We go on to propose doubly robust-style estimators for the density approximations and distances, and study their rates of convergence, showing they can be optimally efficient in large nonparametric models. We also give analogous methods for model selection and aggregation, when many models may be available and of interest. Our results all hold for generic models and distances, but throughout we highlight what happens for particular choices, such as $L_2$ projections on linear models, and KL projections on exponential families. Finally we illustrate by estimating the density of CD4 count among patients with HIV, had all been treated with combination therapy versus zidovudine alone, as well as a density effect. Our results suggest combination therapy may have increased CD4 count most for high-risk patients. Our methods are implemented in the freely available R package npcausal on GitHub.},
 author = {Kennedy, Edward H. and Balakrishnan, Sivaraman and Wasserman, Larry},
 year = {2023},
 title = {Semiparametric counterfactual density estimation},
 url = {http://arxiv.org/pdf/2102.12034v1},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 issn = {0006-3444},
 journal = {Biometrika},
 file = {2102.12034:Attachments/2102.12034.pdf:application/pdf}
}


@misc{Khan.19.05.2023,
 abstract = {Off-policy evaluation (OPE) is the problem of estimating the value of a target policy using historical data collected under a different logging policy. OPE methods typically assume overlap between the target and logging policy, enabling solutions based on importance weighting and/or imputation. In this work, we approach OPE without assuming either overlap or a well-specified model by considering a strategy based on partial identification under non-parametric assumptions on the conditional mean function, focusing especially on Lipschitz smoothness. Under such smoothness assumptions, we formulate a pair of linear programs whose optimal values upper and lower bound the contributions of the no-overlap region to the off-policy value. We show that these linear programs have a concise closed form solution that can be computed efficiently and that their solutions converge, under the Lipschitz assumption, to the sharp partial identification bounds on the off-policy value. Furthermore, we show that the rate of convergence is minimax optimal, up to log factors. We deploy our methods on two semi-synthetic examples, and obtain informative and valid bounds that are tighter than those possible without smoothness assumptions.},
 author = {Khan, Samir and Saveski, Martin and Ugander, Johan},
 date = {19.05.2023},
 title = {Off-policy evaluation beyond overlap: partial identification through smoothness},
 url = {http://arxiv.org/pdf/2305.11812v1},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 file = {2305.11812:Attachments/2305.11812.pdf:application/pdf}
}


@inproceedings{Kilbertus.2017,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Kilbertus, Niki and Rojas-Carulla, Mateo and Parascandolo, Gianbattista and Hardt, Moritz and Janzig, Dominik and Sch{\"o}lkopf, Bernhard},
 title = {Avoiding Discrimination through Causal Reasoning},
 booktitle = {NeurIPS},
 year = {2017},
 file = {NIPS-2017-avoiding-discrimination-through-causal-reasoning-Paper:Attachments/NIPS-2017-avoiding-discrimination-through-causal-reasoning-Paper.pdf:application/pdf}
}


@inproceedings{Kilbertus.2019,
 author = {Kilbertus, Niki and Ball, Philip J. and Kusner, Matt J. and Weller, Adrian and Silva, Ricardo},
 title = {The sensitivity of counterfactual fairness to unmeasured confounding},
 booktitle = {UAI},
 year = {2019},
 file = {kilbertus20a:Attachments/kilbertus20a.pdf:application/pdf}
}


@inproceedings{Kilbertus.2020,
 author = {Kilbertus, Niki and Kusner, Matt J. and Silva, Ricardo},
 title = {A class of algorithms for general instrumental variable models},
 booktitle = {NeurIPS},
 year = {2020},
 file = {NeurIPS-2020-a-class-of-algorithms-for-general-instrumental-variable-models-Paper:Attachments/NeurIPS-2020-a-class-of-algorithms-for-general-instrumental-variable-models-Paper.pdf:application/pdf}
}


@article{Kim.,
 abstract = {We propose a simple and general framework for nonparametric estimation of heterogeneous treatment effects under fairness constraints. Under standard regularity conditions, we show that the resulting estimators possess the double robustness property. We use this framework to characterize the trade-off between fairness and the maximum welfare achievable by the optimal policy. We evaluate the methods in a simulation study and illustrate them in a real-world case study.},
 author = {Kim, Kwangho and Zubizarreta, Jos{\'e} R.},
 title = {Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy  Learning},
 url = {http://arxiv.org/pdf/2306.03625v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 file = {2306.03625:Attachments/2306.03625.pdf:application/pdf}
}


@misc{Kim.2018,
 abstract = {In this paper we develop a framework for characterizing causal effects via distributional distances. In particular we define a causal effect in terms of the $L_1$ distance between different counterfactual outcome distributions, rather than the typical mean difference in outcome values. Comparing entire counterfactual outcome distributions can provide more nuanced and valuable measures for exploring causal effects beyond the average treatment effect. First, we propose a novel way to estimate counterfactual outcome densities, which is of independent interest. Then we develop an efficient estimator of our target causal effect. We go on to provide error bounds and asymptotic properties of the proposed estimator, along with bootstrap-based confidence intervals. Finally, we illustrate the methods via simulations and real data.},
 author = {Kim, Kwangho and Kim, Jisu and Kennedy, Edward H.},
 date = {2018},
 title = {Causal effects based on distributional distances},
 url = {http://arxiv.org/pdf/1806.02935v2},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 file = {1806.02935:Attachments/1806.02935.pdf:application/pdf}
}


@misc{Kingma.20.12.2013,
 abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
 author = {Kingma, Diederik P. and Welling, Max},
 date = {20.12.2013},
 title = {Auto-Encoding Variational Bayes},
 url = {http://arxiv.org/pdf/1312.6114v10},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 file = {162970{\_}1312.6114v10.pd:Attachments/162970{\_}1312.6114v10.pd.pdf:application/pdf}
}


@inproceedings{Kingma.2015,
 abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
 author = {Kingma, Diederik P. and Ba, Jimmy},
 title = {Adam: A method for stochastic optimization},
 booktitle = {ICLR},
 year = {2015}
}


@article{Kleinberg.2018,
 author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Rambachan, Ashesh},
 year = {2018},
 title = {Algorithmic fairness},
 pages = {22--27},
 volume = {108},
 issn = {2574-0768},
 journal = {AEA Papers and Proceedings},
 file = {aer18-fairness:Attachments/aer18-fairness.pdf:application/pdf}
}


@article{Kleinberg.2019,
 author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Sunstein, Cass R.},
 year = {2019},
 title = {Discrimination in the age of algorithms},
 pages = {113--174},
 volume = {10},
 journal = {Journal of Legal Analysis},
 doi = {10.1093/jla/laz001},
 file = {laz001:Attachments/laz001.pdf:application/pdf}
}


@article{Knowles.2010,
 author = {Knowles, David},
 year = {2010},
 title = {Lagrangian Duality for Dummies},
 file = {lagrangian{\_}duality:Attachments/lagrangian{\_}duality.pdf:application/pdf}
}


@article{Kozodoi.2022,
 abstract = {The rise of algorithmic decision-making has spawned much research on fair machine learning (ML). Financial institutions use ML for building risk scorecards that support a range of credit-related decisions. Yet, the literature on fair ML in credit scoring is scarce. The paper makes three contributions. First, we revisit statistical fairness criteria and examine their adequacy for credit scoring. Second, we catalog algorithmic options for incorporating fairness goals in the ML model development pipeline. Last, we empirically compare different fairness processors in a profit-oriented credit scoring context using real-world data. The empirical results substantiate the evaluation of fairness measures, identify suitable options to implement fair credit scoring, and clarify the profit-fairness trade-off in lending decisions. We find that multiple fairness criteria can be approximately satisfied at once and recommend separation as a proper criterion for measuring the fairness of a scorecard. We also find fair in-processors to deliver a good balance between profit and fairness and show that algorithmic discrimination can be reduced to a reasonable level at a relatively low cost. The codes corresponding to the paper are available on GitHub.},
 author = {Kozodoi, Nikita and Jacob, Johannes and Lessmann, Stefan},
 year = {2022},
 title = {Fairness in credit scoring: Assessment, implementation and profit implications},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 pages = {1083--1094},
 volume = {297},
 number = {3},
 issn = {03772217},
 journal = {European Journal of Operational Research},
 file = {2103.01907:Attachments/2103.01907.pdf:application/pdf}
}


@article{Kraus.2020,
 abstract = {European Journal of Operational Research, 281 (2020) 628-641. doi:10.1016/j.ejor.2019.09.018},
 author = {Kraus, Mathias and Feuerriegel, Stefan and Oztekin, Asil},
 year = {2020},
 title = {Deep learning in business analytics and operations research: {M}odels, applications and managerial implications},
 keywords = {Analytics;Deep Learning;Deep neural networks;Managerial implications;Research agenda},
 pages = {628--641},
 volume = {281},
 number = {3},
 issn = {03772217},
 journal = {European Journal of Operational Research},
 doi = {10.1016/j.ejor.2019.09.018},
 file = {1-s2.0-S0377221719307581-main:Attachments/1-s2.0-S0377221719307581-main.pdf:application/pdf}
}


@inproceedings{Kremer.2022,
 abstract = {Proceedings of the International Conference on Machine Learning 2022},
 author = {Kremer, Heiner and Zhu, Jia-Jie and Muandet, Krikamol and Sch{\"o}lkopf, Bernhard},
 title = {Functional generalized empirical likelihood estimation for conditional moment restrictions},
 booktitle = {ICML},
 year = {2022},
 file = {kremer22a:Attachments/kremer22a.pdf:application/pdf}
}


@article{Kuang.2021,
 author = {Kuang, Kun and Li, Yunzhe and Li, Bo and Cui, Peng and Yang, Hongxia and Tao, Jianrong and Wu, Fei},
 year = {2021},
 title = {Continuous treatment effect estimation via generative adversarial de-confounding},
 pages = {2467--2497},
 volume = {35},
 number = {6},
 issn = {1384-5810},
 journal = {Data Mining and Knowledge Discovery},
 doi = {10.1007/s10618-021-00797-x},
 file = {li20a:Attachments/li20a.pdf:application/pdf}
}


@misc{Kugelgen.01.06.2023,
 abstract = {We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions ({\textquotedbl}mixtures{\textquotedbl}) of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or the intervention targets. We instead consider the general setting in which both the causal model and the mixing function are nonparametric. The learning signal takes the form of multiple datasets, or environments, arising from unknown interventions in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that the observational distribution and one perfect intervention per node suffice for identifiability, subject to a genericity condition. This condition rules out spurious solutions that involve fine-tuning of the intervened and observational distributions, mirroring similar conditions for nonlinear cause-effect inference. For an arbitrary number of variables, we show that two distinct paired perfect interventions per node guarantee identifiability. Further, we demonstrate that the strengths of causal influences among the latent variables are preserved by all equivalent solutions, rendering the inferred representation appropriate for drawing causal conclusions from new data. Our study provides the first identifiability results for the general nonparametric setting with unknown interventions, and elucidates what is possible and impossible for causal representation learning without more direct supervision.},
 author = {von K{\"u}gelgen, Julius and Besserve, Michel and Liang, Wendong and Gresele, Luigi and Keki{\'c}, Armin and Bareinboim, Elias and Blei, David M. and Sch{\"o}lkopf, Bernhard},
 date = {01.06.2023},
 title = {Nonparametric Identifiability of Causal Representations from Unknown  Interventions},
 url = {http://arxiv.org/pdf/2306.00542v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Machine Learning},
 file = {2306.00542:Attachments/2306.00542.pdf:application/pdf}
}


@misc{Kuhn.2019,
 abstract = {Many decision problems in science, engineering and economics are affected by uncertain parameters whose distribution is only indirectly observable through samples. The goal of data-driven decision-making is to learn a decision from finitely many training samples that will perform well on unseen test samples. This learning task is difficult even if all training and test samples are drawn from the same distribution---especially if the dimension of the uncertainty is large relative to the training sample size. Wasserstein distributionally robust optimization seeks data-driven decisions that perform well under the most adverse distribution within a certain Wasserstein distance from a nominal distribution constructed from the training samples. In this tutorial we will argue that this approach has many conceptual and computational benefits. Most prominently, the optimal decisions can often be computed by solving tractable convex optimization problems, and they enjoy rigorous out-of-sample and asymptotic consistency guarantees. We will also show that Wasserstein distributionally robust optimization has interesting ramifications for statistical learning and motivates new approaches for fundamental learning tasks such as classification, regression, maximum likelihood estimation or minimum mean square error estimation, among others.},
 author = {Kuhn, Daniel and Esfahani, Peyman Mohajerin and Nguyen, Viet Anh and Shafieezadeh-Abadeh, Soroosh},
 date = {2019},
 title = {Wasserstein distributionally robust optimization: Theory and  applications in machine learning},
 url = {http://arxiv.org/pdf/1908.08729v1},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Statistics - Machine Learning},
 file = {1908.08729:Attachments/1908.08729.pdf:application/pdf}
}


@article{Kunzel.2019,
 abstract = {There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms-such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks-to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
 author = {K{\"u}nzel, S{\"o}ren R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
 year = {2019},
 title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
 pages = {4156--4165},
 volume = {116},
 number = {10},
 journal = {Proceedings of the National Academy of Sciences (PNAS)},
 doi = {10.1073/pnas.1804597116},
 file = {pnas.1804597116:Attachments/pnas.1804597116.pdf:application/pdf}
}


@inproceedings{Kusner.2017,
 abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
 author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
 title = {Counterfactual fairness},
 url = {http://arxiv.org/pdf/1703.06856v3},
 keywords = {Computer Science - Computers and Society;Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2017},
 file = {1703.06856:Attachments/1703.06856.pdf:application/pdf}
}


@inproceedings{Kuzmanovic.2021,
 author = {Kuzmanovic, Milan and Hatt, Tobias and Feuerriegel, Stefan},
 title = {Deconfounding temporal autoencoder: Estimating treatment effects over time using noisy proxies},
 booktitle = {ML4H},
 year = {2021},
 file = {kuzmanovic21a:Attachments/kuzmanovic21a.pdf:application/pdf}
}


@unpublished{Kuzmanovic.2022,
 author = {Kuzmanovic, Milan and Hatt, Tobias and Feuerriegel, Stefan},
 year = {2022},
 title = {Estimating heterogeneous treatment effects with missing treatment information},
 file = {main:Attachments/main.pdf:application/pdf}
}


@article{Kyono.11022021,
 abstract = {Selecting causal inference models for estimating individualized treatment effects (ITE) from observational data presents a unique challenge since the counterfactual outcomes are never observed. The problem is challenged further in the unsupervised domain adaptation (UDA) setting where we only have access to labeled samples in the source domain, but desire selecting a model that achieves good performance on a target domain for which only unlabeled samples are available. Existing techniques for UDA model selection are designed for the predictive setting. These methods examine discriminative density ratios between the input covariates in the source and target domain and do not factor in the model's predictions in the target domain. Because of this, two models with identical performance on the source domain would receive the same risk score by existing methods, but in reality, have significantly different performance in the test domain. We leverage the invariance of causal structures across domains to propose a novel model selection metric specifically designed for ITE methods under the UDA setting. In particular, we propose selecting models whose predictions of interventions' effects satisfy known causal structures in the target domain. Experimentally, our method selects ITE models that are more robust to covariate shifts on several healthcare datasets, including estimating the effect of ventilation in COVID-19 patients from different geographic locations.},
 author = {Kyono, Trent and Bica, Ioana and Qian, Zhaozhi and {van der Schaar}, Mihaela},
 title = {Selecting treatment effects models for domain adaptation using causal  knowledge},
 url = {http://arxiv.org/pdf/2102.06271v1},
 keywords = {Computer Science - Learning;ICML;Machine learning},
 journal = {arXiv preprint},
 file = {2102.06271:Attachments/2102.06271.pdf:application/pdf}
}


@inproceedings{Kyono.2021,
 author = {Kyono, Trent and Zhang, Yao and Bellot, Alexis and {van der Schaar}, Mihaela},
 title = {{MIRACLE}: Causally aware imputation via learning missing data mechanisms},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-miracle-causally-aware-imputation-via-learning-missing-data-mechanisms-Paper:Attachments/NeurIPS-2021-miracle-causally-aware-imputation-via-learning-missing-data-mechanisms-Paper.pdf:application/pdf}
}


@article{Lambrecht.2019,
 abstract = {Management Science 2019.65:2966-2981},
 author = {Lambrecht, Anja and Tucker, Catherine},
 year = {2019},
 title = {Algorithmic bias? {A}n empirical study of apparent gender-based discrimination in the display of {STEM} career ads},
 keywords = {algorithmic bias;Algorithms;artificial intelligence;online advertising},
 pages = {2966--2981},
 volume = {65},
 number = {7},
 issn = {0025-1909},
 journal = {Management Science},
 doi = {10.1287/mnsc.2018.3093},
 file = {mnsc.2018.3093:Attachments/mnsc.2018.3093.pdf:application/pdf}
}


@inproceedings{Lattimore.2016,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Laattimore, Finnian and Lattimore, Tor and Reid, Mark D.},
 title = {Causal Bandits: Learning good interventions via causal inference},
 booktitle = {NeurIPS},
 year = {2016},
 file = {NIPS-2016-causal-bandits-learning-good-interventions-via-causal-inference-Paper:Attachments/NIPS-2016-causal-bandits-learning-good-interventions-via-causal-inference-Paper.pdf:application/pdf}
}


@inproceedings{Li.2021,
 abstract = {G-Net: a Recurrent Network Approach to G-Computation for Counterfactual Prediction Under a Dynamic Treatment RegimeRui Li,~Stephanie Hu,~Mingyu Lu,...},
 author = {Li, Rui and Hu, Stephanie and Lu, Mingyu and Utsumi, Yuria and Chakraborty, Prithwish and Sow, Daby M. and Madan, Piyush and Li, Jun and Ghalwash, Mohamed and Shahn, Zach and Lehman, Li-wei},
 title = {G-Net: A recurrent network approach to G-computation for counterfactual prediction under a dynamic treatment regime},
 booktitle = {ML4H},
 year = {2021},
 file = {Rui Li, Stephanie Hu et al. 2021 - G-Net a Recurrent Network Approach:Attachments/Rui Li, Stephanie Hu et al. 2021 - G-Net a Recurrent Network Approach.pdf:application/pdf}
}


@article{Li.2022,
 author = {Li, Chunxiao and Rudin, Cynthia and McCormick, Typer H.},
 year = {2022},
 title = {Rethinking nonlinear instrumental variable models through prediction validity},
 pages = {1--55},
 volume = {23},
 journal = {Journal of Machine Learning Research},
 file = {21-0082:Attachments/21-0082.pdf:application/pdf}
}


@article{Liao.2020,
 abstract = {Due to the recent advancements in wearables and sensing technology, health scientists are increasingly developing mobile health (mHealth) interventions. In mHealth interventions, mobile devices are used to deliver treatment to individuals as they go about their daily lives. These treatments are generally designed to impact a near time, proximal outcome such as stress or physical activity. The mHealth intervention policies, often called just-in-time adaptive interventions, are decision rules that map an individual's current state (e.g., individual's past behaviors as well as current observations of time, location, social activity, stress and urges to smoke) to a particular treatment at each of many time points. The vast majority of current mHealth interventions deploy expert-derived policies. In this paper, we provide an approach for conducting inference about the performance of one or more such policies using historical data collected under a possibly different policy. Our measure of performance is the average of proximal outcomes over a long time period should the particular mHealth policy be followed. We provide an estimator as well as confidence intervals. This work is motivated by HeartSteps, an mHealth physical activity intervention.},
 author = {Liao, Peng and Klasnja, Predrag and Murphy, Susan},
 year = {2020},
 title = {Off-policy estimation of long-term average outcomes with applications to mobile health},
 keywords = {Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 pages = {1--23},
 volume = {116},
 journal = {Journal of the American Statistical Association},
 file = {1912.13088:Attachments/1912.13088.pdf:application/pdf}
}


@inproceedings{Lim.2018,
 author = {Lim, Bryan and Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Forecasting treatment responses over time using recurrent marginal structural networks},
 booktitle = {NeurIPS},
 year = {2018},
 file = {Forecasting Treatment Responses Over Time 2018:Attachments/Forecasting Treatment Responses Over Time 2018.pdf:application/pdf}
}


@inproceedings{Lin.2020,
 abstract = {We consider nonconvex-concave minimax problems, {\$}\min_{\mathbf{x}} \max_{\mathbf{y} $\backslash$in \mathcal{Y}} f(\mathbf{x}, \mathbf{y}){\$}, where $f$ is nonconvex in {\$}\mathbf{x}{\$} but concave in {\$}\mathbf{y}{\$} and {\$}\mathcal{Y}{\$} is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function {\$}$\backslash$Phi($\backslash$cdot) := \max_{\mathbf{y} $\backslash$in \mathcal{Y}} f($\backslash$cdot, \mathbf{y}){\$} efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding light on its superior practical performance in training generative adversarial networks (GANs) and other real applications.},
 author = {Lin, Tianyi and Jin, Chi and Jordan, Michael I.},
 title = {On gradient descent ascent for nonconvex-concave minimax problems},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Statistics - Machine Learning},
 booktitle = {ICML},
 year = {2020},
 file = {lin20a:Attachments/lin20a.pdf:application/pdf}
}


@inproceedings{Liu.2020,
 author = {Liu, Ruoqi and Yin, Changchang and Zhang, Ping},
 title = {Estimating individual treatment effects with time-varying confounders},
 booktitle = {ICDM},
 year = {2020}
}


@article{Liu.2023,
 abstract = {Higher-Order Influence Functions (HOIFs) provide a unified theory for constructing rate-optimal estimators for a large class of low-dimensional (smooth) statistical functionals/parameters (and sometimes even infinite-dimensional functions) that arise in substantive fields including epidemiology, economics, and the social sciences. Since the introduction of HOIFs by Robins et al. (2008), they have been viewed mostly as a theoretical benchmark rather than a useful tool for statistical practice. Works aimed to flip the script are scant, but a few recent papers Liu et al. (2017, 2021b) make some partial progress. In this paper, we take a fresh attempt at achieving this goal by constructing new, numerically stable HOIF estimators (or sHOIF estimators for short with ``s'' standing for ``stable'') with provable statistical, numerical, and computational guarantees. This new class of sHOIF estimators (up to the 2nd order) was foreshadowed in synthetic experiments conducted by Liu et al. (2020a).},
 author = {Liu, Lin and Li, Chang},
 title = {New {\$}\sqrt{n}{\$}-consistent, numerically stable higher-order influence  function estimators},
 url = {http://arxiv.org/pdf/2302.08097v1},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Methodology;Statistics - Theory},
 volume = {arXiv:2302.08097},
 journal = {arXiv preprint},
 file = {2302.08097:Attachments/2302.08097.pdf:application/pdf}
}


@inproceedings{Locatello.2019,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Locatello, Francesco and Abbati, Gabriele and Rainforth, Tom and Bauer, Stefan and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
 title = {On the fairness of disentangled representations},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-on-the-fairness-of-disentangled-representations-Paper:Attachments/NeurIPS-2019-on-the-fairness-of-disentangled-representations-Paper.pdf:application/pdf}
}


@inproceedings{Louizos.2017,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
 title = {Causal effect inference with deep latent-variable models},
 booktitle = {NeurIPS},
 year = {2017},
 file = {NIPS-2017-causal-effect-inference-with-deep-latent-variable-models-Paper:Attachments/NIPS-2017-causal-effect-inference-with-deep-latent-variable-models-Paper.pdf:application/pdf}
}


@misc{Lu.28.05.2023,
 abstract = {Causal inference with observational studies often suffers from unmeasured confounding, yielding biased estimators based on the unconfoundedness assumption. Sensitivity analysis assesses how the causal conclusions change with respect to different degrees of unmeasured confounding. Most existing sensitivity analysis methods work well for specific types of estimation or testing strategies. We propose a flexible sensitivity analysis framework that can deal with commonly-used inverse probability weighting, outcome regression, and doubly robust estimators simultaneously. It is based on the well-known parametrization of the selection bias as comparisons of the observed and counterfactual outcomes conditional on observed covariates. It is attractive for practical use because it only requires simple modifications of the standard estimators. Moreover, it naturally extends to many other causal inference settings, including the average treatment effect on the treated units and studies with survival outcomes. We also develop an R package saci that implements our sensitivity analysis estimators.},
 author = {Lu, Sizhu and Ding, Peng},
 date = {28.05.2023},
 title = {Flexible sensitivity analysis for causal inference in observational  studies subject to unmeasured confounding},
 url = {http://arxiv.org/pdf/2305.17643v1},
 keywords = {Statistics - Methodology},
 file = {2305.17643:Attachments/2305.17643.pdf:application/pdf}
}


@inproceedings{Ma.2021,
 author = {Ma, Ymupu and Tresp, Volker},
 title = {Causal inference under networked interference and intervention policy enhancement},
 booktitle = {AISTATS},
 year = {2021},
 file = {2002.08506:Attachments/2002.08506.pdf:application/pdf;ma21c:Attachments/ma21c.pdf:application/pdf}
}


@article{MacKinnon.2007,
 abstract = {Mediating variables are prominent in psychological theory and research. A mediating variable transmits the effect of an independent variable on a dependent variable. Differences between mediating variables and confounders, moderators, and covariates are outlined. Statistical methods to assess mediation and modern comprehensive approaches are described. Future directions for mediation analysis are discussed.},
 author = {MacKinnon, David P. and Fairchild, Amanda J. and Fritz, Matthew S.},
 year = {2007},
 title = {Mediation analysis},
 keywords = {Analysis of Variance;Biomedical Research;Causality;Effect Modifier, Epidemiologic;Humans;Longitudinal Studies;Mathematical Computing;Models, Psychological;Models, Statistical;Psychological Theory;Treatment Outcome},
 pages = {593--614},
 volume = {58},
 issn = {0066-4308},
 journal = {Annual review of psychology},
 doi = {10.1146/annurev.psych.58.110405.085542},
 file = {MacKinnonetal{\_}AR2007:Attachments/MacKinnonetal{\_}AR2007.pdf:application/pdf}
}


@inproceedings{Madras.2018,
 author = {Madras, David and Creager, Elliot and Pitassi, Tohiann and Zemel, Richard},
 title = {Learning adversarially fair and transferable representations},
 booktitle = {ICML},
 year = {2018},
 file = {madras18a:Attachments/madras18a.pdf:application/pdf}
}


@inproceedings{Madry.2018,
 author = {Madry, Aleksander and Makelov, Aleksander and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
 title = {Towards deep learning models resistant to adversarial attacks},
 booktitle = {ICLR},
 year = {2018},
 file = {towards{\_}deep{\_}learning{\_}models{\_}r:Attachments/towards{\_}deep{\_}learning{\_}models{\_}r.pdf:application/pdf}
}


@misc{Mahajan.2022,
 abstract = {We study the problem of model selection in causal inference, specifically for the case of conditional average treatment effect (CATE) estimation under binary treatments. Unlike model selection in machine learning, we cannot use the technique of cross-validation here as we do not observe the counterfactual potential outcome for any data point. Hence, we need to design model selection techniques that do not explicitly rely on counterfactual data. As an alternative to cross-validation, there have been a variety of proxy metrics proposed in the literature, that depend on auxiliary nuisance models also estimated from the data (propensity score model, outcome regression model). However, the effectiveness of these metrics has only been studied on synthetic datasets as we can observe the counterfactual data for them. We conduct an extensive empirical analysis to judge the performance of these metrics, where we utilize the latest advances in generative modeling to incorporate multiple realistic datasets. We evaluate 9 metrics on 144 datasets for selecting between 415 estimators per dataset, including datasets that closely mimic real-world datasets. Further, we use the latest techniques from AutoML to ensure consistent hyperparameter selection for nuisance models for a fair comparison across metrics.},
 author = {Mahajan, Divyat and Mitliagkas, Ioannis and Neal, Brady and Syrgkanis, Vasilis},
 date = {2022},
 title = {Empirical analysis of model selection for heterogenous causal effect estimation},
 url = {http://arxiv.org/pdf/2211.01939v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Methodology},
 file = {2211.01939:Attachments/2211.01939.pdf:application/pdf}
}


@article{Manski.1990,
 author = {Manski, Charles F.},
 year = {1990},
 title = {Nonparametric bounds on treatment effects},
 pages = {319--323},
 volume = {80},
 number = {2},
 journal = {The American Economic Review},
 file = {2006592:Attachments/2006592.pdf:application/pdf}
}


@misc{MarieDavidian.,
 author = {{Marie Davidian}},
 title = {Double robustness in estimation of causal treatment effects},
 file = {Double Robustness in Estimation:Attachments/Double Robustness in Estimation.pdf:application/pdf}
}


@inproceedings{Marmarelis.2023,
 author = {Marmarelis, Myrl G. and Haddad, Elizabeth and Jesson, Andrew and Jahanshad, Jeda and Galstyan, Aram and {Ver Steeg}, Greg},
 title = {Partial identification of dose responses with hidden confounders},
 booktitle = {UAI},
 year = {2023},
 file = {249{\_}partial{\_}identification{\_}of{\_}dose:Attachments/249{\_}partial{\_}identification{\_}of{\_}dose.pdf:application/pdf}
}


@article{Marmarelis.2023b,
 abstract = {Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.},
 author = {Marmarelis, Myrl G. and {Ver Steeg}, Greg and Galstyan, Aram and Morstatter, Fred},
 year = {2023},
 title = {Tighter prediction intervals for causal outcomes under hidden confounding},
 url = {http://arxiv.org/pdf/2306.09520v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 volume = {arXiv:2306.09520},
 journal = {arXiv preprint},
 file = {2306.09520:Attachments/2306.09520.pdf:application/pdf}
}


@article{McGrath.2020,
 abstract = {Researchers are often interested in estimating the causal effects of sustained treatment strategies, i.e., of (hypothetical) interventions involving time-varying treatments. When using observational data, estimating those effects requires adjustment for confounding. However, conventional regression methods cannot appropriately adjust for confounding in the presence of treatment-confounder feedback. In contrast, estimators derived from Robins's g-formula may correctly adjust for confounding even if treatment-confounder feedback exists. The package gfoRmula implements in R one such estimator: the parametric g-formula. This estimator can be used to estimate the effects of binary or continuous time-varying treatments as well as contrasts defined by static or dynamic, deterministic, or random interventions, as well as interventions that depend on the natural value of treatment. The package accommodates survival outcomes as well as binary or continuous outcomes measured at the end of follow-up. This paper describes the gfoRmula package, along with motivating background, features, and examples.},
 author = {McGrath, Sean and Lin, Victoria and Zhang, Zilu and Petito, Lucia C. and Logan, Roger W. and Hern{\'a}n, Miguel A. and Young, Jessica G.},
 year = {2020},
 title = {gfoRmula: An R package for estimating the effects of sustained treatment strategies via the parametric G-formula},
 volume = {1},
 number = {3},
 journal = {Patterns},
 doi = {10.1016/j.patter.2020.100008}
}


@inproceedings{Melnychuk.2022,
 abstract = {Estimating counterfactual outcomes over time from observational data is relevant for many applications (e.g., personalized medicine). Yet, state-of-the-art methods build upon simple long short-term memory (LSTM) networks, thus rendering inferences for complex, long-range dependencies challenging. In this paper, we develop a novel Causal Transformer for estimating counterfactual outcomes over time. Our model is specifically designed to capture complex, long-range dependencies among time-varying confounders. For this, we combine three transformer subnetworks with separate inputs for time-varying covariates, previous treatments, and previous outcomes into a joint network with in-between cross-attentions. We further develop a custom, end-to-end training procedure for our Causal Transformer. Specifically, we propose a novel counterfactual domain confusion loss to address confounding bias: it aims to learn adversarial balanced representations, so that they are predictive of the next outcome but non-predictive of the current treatment assignment. We evaluate our Causal Transformer based on synthetic and real-world datasets, where it achieves superior performance over current baselines. To the best of our knowledge, this is the first work proposing transformer-based architecture for estimating counterfactual outcomes from longitudinal data.},
 author = {Melnychuk, Valentyn and Frauen, Dennis and Feuerriegel, Stefan},
 title = {Causal transformer for estimating counterfactual outcomes},
 url = {http://arxiv.org/pdf/2204.07258v2},
 keywords = {Computer Science - Learning;counterfactual inference;personalized medicine;Statistics - Machine Learning;transformer;treatment effect estimation},
 booktitle = {ICML},
 year = {2022},
 file = {2204.07258 (1):Attachments/2204.07258 (1).pdf:application/pdf}
}


@inproceedings{Melnychuk.2023,
 abstract = {Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a teacher flow for estimating nuisance parameters and (ii) a student flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective via a one-step bias correction for an efficient and doubly robust estimation of the student flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Interventional Normalizing Flows are expressive and highly effective, and scale well with both sample size and high-dimensional confounding. To the best of our knowledge, our Interventional Normalizing Flows are the first fully-parametric, deep learning method for density estimation of potential outcomes.},
 author = {Melnychuk, Valentyn and Frauen, Dennis and Feuerriegel, Stefan},
 title = {Normalizing flows for interventional density estimation},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Methodology},
 booktitle = {ICML},
 year = {2023},
 file = {2209.06203:Attachments/2209.06203.pdf:application/pdf}
}


@article{Melnychuk.2023b,
 abstract = {Counterfactual inference aims to answer retrospective ''what if'' questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual identification methods are special cases of our Curvature Sensitivity Model when the bound of the curvature is set to zero. We then propose an implementation of our Curvature Sensitivity Model in the form of a novel deep generative model, which we call Augmented Pseudo-Invertible Decoder. Our implementation employs (i) residual normalizing flows with (ii) variational augmentations. We empirically demonstrate the effectiveness of our Augmented Pseudo-Invertible Decoder. To the best of our knowledge, ours is the first partial identification model for Markovian structural causal models with continuous outcomes.},
 author = {Melnychuk, Valentyn and Frauen, Dennis and Feuerriegel, Stefan},
 year = {2023},
 title = {Partial counterfactual identification of continuous outcomes with a curvature sensitivity model},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 volume = {arXiv:2306.01424},
 journal = {arXiv preprint},
 file = {2306.01424:Attachments/2306.01424.pdf:application/pdf}
}


@article{Mitchell.2021,
 abstract = {Annu. Rev. Stat. Appl. 2021.8:141-163},
 author = {Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian},
 year = {2021},
 title = {Algorithmic fairness: Choices, assumptions, and definitions},
 keywords = {Algorithmic Fairness;decision theory;Machine learning;predictive modeling;statistical learning},
 pages = {141--163},
 volume = {8},
 number = {1},
 journal = {Annual Review of Statistics and Its Application},
 file = {annurev-statistics-042720-125902:Attachments/annurev-statistics-042720-125902.pdf:application/pdf}
}


@proceedings{ML4H.2016,
 year = {2016},
 title = {ML4H}
}


@proceedings{ML4H.2021,
 year = {2021},
 title = {ML4H}
}


@article{MolgaardNielsen.2020,
 abstract = {OBJECTIVES

A conceptually oriented preprocessing of a large number of potential prognostic factors may improve the development of a prognostic model. This study investigated whether various forms of conceptually oriented preprocessing or the preselection of established factors was superior to using all factors as input.

STUDY DESIGN AND SETTING

We made use of an existing project that developed two conceptually oriented subgroupings of low back pain patients. Based on the prediction of six outcome variables by seven statistical methods, this type of preprocessing was compared with medical experts' preselection of established factors, as well as using all 112 available baseline factors.

RESULTS

Subgrouping of patients was associated with low prognostic capacity. Applying a Lasso-based variable selection to all factors or to domain-specific principal component scores performed best. The preselection of established factors showed a good compromise between model complexity and prognostic capacity.

CONCLUSION

The prognostic capacity is hard to improve by means of a conceptually oriented preprocessing when compared to purely statistical approaches. However, a careful selection of already established factors combined in a simple linear model should be considered as an option when constructing a new prognostic rule based on a large number of potential prognostic factors.},
 author = {{Molgaard Nielsen}, Anne and Binding, Adrian and Ahlbrandt-Rains, Casey and Boeker, Martin and Feuerriegel, Stefan and Vach, Werner},
 year = {2020},
 title = {Exploring conceptual preprocessing for developing prognostic models: A case study in low back pain patients},
 pages = {27--34},
 volume = {122},
 journal = {Journal of Clinical Epidemiology},
 doi = {10.1016/j.jclinepi.2020.02.005}
}


@article{Mooij.2020,
 author = {Mooij, Joris M. and Magliacane, Sara and Claassen, Tom},
 year = {2020},
 title = {Joint causal inference from multiple contexts},
 pages = {1--108},
 volume = {21},
 journal = {Journal of Machine Learning Research},
 file = {17-123:Attachments/17-123.pdf:application/pdf}
}


@misc{Morzywolek.22.03.2023,
 abstract = {Motivated by applications in personalized medicine and individualized policy making, there is a growing interest in techniques for quantifying treatment effect heterogeneity in terms of the conditional average treatment effect (CATE). Some of the most prominent methods for CATE estimation developed in recent years are T-Learner, DR-Learner and R-Learner. The latter two were designed to improve on the former by being Neyman-orthogonal. However, the relations between them remain unclear, and likewise does the literature remain vague on whether these learners converge to a useful quantity or (functional) estimand when the underlying optimization procedure is restricted to a class of functions that does not include the CATE. In this article, we provide insight into these questions by discussing DR-learner and R-learner as special cases of a general class of Neyman-orthogonal learners for the CATE, for which we moreover derive oracle bounds. Our results shed light on how one may construct Neyman-orthogonal learners with desirable properties, on when DR-learner may be preferred over R-learner (and vice versa), and on novel learners that may sometimes be preferable to either of these. Theoretical findings are confirmed using results from simulation studies on synthetic data, as well as an application in critical care medicine.},
 author = {Morzywolek, Pawel and Decruyenaere, Johan and Vansteelandt, Stijn},
 date = {22.03.2023},
 title = {On a General Class of Orthogonal Learners for the Estimation of  Heterogeneous Treatment Effects},
 url = {http://arxiv.org/pdf/2303.12687v1},
 keywords = {Statistics - Methodology},
 file = {2303.12687:Attachments/2303.12687.pdf:application/pdf}
}


@article{Muandet.2021,
 author = {Muandet, Krikamol and Kanagawa, Montonobu and Saengkyongam, Sorawit and Marukatat, Sanparith},
 year = {2021},
 title = {Counterfactual mean embeddings},
 pages = {1--71},
 volume = {22},
 journal = {Journal of Machine Learning Research},
 file = {20-185:Attachments/20-185.pdf:application/pdf}
}


@article{Murphy.2003,
 abstract = {Summary. A dynamic treatment regime is a list of decision rules, one per time interval, for how the level of treatment will be tailored through time to an individual's changing status. The goal of ...},
 author = {Murphy, Susan A.},
 year = {2003},
 title = {Optimal dynamic treatment regimes},
 pages = {331--355},
 volume = {65},
 number = {2},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 doi = {10.1111/1467-9868.00389},
 file = {Murphy 2003 - Optimal dynamic treatment regimes:Attachments/Murphy 2003 - Optimal dynamic treatment regimes.pdf:application/pdf}
}


@inproceedings{Nabi.2018,
 abstract = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 author = {Nabi, Razieh and Shpitser, Ilya},
 title = {Fair inference on outcomes},
 keywords = {Knowledge Representation and Reasoning Track},
 booktitle = {AAAI},
 year = {2018},
 file = {11553-Article Text-15081-1-2-20201228:Attachments/11553-Article Text-15081-1-2-20201228.pdf:application/pdf}
}


@inproceedings{Nabi.2019,
 abstract = {Proceedings of the International Conference on Machine Learning 2019},
 author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
 title = {Learning optimal fair policies},
 keywords = {Algorithmic Fairness;causal inference;Dynamic Treatment Regimes;Fair Policies;Mediation Analysis;Optimal Policies;Policy Learning},
 booktitle = {ICML},
 year = {2019},
 file = {nabi19a:Attachments/nabi19a.pdf:application/pdf}
}


@inproceedings{Nabi.2022,
 author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
 title = {Optimal training of fair predictive models},
 booktitle = {CLeaR},
 year = {2022},
 file = {nabi22a:Attachments/nabi22a.pdf:application/pdf}
}


@article{Naimi.2017,
 abstract = {Robins' generalized methods (g methods) provide consistent estimates of contrasts (e.g. differences, ratios) of potential outcomes under a less restrictive set of identification conditions than do standard regression methods (e.g. linear, logistic, Cox regression). Uptake of g methods by epidemiologists has been hampered by limitations in understanding both conceptual and technical details. We present a simple worked example that illustrates basic concepts, while minimizing technical complications.},
 author = {Naimi, Ashley I. and Cole, Stephen R. and Kennedy, Edward H.},
 year = {2017},
 title = {An introduction to g methods},
 pages = {756--762},
 volume = {46},
 number = {2},
 journal = {International Journal of Epidemiology},
 doi = {10.1093/ije/dyw323}
}


@proceedings{NeurIPS.2000,
 year = {2000},
 title = {NeurIPS}
}


@book{NeurIPS.2015,
 year = {2015},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2016,
 year = {2016},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2017,
 year = {2017},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2018,
 year = {2018},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2019,
 year = {2019},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2020,
 year = {2020},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2021,
 year = {2021},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2022,
 year = {2022},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2023,
 year = {2023},
 title = {NeurIPS}
}


@article{Newey.2003,
 author = {Newey, Whitney K. and Powell, James L.},
 year = {2003},
 title = {Instrumental variable estimation of nonparametric models},
 pages = {1565--1578},
 volume = {71},
 number = {5},
 journal = {Econometrica},
 file = {npiv:Attachments/npiv.pdf:application/pdf}
}


@article{Nie.2021,
 abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: In both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.},
 author = {Nie, Xinkun and Wager, Stefan},
 year = {2021},
 title = {Quasi-oracle estimation of heterogeneous treatment effects},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 pages = {299--319},
 volume = {108},
 number = {2},
 issn = {0006-3444},
 journal = {Biometrika},
 file = {1712.04912:Attachments/1712.04912.pdf:application/pdf}
}


@inproceedings{Nie.2021b,
 abstract = {Motivated by the rising abundance of observational data with continuous treatments, we investigate the problem of estimating the average dose-response curve (ADRF). Available parametric methods are limited in their model space, and previous attempts in leveraging neural network to enhance model expressiveness relied on partitioning continuous treatment into blocks and using separate heads for each block; this however produces in practice discontinuous ADRFs. Therefore, the question of how to adapt the structure and training of neural network to estimate ADRFs remains open. This paper makes two important contributions. First, we propose a novel varying coefficient neural network (VCNet) that improves model expressiveness while preserving continuity of the estimated ADRF. Second, to improve finite sample performance, we generalize targeted regularization to obtain a doubly robust estimator of the whole ADRF curve.},
 author = {Nie, Lizhen and Ye, Mao and Liu, Qiang and Nicolae, Dan},
 title = {VCNet and functional targeted regularization for learning causal effects  of continuous treatments},
 url = {http://arxiv.org/pdf/2103.07861v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {ICLR},
 year = {2021},
 file = {2103.07861:Attachments/2103.07861.pdf:application/pdf}
}


@article{Nielsen.2017,
 abstract = {BACKGROUND

Heterogeneity in patients with low back pain (LBP) is well recognised and different approaches to subgrouping have been proposed. Latent Class Analysis (LCA) is a statistical technique that is increasingly being used to identify subgroups based on patient characteristics. However, as LBP is a complex multi-domain condition, the optimal approach when using LCA is unknown. Therefore, this paper describes the exploration of two approaches to LCA that may help improve the identification of clinically relevant and interpretable LBP subgroups.

METHODS

From 928 LBP patients consulting a chiropractor, baseline data were used as input to the statistical subgrouping. In a single-stage LCA, all variables were modelled simultaneously to identify patient subgroups. In a two-stage LCA, we used the latent class membership from our previously published LCA within each of six domains of health (activity, contextual factors, pain, participation, physical impairment and psychology) (first stage) as the variables entered into the second stage of the two-stage LCA to identify patient subgroups. The description of the results of the single-stage and two-stage LCA was based on a combination of statistical performance measures, qualitative evaluation of clinical interpretability (face validity) and a subgroup membership comparison.

RESULTS

For the single-stage LCA, a model solution with seven patient subgroups was preferred, and for the two-stage LCA, a nine patient subgroup model. Both approaches identified similar, but not identical, patient subgroups characterised by (i) mild intermittent LBP, (ii) recent severe LBP and activity limitations, (iii) very recent severe LBP with both activity and participation limitations, (iv) work-related LBP, (v) LBP and several negative consequences and (vi) LBP with nerve root involvement.

CONCLUSIONS

Both approaches identified clinically interpretable patient subgroups. The potential importance of these subgroups needs to be investigated by exploring whether they can be identified in other cohorts and by examining their possible association with patient outcomes. This may inform the selection of a preferred LCA approach.},
 author = {Nielsen, Anne Molgaard and Kent, Peter and Hestbaek, Lise and Vach, Werner and Kongsted, Alice},
 year = {2017},
 title = {Identifying subgroups of patients using latent class analysis: Should we use a single-stage or a two-stage approach? A methodological study using a cohort of patients with low back pain},
 pages = {57},
 volume = {18},
 number = {1},
 journal = {BMC Musculoskeletal Disorders},
 doi = {10.1186/s12891-017-1411-x},
 file = {Nielsen, Kent et al. 2017 - Identifying subgroups of patients using:Attachments/Nielsen, Kent et al. 2017 - Identifying subgroups of patients using.pdf:application/pdf}
}


@inproceedings{Nilforoshan.2022,
 abstract = {Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions \emph{almost always} -- in a measure theoretic sense -- result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity. Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of causal fairness.},
 author = {Nilforoshan, Hamed and Gaebler, Johann and Shroff, Ravi and Goel, Sharad},
 title = {Causal conceptions of fairness and their consequences},
 url = {http://arxiv.org/pdf/2207.05302v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computers and Society;Computer Science - Learning;ICML;Machine learning},
 booktitle = {ICML},
 year = {2022},
 file = {2207.05302:Attachments/2207.05302.pdf:application/pdf}
}


@misc{Nilforoshan.28.01.2023,
 abstract = {Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features{\~{}}(\emph{e.g.}, a patient's history), CaML is able to predict the personalized effects of novel interventions that do not exist at the time of training. Experimental results on real world datasets in large-scale medical claims and cell-line perturbations demonstrate the effectiveness of our approach. Most strikingly, CaML's zero-shot predictions outperform even strong baselines trained directly on data from the test interventions.},
 author = {Nilforoshan, Hamed and Moor, Michael and Roohani, Yusuf and Chen, Yining and {\v{S}}urina, Anja and Yasunaga, Michihiro and Oblak, Sara and Leskovec, Jure},
 date = {28.01.2023},
 title = {Zero-shot causal learning},
 url = {http://arxiv.org/pdf/2301.12292v2},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computers and Society;Computer Science - Human-Computer Interaction;Computer Science - Learning},
 file = {2301.12292:Attachments/2301.12292.pdf:application/pdf}
}


@proceedings{NIPS.2007,
 year = {2007},
 title = {NIPS}
}


@article{Nkonde.2019,
 author = {Nkonde, Mutale},
 year = {2019},
 title = {Is AI bias a corporate social responsibility issue?},
 journal = {Harvard Business Review},
 file = {ai{\_}bias:Attachments/ai{\_}bias.pdf:application/pdf}
}


@book{Nocedal.2006,
 author = {Nocedal, Jorge and Wright, Stephen J.},
 year = {2006},
 title = {Numerical optimization},
 keywords = {Mathematical optimization},
 address = {New York},
 edition = {2nd ed.},
 publisher = {Springer},
 isbn = {0387303030},
 series = {Springer series in operations research},
 file = {num{\_}optimization:Attachments/num{\_}optimization.pdf:application/pdf}
}


@article{Obermeyer.2019,
 abstract = {Science 2019.366:447-453},
 author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
 year = {2019},
 title = {Dissecting racial bias in an algorithm used to manage the health of populations},
 pages = {447--453},
 volume = {366},
 number = {6464},
 journal = {Science},
 doi = {10.1530/ey.17.12.7},
 file = {science.aax2342:Attachments/science.aax2342.pdf:application/pdf}
}


@article{Obermeyer.2019b,
 abstract = {Science 2019.366:447-453},
 author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
 year = {2019},
 title = {Dissecting racial bias in an algorithm used to manage the health of populations},
 pages = {447--453},
 volume = {366},
 number = {6404},
 journal = {Science},
 file = {science.aax2342 (1):Attachments/science.aax2342 (1).pdf:application/pdf}
}


@article{Ogburn.2015,
 abstract = {We consider estimation of the causal effect of a binary treatment on an outcome, conditionally on covariates, from observational studies or natural experiments in which there is a binary instrument for treatment. We describe a doubly robust, locally efficient estimator of the parameters indexing a model for the local average treatment effect conditionally on covariates V when randomization of the instrument is only true conditionally on a high dimensional vector of covariates X, possibly bigger than V. We discuss the surprising result that inference is identical to inference for the parameters of a model for an additive treatment effect on the treated conditionally on V that assumes no treatment-instrument interaction. We illustrate our methods with the estimation of the local average effect of participating in 401(k) retirement programs on savings by using data from the US Census Bureau's 1991 Survey of Income and Program Participation.},
 author = {Ogburn, Elizabeth L. and Rotnitzky, Andrea and Robins, James M.},
 year = {2015},
 title = {Doubly robust estimation of the local average treatment effect curve},
 pages = {373--396},
 volume = {77},
 number = {2},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 doi = {10.1111/rssb.12078},
 file = {Ogburn, Rotnitzky et al. 2015 - Doubly robust estimation:Attachments/Ogburn, Rotnitzky et al. 2015 - Doubly robust estimation.pdf:application/pdf}
}


@article{Ogburn.2019,
 abstract = {(This comment has been updated to respond to Wang and Blei's rejoinder [arXiv:1910.07320].)  The premise of the deconfounder method proposed in {\textquotedbl}Blessings of Multiple Causes{\textquotedbl} by Wang and Blei [arXiv:1805.06826], namely that a variable that renders multiple causes conditionally independent also controls for unmeasured multi-cause confounding, is incorrect. This can be seen by noting that no fact about the observed data alone can be informative about ignorability, since ignorability is compatible with any observed data distribution. Methods to control for unmeasured confounding may be valid with additional assumptions in specific settings, but they cannot, in general, provide a checkable approach to causal inference, and they do not, in general, require weaker assumptions than the assumptions that are commonly used for causal inference. While this is outside the scope of this comment, we note that much recent work on applying ideas from latent variable modeling to causal inference problems suffers from similar issues.},
 author = {Ogburn, Elizabeth L. and Shpitser, Ilya and Tchetgen, Eric J. Tchetgen},
 year = {2019},
 title = {Comment on {\textquotedbl}Blessings of multiple causes{\textquotedbl}},
 url = {http://arxiv.org/pdf/1910.05438v3},
 keywords = {Statistics - Machine Learning;Statistics - Methodology},
 journal = {arXiv preprint},
 file = {1910.05438 (1):Attachments/1910.05438 (1).pdf:application/pdf}
}


@article{Ogburn.2020,
 abstract = {We describe semiparametric estimation and inference for causal effects using observational data from a single social network. Our asymptotic result is the first to allow for dependence of each observation on a growing number of other units as sample size increases. While previous methods have generally implicitly focused on one of two possible sources of dependence among social network observations, we allow for both dependence due to transmission of information across network ties, and for dependence due to latent similarities among nodes sharing ties. We describe estimation and inference for new causal effects that are specifically of interest in social network settings, such as interventions on network ties and network structure. Using our methods to reanalyze the Framingham Heart Study data used in one of the most influential and controversial causal analyses of social network data, we find that after accounting for network structure there is no evidence for the causal effects claimed in the original paper.},
 author = {Ogburn, Elizabeth L. and Sofrygin, Oleg and Diaz, Ivan and {van der Laan}, Mark J.},
 year = {2020},
 title = {Causal inference for social network data},
 url = {http://arxiv.org/pdf/1705.08527v5},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 journal = {arXiv preprint},
 file = {1705.08527:Attachments/1705.08527.pdf:application/pdf}
}


@article{Okui.2012,
 author = {Okui, Ryo and Small, Dylan S. and Tan, Zhiqiang and Robins, James M.},
 year = {2012},
 title = {Doubly robust instrumental variable regression},
 pages = {173--205},
 volume = {22},
 number = {1},
 journal = {Statistica Sinica},
 file = {24310144:Attachments/24310144.pdf:application/pdf}
}


@inproceedings{Oprescu.2023,
 abstract = {Estimating heterogeneous treatment effects from observational data is a crucial task across many fields, helping policy and decision-makers take better actions. There has been recent progress on robust and efficient methods for estimating the conditional average treatment effect (CATE) function, but these methods often do not take into account the risk of hidden confounding, which could arbitrarily and unknowingly bias any causal estimate based on observational data. We propose a meta-learner called the B-Learner, which can efficiently learn sharp bounds on the CATE function under limits on the level of hidden confounding. We derive the B-Learner by adapting recent results for sharp and valid bounds of the average treatment effect (Dorn et al., 2021) into the framework given by Kallus {\&} Oprescu (2022) for robust and model-agnostic learning of distributional treatment effects. The B-Learner can use any function estimator such as random forests and deep neural networks, and we prove its estimates are valid, sharp, efficient, and have a quasi-oracle property with respect to the constituent estimators under more general conditions than existing methods. Semi-synthetic experimental comparisons validate the theoretical findings, and we use real-world data demonstrate how the method might be used in practice.},
 author = {Oprescu, Miruna and Dorn, Jacob and Ghoummaid, Marah and Jesson, Andrew and Kallus, Nathan and Shalit, Uri},
 title = {B-learner: Quasi-oracle bounds on heterogeneous causal effects under hidden confounding},
 url = {http://arxiv.org/pdf/2304.10577v1},
 keywords = {causal inference;Computer Science - Learning;heterogeneous treatment effect;hidden confounding;SENSITIVITY ANALYSIS;Statistics - Machine Learning},
 booktitle = {ICML},
 year = {2023},
 file = {2304.10577:Attachments/2304.10577.pdf:application/pdf}
}


@misc{Pace.01.06.2023,
 abstract = {A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive experiments and ablations the efficacy of our approach on a sepsis management benchmark, as well as on electronic health records. Our results suggest that nonidentifiable hidden confounding bias can be mitigated to improve offline RL solutions in practice.},
 author = {Pace, Aliz{\'e}e and Y{\`e}che, Hugo and Sch{\"o}lkopf, Bernhard and R{\"a}tsch, Gunnar and Tennenholtz, Guy},
 date = {01.06.2023},
 title = {Delphic Offline Reinforcement Learning under Nonidentifiable Hidden  Confounding},
 url = {http://arxiv.org/pdf/2306.01157v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning},
 file = {2306.01157:Attachments/2306.01157.pdf:application/pdf}
}


@inproceedings{Padh.2023,
 abstract = {Causal effect estimation is important for numerous tasks in the natural and social sciences. However, identifying effects is impossible from observational data without making strong, often untestable assumptions. We consider algorithms for the partial identification problem, bounding treatment effects from multivariate, continuous treatments over multiple possible causal models when unmeasured confounding makes identification impossible. We consider a framework where observable evidence is matched to the implications of constraints encoded in a causal model by norm-based criteria. This generalizes classical approaches based purely on generative models. Casting causal effects as objective functions in a constrained optimization problem, we combine flexible learning algorithms with Monte Carlo methods to implement a family of solutions under the name of stochastic causal programming. In particular, we present ways by which such constrained optimization problems can be parameterized without likelihood functions for the causal or the observed data model, reducing the computational and statistical complexity of the task.},
 author = {Padh, Kirtan and Zeitler, Jakob and Watson, David and Kusner, Matt and Silva, Ricardo and Kilbertus, Niki},
 title = {Stochastic causal programming for bounding treatment effects},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {CLeaR},
 year = {2023},
 file = {2202.10806:Attachments/2202.10806.pdf:application/pdf}
}


@article{Parbhoo.2021,
 abstract = {Estimating an individual's potential response to interventions from observational data is of high practical relevance for many domains, such as healthcare, public policy or economics. In this setting, it is often the case that combinations of interventions may be applied simultaneously, for example, multiple prescriptions in healthcare or different fiscal and monetary measures in economics. However, existing methods for counterfactual inference are limited to settings in which actions are not used simultaneously. Here, we present Neural Counterfactual Relation Estimation (NCoRE), a new method for learning counterfactual representations in the combination treatment setting that explicitly models cross-treatment interactions. NCoRE is based on a novel branched conditional neural representation that includes learnt treatment interaction modulators to infer the potential causal generative process underlying the combination of multiple treatments. Our experiments show that NCoRE significantly outperforms existing state-of-the-art methods for counterfactual treatment effect estimation that do not account for the effects of combining multiple treatments across several synthetic, semi-synthetic and real-world benchmarks.},
 author = {Parbhoo, Sonali and Bauer, Stefan and Schwab, Patrick},
 year = {2021},
 title = {{NC}o{RE}: Neural counterfactual representation learning for combinations of treatments},
 keywords = {Combination Treatments;Computer Science - Learning;counterfactual inference;Healthcare;Statistics - Machine Learning;Statistics - Methodology},
 volume = {arXiv:2103.11175},
 journal = {arXiv preprint},
 file = {2103.11175:Attachments/2103.11175.pdf:application/pdf}
}


@inproceedings{Parikh.2022,
 abstract = {Proceedings of the International Conference on Machine Learning 2022},
 author = {Parikh, Harsh and Vajao, Carlos and Xu, Louise and Tchetgen, Eric J. Tchetgen},
 title = {Validating causal inference methods},
 booktitle = {ICML},
 year = {2022},
 file = {parikh22a:Attachments/parikh22a.pdf:application/pdf}
}


@misc{Park.19.05.2023,
 abstract = {Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of \textit{what happens when one intervenes on a system}, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a \textit{causal space}, consisting of a probability space along with a collection of transition probability kernels, called \textit{causal kernels}, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.},
 author = {Park, Junhyung and Buchholz, Simon and Sch{\"o}lkopf, Bernhard and Muandet, Krikamol},
 date = {19.05.2023},
 title = {A Measure-Theoretic Axiomatisation of Causality},
 url = {http://arxiv.org/pdf/2305.17139v1},
 keywords = {Computer Science - Artificial Intelligence;Mathematics - Statistics;Statistics - Theory},
 file = {2305.17139 (1):Attachments/2305.17139 (1).pdf:application/pdf}
}


@inproceedings{Park.2021,
 author = {Park, Junhyung and Shalit, Uri and Sch{\"o}lkopf, Bernhard and Muandet, Krikamol},
 title = {Conditional distributional treatment effect with kernel conditional mean embeddings and {U}-{S}tatistic regression},
 booktitle = {ICML},
 year = {2021},
 file = {park21c:Attachments/park21c.pdf:application/pdf}
}


@inproceedings{Pawlowski.2020,
 abstract = {We formulate a general framework for building structural causal models (SCMs) with deep learning components. The proposed approach employs normalising flows and variational inference to enable tractable inference of exogenous noise variables - a crucial step for counterfactual inference that is missing from existing deep causal learning methods. Our framework is validated on a synthetic dataset built on MNIST as well as on a real-world medical dataset of brain MRI scans. Our experimental results indicate that we can successfully train deep SCMs that are capable of all three levels of Pearl's ladder of causation: association, intervention, and counterfactuals, giving rise to a powerful new approach for answering causal questions in imaging applications and beyond. The code for all our experiments is available at https://github.com/biomedia-mira/deepscm.},
 author = {Pawlowski, Nick and Castro, Daniel C. and Glocker, Ben},
 title = {Deep structural causal models for tractable counterfactual inference},
 url = {http://arxiv.org/pdf/2006.06485v2},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2020},
 file = {2006.06485:Attachments/2006.06485.pdf:application/pdf}
}


@book{Pearl.2009,
 abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
 author = {Pearl, Judea},
 year = {2009},
 title = {Causality},
 address = {New York City},
 publisher = {{Cambridge University Press}},
 isbn = {9780521895606},
 file = {Pearl 2009 - Causality:Attachments/Pearl 2009 - Causality.pdf:application/pdf}
}


@article{Pearl.2009b,
 author = {Pearl, Judea},
 year = {2009},
 title = {Causal inference in statistics: An overview},
 pages = {96--146},
 volume = {3},
 journal = {Statistics Surveys},
 doi = {10.1214/09-SS057},
 file = {09-SS057:Attachments/09-SS057.pdf:application/pdf}
}


@article{Pearl.2014,
 abstract = {This article reviews the foundations of causal mediation analysis and offers a general and transparent account of the conditions necessary for the identification of natural direct and indirect effects, thus facilitating a more informed judgment of the plausibility of these conditions in specific applications. I show that the conditions usually cited in the literature are overly restrictive and can be relaxed substantially without compromising identification. In particular, I show that natural effects can be identified by methods that go beyond standard adjustment for confounders, applicable to observational studies in which treatment assignment remains confounded with the mediator or with the outcome. These identification conditions can be validated algorithmically from the diagrammatic description of one's model and are guaranteed to produce unbiased results whenever the description is correct. The identification conditions can be further relaxed in parametric models, possibly including interactions, and permit one to compare the relative importance of several pathways, mediated by interdependent variables.},
 author = {Pearl, Judea},
 year = {2014},
 title = {Interpretation and identification of causal mediation},
 keywords = {Causality;Humans;Models, Statistical;Statistics as Topic},
 pages = {459--481},
 volume = {19},
 number = {4},
 journal = {Psychological methods},
 doi = {10.1037/a0036434},
 file = {r389:Attachments/r389.pdf:application/pdf}
}


@article{Persson.2021,
 abstract = {In response to the novel coronavirus disease (COVID-19), governments have introduced severe policy measures with substantial effects on human behavior. Here, we perform a large-scale, spatiotemporal analysis of human mobility during the COVID-19 epidemic. We derive human mobility from anonymized, aggregated telecommunication data in a nationwide setting (Switzerland; 10 February to 26 April 2020), consisting of $\sim$1.5 billion trips. In comparison to the same time period from 2019, human movement in Switzerland dropped by 49.1{\%}. The strongest reduction is linked to bans on gatherings of more than five people, which are estimated to have decreased mobility by 24.9{\%}, followed by venue closures (stores, restaurants, and bars) and school closures. As such, human mobility at a given day predicts reported cases 7 to 13 d ahead. A 1{\%} reduction in human mobility predicts a 0.88 to 1.11{\%} reduction in daily reported COVID-19 cases. When managing epidemics, monitoring human mobility via telecommunication data can support public decision makers in two ways. First, it helps in assessing policy impact; second, it provides a scalable tool for near real-time epidemic surveillance, thereby enabling evidence-based policies.},
 author = {Persson, Joel and Parie, Jurriaan F. and Feuerriegel, Stefan},
 year = {2021},
 title = {Monitoring the COVID-19 epidemic with nationwide telecommunication data},
 keywords = {COVID-19/epidemiology;Health Policy/trends;Humans;Population Surveillance;Public Health;SARS-CoV-2;Switzerland/epidemiology;Telecommunications/statistics {\&} numerical data;Travel/statistics {\&} numerical data},
 volume = {118},
 number = {26},
 journal = {Proceedings of the National Academy of Sciences of the United States of America},
 doi = {10.1073/pnas.2100664118},
 file = {pnas.2100664118:Attachments/pnas.2100664118.pdf:application/pdf}
}


@article{Peters.2016,
 abstract = {Journal of the Royal Statistical Society: Series B (Statistical Methodology) 2016.78:947-1012},
 author = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
 year = {2016},
 title = {Causal inference by using invariant prediction: Identification and confidence intervals},
 pages = {947--1012},
 number = {5},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 doi = {10.1111/rssb.12167},
 file = {Journal of the Royal Statistical Society  Series B  Statistical Methodology - 2016 - Peters - Causal inference by using:Attachments/Journal of the Royal Statistical Society  Series B  Statistical Methodology - 2016 - Peters - Causal inference by using.pdf:application/pdf}
}


@book{Peters.2017,
 author = {Peters, Jonas and Janzig, Dominik and Sch{\"o}lkopf, Bernhard},
 year = {2017},
 title = {Elements of causal inference: Foundations and learning algorithms},
 address = {Cambridge, Massachusetts},
 publisher = {{MIT Press}},
 isbn = {9780262037310},
 file = {11283:Attachments/11283.pdf:application/pdf}
}


@article{Peyre.,
 author = {Peyre, Gabriel},
 title = {Numerical optimal transport: Dual and semi-discrete},
 file = {SemiDiscrete:Attachments/SemiDiscrete.pdf:application/pdf}
}


@article{Pfister.2019,
 abstract = {Journal of the American Statistical Association, 2018. doi:10.1080/01621459.2018.1491403},
 author = {Pfister, Niklas and B{\"u}hlmann, Peter and Peters, Jonas},
 year = {2019},
 title = {Invariant causal prediction for sequential data},
 keywords = {Causal structure learning;Change point model;Chow statistic;Instantaneous causal effects;Monetary policy},
 pages = {1264--1276},
 volume = {114},
 number = {527},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2018.1491403},
 file = {01621459.2018:Attachments/01621459.2018.pdf:application/pdf}
}


@article{Plecko.2022,
 author = {Plecko, Drago and Bareinboim, Elias},
 year = {2022},
 title = {Causal fairness analysis},
 volume = {arxiv:2207.11385},
 journal = {arXiv preprint},
 file = {r90:Attachments/r90.pdf:application/pdf}
}


@article{Plecko.2023,
 abstract = {Proceedings of the International Conference on Machine Learning 2023},
 author = {Ple{\v{c}}ko, Drago and Bareinboim, Elias},
 year = {2023},
 title = {Reconciling predictive and statistical parity: A causal approach},
 keywords = {Algorithmic Fairness;causal inference},
 file = {r92:Attachments/r92.pdf:application/pdf}
}


@book{Polyanskiy.2022,
 author = {Polyanskiy, Yury and Wu, Yihong},
 year = {2022},
 title = {Information theory: From coding to learning},
 publisher = {{Cambridge University Press}},
 series = {Draft},
 file = {itbook-export:Attachments/itbook-export.pdf:application/pdf}
}


@article{Prentice.1989,
 abstract = {Statistics in Medicine 1989.8:431-440},
 author = {Prentice, Ross L.},
 year = {1989},
 title = {Surrogate endpoints in clinical trials: Definition and operational criteria},
 pages = {431--440},
 volume = {8},
 journal = {Statistics in Medicine},
 file = {SURROGATE ENDPOINTS IN CLINICAL TRIALS:Attachments/SURROGATE ENDPOINTS IN CLINICAL TRIALS.pdf:application/pdf}
}


@article{Qian.2011,
 abstract = {Because many illnesses show heterogeneous response to treatment, there is increasing interest in individualizing treatment to patients [11]. An individualized treatment rule is a decision rule that recommends treatment according to patient characteristics. We consider the use of clinical trial data in the construction of an individualized treatment rule leading to highest mean response. This is a difficult computational problem because the objective function is the expectation of a weighted indicator function that is non-concave in the parameters. Furthermore there are frequently many pretreatment variables that may or may not be useful in constructing an optimal individualized treatment rule yet cost and interpretability considerations imply that only a few variables should be used by the individualized treatment rule. To address these challenges we consider estimation based on l(1) penalized least squares. This approach is justified via a finite sample upper bound on the difference between the mean response due to the estimated individualized treatment rule and the mean response due to the optimal individualized treatment rule.},
 author = {Qian, Min and Murphy, Susan A.},
 year = {2011},
 title = {Performance guarantees for individualized treatment rules},
 keywords = {62H99;62J07;62P10;Decision making;l1-penalized least squares;value},
 pages = {1180--1210},
 volume = {39},
 number = {2},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 doi = {10.1214/10-AOS864},
 file = {10-AOS864:Attachments/10-AOS864.pdf:application/pdf}
}


@inproceedings{Qian.2021,
 author = {Qian, Zhaozhi and Zhang, Yao and Bica, Ioana and Wood, Angela M. and {van der Schaar}, Mihaela},
 title = {SyncTwin: Treatment effect estimation with longitudinal outcomes},
 booktitle = {NeurIPS},
 year = {2021},
 file = {SyncTwin Treatment Effect Estimation 2021:Attachments/SyncTwin Treatment Effect Estimation 2021.pdf:application/pdf}
}


@inproceedings{Qian.2021b,
 author = {Qian, Zhaozhi and Curth, Alicia and {van der Schaar}, Mihaela},
 title = {Estimating multi-cause treatment effects via single cause perturbation},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-estimating-multi-cause-treatment-effects-via-single-cause-perturbation-Paper:Attachments/NeurIPS-2021-estimating-multi-cause-treatment-effects-via-single-cause-perturbation-Paper.pdf:application/pdf}
}


@misc{Qiu.05.10.2022,
 abstract = {To infer the treatment effect for a single treated unit using panel data, synthetic control methods search for a linear combination of control units' outcomes that mimics the treated unit's pre-treatment outcome trajectory. This linear combination is subsequently used to impute the counterfactual outcomes of the treated unit had it not been treated in the post-treatment period, and used to estimate the treatment effect. Existing synthetic control methods rely on correctly modeling certain aspects of the counterfactual outcome generating mechanism and may require near-perfect matching of the pre-treatment trajectory. Inspired by proximal causal inference, we obtain two novel nonparametric identifying formulas for the average treatment effect for the treated unit: one is based on weighting, and the other combines models for the counterfactual outcome and the weighting function. We develop two GMM-based treatment effect estimators based on these two formulas. One new estimator is doubly robust: it is consistent and asymptotically normal if at least one of the outcome and weighting models is correctly specified. We demonstrate the performance of the methods via simulations and apply them to evaluate the effect of Sweden's carbon tax on CO$_2$ emission.},
 author = {Qiu, Hongxiang and Shi, Xu and Miao, Wang and Dobriban, Edgar and Tchetgen, Eric Tchetgen},
 date = {2022},
 title = {Doubly robust proximal synthetic controls},
 url = {http://arxiv.org/pdf/2210.02014v1},
 keywords = {Statistics - Methodology},
 file = {2210.02014:Attachments/2210.02014.pdf:application/pdf}
}


@book{Rasmussen.2008,
 author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
 year = {2008},
 title = {Gaussian processes for machine learning},
 address = {Cambridge, Mass.},
 edition = {3. print},
 publisher = {{MIT Press}},
 isbn = {9780262182539},
 series = {Adaptive computation and machine learning},
 file = {GP:Attachments/GP.pdf:application/pdf}
}


@article{Rea.2021,
 abstract = {Production and Operations Management 0.0:null-null},
 author = {Rea, David and Froehle, Craig and Masterson, Suzanne and Stettler, Brian and Fermann, Gregory and Pancioli, Arthur},
 year = {2021},
 title = {Unequal but fair: Incorporating distributive justice in operational allocation models},
 pages = {2304--2320},
 volume = {30},
 number = {7},
 journal = {Production and Operations Management},
 file = {poms.13369:Attachments/poms.13369.pdf:application/pdf}
}


@article{Rea.2021b,
 abstract = {Production and Operations Management 2021.30:2304-2320},
 author = {Rea, David and Froehle, Craig and Masterson, Suzanne and Stettler, Brian and Fermann, Gregory and Pancioli, Arthur},
 year = {2021},
 title = {Unequal but fair: {I}ncorporating distributive justice in operational allocation models},
 pages = {2304--2320},
 volume = {30},
 number = {7},
 journal = {Production and Operations Management},
 doi = {10.1111/poms.13369},
 file = {Reaetal-UnequalButFair-POM2021:Attachments/Reaetal-UnequalButFair-POM2021.pdf:application/pdf}
}


@inproceedings{Rezende.2015,
 abstract = {Proceedings of the International Conference on Machine Learning 2015},
 author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
 title = {Variational inference with normalizing flows},
 keywords = {Machine learning;normalizing flow;variational inference},
 booktitle = {ICML},
 year = {2015},
 file = {rezende15:Attachments/rezende15.pdf:application/pdf}
}


@article{Robins.1986,
 author = {Robins, James M.},
 year = {1986},
 title = {A new approach to causal inference in mortality studies with a sustained exposure period: Application to control of the healthy worker survivor effect},
 pages = {1393--1512},
 volume = {7},
 journal = {Mathematical Modelling},
 file = {1-s2.0-0270025586900886-main:Attachments/1-s2.0-0270025586900886-main.pdf:application/pdf}
}


@article{Robins.1994,
 author = {Robins, James M.},
 year = {1994},
 title = {Correcting for non-compliance in randomized trials using structural nested mean models},
 pages = {2379--2412},
 volume = {23},
 number = {8},
 issn = {0361-0926},
 journal = {Communications in Statistics - Theory and Methods},
 doi = {10.1080/03610929408831393},
 file = {Correcting for non compliance in randomized trials using structural nested mean models:Attachments/Correcting for non compliance in randomized trials using structural nested mean models.pdf:application/pdf}
}


@article{Robins.1999,
 author = {Robins, James M.},
 year = {1999},
 title = {Robust estimation in sequentially ignorable missing data and causal inference models},
 pages = {6--10},
 journal = {Proceedings of the American Statistical Association on Bayesian Statistical Science},
 file = {jsaprocpat1:Attachments/jsaprocpat1.pdf:application/pdf}
}


@article{Robins.2000,
 abstract = {In observational studies with exposures or treatments that vary over time, standard approaches for adjustment of confounding are biased when there exist time-dependent confounders that are also affected by previous treatment. This paper introduces marginal structural models, a new class of causal models that allow for improved adjustment of confounding in those situations. The parameters of a marginal structural model can be consistently estimated using a new class of estimators, the inverse-probability-of-treatment weighted estimators.},
 author = {Robins, James M. and Hern{\'a}n, Miguel A. and Brumback, Babette},
 year = {2000},
 title = {Marginal structural models and causal inference in epidemiology},
 keywords = {Anti-HIV Agents/therapeutic use;Causality;Confounding Factors, Epidemiologic;Epidemiologic Methods;HIV Infections/drug therapy/mortality;Humans;Models, Statistical;Risk Factors;Time Factors;Zidovudine/therapeutic use},
 pages = {550--560},
 volume = {11},
 number = {5},
 journal = {Epidemiology},
 doi = {10.1097/00001648-200009000-00011},
 file = {Marginal{\_}Structural{\_}Models{\_}and{\_}Causal{\_}Inference{\_}in.11:Attachments/Marginal{\_}Structural{\_}Models{\_}and{\_}Causal{\_}Inference{\_}in.11.pdf:application/pdf}
}


@article{Robins.2000b,
 author = {Robins, James M. and Rotnitzky, Andrea and Scharfstein, Daniel o.},
 year = {2000},
 title = {Sensitivity analysis for selection bias an unmeasured confounding in missing data and causal inference models},
 pages = {1--94},
 volume = {116},
 journal = {Statistical Models in Epidemiology, the Environment, and Clinical Trials},
 file = {sasbci-fnl:Attachments/sasbci-fnl.pdf:application/pdf}
}


@book{Robins.2009,
 author = {Robins, James M. and Hern{\'a}n, Miguel A.},
 year = {2009},
 title = {Estimation of the causal effects of time-varying exposures},
 keywords = {Longitudinal method;Multivariate analysis;Regression analysis},
 address = {Boca Raton},
 publisher = {{CRC Press}},
 isbn = {9781584886587},
 series = {Chapman {\&} Hall/CRC handbooks of modern statistical methods},
 file = {abc:Attachments/abc.pdf:application/pdf}
}


@article{Rodolfa.2021,
 abstract = {Growing use of machine learning in policy and social impact settings have raised concerns for fairness implications, especially for racial minorities. These concerns have generated considerable interest among machine learning and artificial intelligence researchers, who have developed new methods and established theoretical bounds for improving fairness, focusing on the source data, regularization and model training, or post-hoc adjustments to model scores. However, little work has studied the practical trade-offs between fairness and accuracy in real-world settings to understand how these bounds and methods translate into policy choices and impact on society. Our empirical study fills this gap by investigating the impact of mitigating disparities on accuracy, focusing on the common context of using machine learning to inform benefit allocation in resource-constrained programs across education, mental health, criminal justice, and housing safety. Here we describe applied work in which we find fairness-accuracy trade-offs to be negligible in practice. In each setting studied, explicitly focusing on achieving equity and using our proposed post-hoc disparity mitigation methods, fairness was substantially improved without sacrificing accuracy. This observation was robust across policy contexts studied, scale of resources available for intervention, time, and relative size of the protected groups. These empirical results challenge a commonly held assumption that reducing disparities either requires accepting an appreciable drop in accuracy or the development of novel, complex methods, making reducing disparities in these applications more practical.},
 author = {Rodolfa, Kit T. and Lamba, Hemank and Ghani, Rayid},
 year = {2021},
 title = {Empirical observation of negligible fairness-accuracy trade-offs in  machine learning for public policy},
 keywords = {Computer Science - Computers and Society;Computer Science - Learning},
 pages = {896--904},
 volume = {3},
 number = {10},
 journal = {Nature Machine Intelligence},
 file = {2012.02972:Attachments/2012.02972.pdf:application/pdf}
}


@article{Rosenbaum.1983,
 author = {Rosenbaum, Paul R. and Rubin, Donald B.},
 year = {1983},
 title = {The central role of the propensity score in observational studies for causal effects},
 pages = {41--55},
 volume = {70},
 number = {1},
 issn = {0006-3444},
 journal = {Biometrika},
 doi = {10.1093/biomet/70.1.41}
}


@article{Rosenbaum.1983b,
 author = {Rosenbaum, Paul R. and Rubin, Donald B.},
 year = {1983},
 title = {Assessing sensitivity to an unobserved binary covariate in an observational Study with binary outcome},
 pages = {212--218},
 volume = {45},
 number = {2},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 file = {2345524:Attachments/2345524.pdf:application/pdf}
}


@article{Rosenbaum.1987,
 author = {Rosenbaum, Paul R.},
 year = {1987},
 title = {Sensitivity analysis for certain permutation inferences in matched observational studies},
 pages = {13--26},
 volume = {74},
 number = {1},
 issn = {0006-3444},
 journal = {Biometrika},
 file = {2336017:Attachments/2336017.pdf:application/pdf}
}


@inproceedings{Rubenstein.2017,
 author = {Rubenstein, Paul K. and Weichwald, Sebastian and Bongers, Stephan and Mooij, Joris M. and Janzig, Dominik and Grosse-Wentrup, Moritz and Sch{\"o}lkopf, Bernhard},
 title = {Causal consistency of structural equation models},
 booktitle = {UAI},
 year = {2017},
 file = {11:Attachments/11.pdf:application/pdf}
}


@article{Rubin.1974,
 author = {Rubin, Donald B.},
 year = {1974},
 title = {Estimating causal effects of treatments in randomized and nonrandomized studies},
 pages = {688--701},
 volume = {66},
 number = {5},
 issn = {0022-0663},
 journal = {Journal of Educational Psychology},
 doi = {10.1037/h0037350}
}


@article{Rubin.1978,
 author = {Rubin, Donald B.},
 year = {1978},
 title = {Bayesian inference for causal effects: The role of randomization},
 keywords = {Potential outcomes},
 pages = {34--58},
 volume = {6},
 number = {1},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 doi = {10.1214/aos/1176344064},
 file = {1176344064:Attachments/1176344064.pdf:application/pdf}
}


@phdthesis{Rydgaard.,
 author = {Rydgaard, Helene},
 title = {Targeted causal learning for longitudinal data},
 file = {thesis-master:Attachments/thesis-master.pdf:application/pdf}
}


@misc{Rytgaard.2021,
 abstract = {This paper studies the generalization of the targeted minimum loss-based estimation (TMLE) framework to estimation of effects of time-varying interventions in settings where both interventions, covariates, and outcome can happen at subject-specific time-points on an arbitrarily fine time-scale. TMLE is a general template for constructing asymptotically linear substitution estimators for smooth low-dimensional parameters in infinite-dimensional models. Existing longitudinal TMLE methods are developed for data where observations are made on a discrete time-grid.  We consider a continuous-time counting process model where intensity measures track the monitoring of subjects, and focus on a low-dimensional target parameter defined as the intervention-specific mean outcome at the end of follow-up. To construct our TMLE algorithm for the given statistical estimation problem we derive an expression for the efficient influence curve and represent the target parameter as a functional of intensities and conditional expectations. The high-dimensional nuisance parameters of our model are estimated and updated in an iterative manner according to separate targeting steps for the involved intensities and conditional expectations.  The resulting estimator solves the efficient influence curve equation. We state a general efficiency theorem and describe a highly adaptive lasso estimator for nuisance parameters that allows us to establish asymptotic linearity and efficiency of our estimator under minimal conditions on the underlying statistical model.},
 author = {Rytgaard, Helene C. and Gerds, Thomas A. and {van der Laan}, Mark J.},
 date = {2021},
 title = {Continuous-time targeted minimum loss-based estimation of  intervention-specific mean outcomes},
 url = {http://arxiv.org/pdf/2105.02088v1},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 file = {2105.02088 (1):Attachments/2105.02088 (1).pdf:application/pdf}
}


@book{Sammut.2010,
 author = {Sammut, Claude and Webb, Geoffrey I.},
 year = {2010},
 title = {Encyclopedia of Machine Learning},
 address = {Boston, MA},
 publisher = {{Springer US}},
 isbn = {978-0-387-30768-8},
 doi = {10.1007/978-0-387-30164-8},
 file = {Aggarwal2010{\_}ReferenceWorkEntry{\_}GraphClustering:Attachments/Aggarwal2010{\_}ReferenceWorkEntry{\_}GraphClustering.pdf:application/pdf}
}


@inproceedings{Schulam.2017,
 author = {Schulam, Peter and Saria, Suchi},
 title = {Reliable decision support using counterfactual models},
 booktitle = {NeurIPS},
 year = {2017},
 file = {Schulam, Saria 2017 - Reliable Decision Support using Counterfactual (2):Attachments/Schulam, Saria 2017 - Reliable Decision Support using Counterfactual (2).pdf:application/pdf}
}


@inproceedings{Schwab.2020,
 abstract = {Estimating what would be an individual's potential response to varying levels of exposure to a treatment is of high practical relevance for several important fields, such as healthcare, economics and public policy. However, existing methods for learning to estimate counterfactual outcomes from observational data are either focused on estimating average dose-response curves, or limited to settings with only two treatments that do not have an associated dosage parameter. Here, we present a novel machine-learning approach towards learning counterfactual representations for estimating individual dose-response curves for any number of treatments with continuous dosage parameters with neural networks. Building on the established potential outcomes framework, we introduce performance metrics, model selection criteria, model architectures, and open benchmarks for estimating individual dose-response curves. Our experiments show that the methods developed in this work set a new state-of-the-art in estimating individual dose-response.},
 author = {Schwab, Patrick and Linhardt, Lorenz and Bauer, Stefan and Buhmann, Joachim M. and Karlen, Walter},
 title = {Learning counterfactual representations for estimating individual  dose-response curves},
 url = {http://arxiv.org/pdf/1902.00981v3},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 pages = {5612--5619},
 volume = {34},
 booktitle = {AAAI},
 year = {2020},
 file = {1902.00981:Attachments/1902.00981.pdf:application/pdf}
}


@inproceedings{Seguy.2018,
 abstract = {This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a \textit{Monge map} as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. This parameterization allows generalization of the mapping outside the support of the input measure. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT plan and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.},
 author = {Seguy, Vivien and Damodaran, Bharath Bhushan and Flamary, R{\'e}mi and Courty, Nicolas and Rolet, Antoine and Blondel, Mathieu},
 title = {Large-scale optimal transport and mapping estimation},
 keywords = {Statistics - Machine Learning},
 booktitle = {ICLR},
 year = {2018},
 file = {1711.02283:Attachments/1711.02283.pdf:application/pdf}
}


@article{Semenova.2021,
 abstract = {This paper provides estimation and inference methods for the best linear predictor (approximation) of a structural function, such as conditional average structural and treatment effects, and structural derivatives, based on modern machine learning (ML) tools. We represent this structural function as a conditional expectation of an unbiased signal that depends on a nuisance parameter, which we estimate by modern machine learning techniques. We first adjust the signal to make it insensitive (Neyman-orthogonal) with respect to the first-stage regularization bias. We then project the signal onto a set of basis functions, growing with sample size, which gives us the best linear predictor of the structural function. We derive a complete set of results for estimation and simultaneous inference on all parameters of the best linear predictor, conducting inference by Gaussian bootstrap. When the structural function is smooth and the basis is sufficiently rich, our estimation and inference result automatically targets this function. When basis functions are group indicators, the best linear predictor reduces to group average treatment/structural effect, and our inference automatically targets these parameters. We demonstrate our method by estimating uniform confidence bands for the average price elasticity of gasoline demand conditional on income.},
 author = {Semenova, Vira and Chernozhukov, Victor},
 year = {2021},
 title = {Debiased machine learning of conditional average treatment effects and  other causal functions},
 keywords = {Statistics - Machine Learning;Statistics - Methodology},
 pages = {264--289},
 volume = {24},
 number = {2},
 issn = {1368-4221},
 journal = {The Econometrics Journal},
 file = {1702.06240:Attachments/1702.06240.pdf:application/pdf}
}


@inproceedings{Shalit.2017,
 abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a {\textquotedbl}balanced{\textquotedbl} representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
 author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
 title = {Estimating individual treatment effect: Generalization bounds and  algorithms},
 keywords = {causal effects;Computer Science - Artificial Intelligence;Computer Science - Learning;counterfactual inference;Statistics - Machine Learning},
 booktitle = {ICML},
 year = {2017},
 file = {Individual{\_}treatment{\_}generalization{\_}bounds:Attachments/Individual{\_}treatment{\_}generalization{\_}bounds.pdf:application/pdf}
}


@inproceedings{Shi.2019,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Shi, Claudia and Blei, David M. and Veitch, Victor},
 title = {Adapting neural networks for the estimation of treatment effects},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-adapting-neural-networks-for-the-estimation-of-treatment-effects-Paper:Attachments/NeurIPS-2019-adapting-neural-networks-for-the-estimation-of-treatment-effects-Paper.pdf:application/pdf}
}


@inproceedings{Shi.2021,
 author = {Shi, Claudia and Veitch, Victor and Blei, David M.},
 title = {Invariant representation learning for treatment effect estimation},
 booktitle = {UAI},
 year = {2021},
 file = {shi21a:Attachments/shi21a.pdf:application/pdf}
}


@inproceedings{Shi.2022,
 author = {Shi, Claudia and Sridhar, Dhanya and Misra, Vishal and Blei, David M.},
 title = {On the assumptions on synthetic control methods},
 booktitle = {AISTATS},
 year = {2022},
 file = {shi22b:Attachments/shi22b.pdf:application/pdf}
}


@inproceedings{Shpister.2007,
 abstract = {UAI-07 Conference},
 author = {Shpitser, Ilya and Pearl, Judea},
 title = {What counterfactuals can be tested},
 booktitle = {UAI},
 year = {2007},
 file = {1206.5294:Attachments/1206.5294.pdf:application/pdf}
}


@inproceedings{Shpister.2009,
 author = {Shpitser, Ilya and Pearl, Judea},
 title = {Effects of treatment on the treated: Identification and generalization},
 booktitle = {UAI},
 year = {2009},
 file = {1205.2615:Attachments/1205.2615.pdf:application/pdf}
}


@inproceedings{Siddique.2020,
 abstract = {Proceedings of the International Conference on Machine Learning 2020},
 author = {Siddique, Umer and Weng, Paul and Zimmer, Matthieu},
 title = {Learning fair policies in multiobjective (deep) reinforcement learning with average and discounted rewards},
 keywords = {ICML;Machine learning},
 booktitle = {ICML},
 year = {2020},
 file = {siddique20a:Attachments/siddique20a.pdf:application/pdf}
}


@inproceedings{Singh.2018,
 abstract = {-  Information systems  -{\textgreater}  Probabilistic retrieval models; Retrieval effectiveness; Presentation of retrieval results;},
 author = {Singh, Ashudeep and Joachims, Thorsten},
 title = {Fairness of exposure in rankings},
 keywords = {algorithmic bias;equal opportunity;fairness;fairness in rankings;position bias},
 booktitle = {KDD},
 year = {2018},
 file = {3219819.3220088:Attachments/3219819.3220088.pdf:application/pdf}
}


@inproceedings{Singh.2019,
 author = {Singh, Rahul and Sahani, Maneesh and Gretton, Arthur},
 title = {Kernel instrumental variable regression},
 booktitle = {NeurIPS},
 year = {2019},
 file = {main (1):Attachments/main (1).pdf:application/pdf;kiv{\_}appendix:Attachments/kiv{\_}appendix.pdf:application/pdf}
}


@article{Singh.2019b,
 abstract = {We study low dimensional complier parameters that are identified using a binary instrumental variable $Z$, which is valid conditional on a possibly high dimensional vector of covariates $X$. We characterize the doubly robust moment function for the entire class of complier parameters defined by Abadie (2003) by combining two classic formulations: the Wald formula and the {\$}$\backslash$kappa{\$} weight. In particular, we reinterpret the {\$}$\backslash$kappa{\$} weight as the Riesz representer to the Wald formula, which appears to be a new insight. The main result includes new cases such as average complier characteristics. We use the main result to propose a hypothesis test, free of functional form restrictions, to evaluate (i) whether two different instruments induce compliers with the same observable characteristics on average, and (ii) whether compliers have observable characteristics that are the same as the full population on average. By developing this hypothesis test, we equip empirical researchers with a new robustness check.},
 author = {Singh, Rahul and Sun, Liyang},
 year = {2019},
 title = {Double robustness for complier parameters and a semiparametric test for complier characteristics},
 url = {http://arxiv.org/pdf/1909.05244v6},
 keywords = {Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 journal = {arXiv preprint},
 file = {1909.05244:Attachments/1909.05244.pdf:application/pdf}
}


@inproceedings{Singh.2019c,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Singh, Ashudeep and Joachims, Thorsten},
 title = {Policy learning for fairness in ranking},
 booktitle = {NeurIPS},
 year = {2019},
 file = {singh{\_}joachims{\_}19a:Attachments/singh{\_}joachims{\_}19a.pdf:application/pdf}
}


@article{Smith.2023,
 abstract = {Marketing Science 0.0},
 author = {Smith, Adam N. and Seiler, Stephan and Aggarwal, Ishant},
 year = {2023},
 title = {Optimal price targeting},
 keywords = {choice models;heterogeneity;Machine learning;personalization;targeting},
 journal = {Marketing Science},
 file = {mksc.2022.1387:Attachments/mksc.2022.1387.pdf:application/pdf}
}


@inproceedings{Soleimani.2017,
 abstract = {Treatment effects can be estimated from observational data as the difference in potential outcomes. In this paper, we address the challenge of estimating the potential outcome when treatment-dose levels can vary continuously over time. Further, the outcome variable may not be measured at a regular frequency. Our proposed solution represents the treatment response curves using linear time-invariant dynamical systems---this provides a flexible means for modeling response over time to highly variable dose curves. Moreover, for multivariate data, the proposed method: uncovers shared structure in treatment response and the baseline across multiple markers; and, flexibly models challenging correlation structure both across and within signals over time. For this, we build upon the framework of multiple-output Gaussian Processes. On simulated and a challenging clinical dataset, we show significant gains in accuracy over state-of-the-art models.},
 author = {Soleimani, Hossein and Subbaswamy, Adarsh and Saria, Suchi},
 title = {Treatment-response models for counterfactual reasoning with  continuous-time, continuous-valued interventions},
 booktitle = {UAI},
 year = {2017},
 file = {Soleimani, Subbaswamy et al. 07.04.2017 - Treatment-Response Models for Counterfactual Reasoning:Attachments/Soleimani, Subbaswamy et al. 07.04.2017 - Treatment-Response Models for Counterfactual Reasoning.pdf:application/pdf}
}


@article{Soriano.2023,
 abstract = {Assessing sensitivity to unmeasured confounding is an important step in observational studies, which typically estimate effects under the assumption that all confounders are measured. In this paper, we develop a sensitivity analysis framework for balancing weights estimators, an increasingly popular approach that solves an optimization problem to obtain weights that directly minimizes covariate imbalance. In particular, we adapt a sensitivity analysis framework using the percentile bootstrap for a broad class of balancing weights estimators. We prove that the percentile bootstrap procedure can, with only minor modifications, yield valid confidence intervals for causal effects under restrictions on the level of unmeasured confounding. We also propose an amplification to allow for interpretable sensitivity parameters in the balancing weights framework. We illustrate our method through extensive real data examples.},
 author = {Soriano, Dan and Ben-Michael, Eli and Bickel, Peter J. and Feller, Avi and Pimentel, Samuel D.},
 year = {2023},
 title = {Interpretable sensitivity analysis for balancing weights},
 url = {http://arxiv.org/pdf/2102.13218v3},
 keywords = {Statistics - Methodology},
 pages = {113},
 volume = {93},
 number = {1},
 issn = {0964-1998},
 journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
 doi = {10.1093/jrsssa/qnad032},
 file = {2102.13218:Attachments/2102.13218.pdf:application/pdf}
}


@article{Stone.1980,
 author = {Stone, Charles J.},
 year = {1980},
 title = {Optimal rates of convergence for nonparametric estimators},
 volume = {8},
 number = {6},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 doi = {10.1214/aos/1176345206}
}


@article{Sverdrup.2023,
 abstract = {Efficiently and flexibly estimating treatment effect heterogeneity is an important task in a wide variety of settings ranging from medicine to marketing, and there are a considerable number of promising conditional average treatment effect estimators currently available. These, however, typically rely on the assumption that the measured covariates are enough to justify conditional exchangeability. We propose the P-learner, motivated by the R-learner, a tailored two-stage loss function for learning heterogeneous treatment effects in settings where exchangeability given observed covariates is an implausible assumption, and we wish to rely on proxy variables for causal inference. Our proposed estimator can be implemented by off-the-shelf loss-minimizing machine learning methods, which in the case of kernel regression satisfies an oracle bound on the estimated error as long as the nuisance components are estimated reasonably well.},
 author = {Sverdrup, Erik and Cui, Yifan},
 year = {2023},
 title = {Proximal causal learning of heterogeneous treatment effects},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 volume = {arXiv:2301.10913},
 journal = {arXiv preprint},
 file = {2301.10913:Attachments/2301.10913.pdf:application/pdf}
}


@inproceedings{Syrgkanis.2019,
 abstract = {We consider the estimation of heterogeneous treatment effects with arbitrary machine learning methods in the presence of unobserved confounders with the aid of a valid instrument. Such settings arise in A/B tests with an intent-to-treat structure, where the experimenter randomizes over which user will receive a recommendation to take an action, and we are interested in the effect of the downstream action. We develop a statistical learning approach to the estimation of heterogeneous effects, reducing the problem to the minimization of an appropriate loss function that depends on a set of auxiliary models (each corresponding to a separate prediction task). The reduction enables the use of all recent algorithmic advances (e.g. neural nets, forests). We show that the estimated effect model is robust to estimation errors in the auxiliary models, by showing that the loss satisfies a Neyman orthogonality criterion. Our approach can be used to estimate projections of the true effect model on simpler hypothesis spaces. When these spaces are parametric, then the parameter estimates are asymptotically normal, which enables construction of confidence sets. We applied our method to estimate the effect of membership on downstream webpage engagement on TripAdvisor, using as an instrument an intent-to-treat A/B test among 4 million TripAdvisor users, where some users received an easier membership sign-up process. We also validate our method on synthetic data and on public datasets for the effects of schooling on income.},
 author = {Syrgkanis, Vasilis and Lei, Victor and Oprescu, Miruna and Hei, Maggie and Battocchi, Keith and Lewis, Greg},
 title = {Machine learning estimation of heterogeneous treatment effects with instruments},
 keywords = {Computer Science - Learning;Statistics - Applications;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-machine-learning-estimation-of-heterogeneous-treatment-effects-with-instruments-Paper:Attachments/NeurIPS-2019-machine-learning-estimation-of-heterogeneous-treatment-effects-with-instruments-Paper.pdf:application/pdf}
}


@article{T.Bluemlein.,
 author = {{T. Bluemlein} and {Joel Persson} and {and Stefan Feuerriegel}},
 title = {Learning Optimal Dynamic Treatment Regimes Using Causal Tree Methods with Application to Medicine},
 journal = {arXiv preprint},
 file = {DTR:Attachments/DTR.pdf:application/pdf}
}


@article{Tan.2006,
 author = {Tan, Zhiqiang},
 year = {2006},
 title = {A distributional approach for causal inference using propensity scores},
 keywords = {causal inference;CONTROL VARIATE;NONPARAMETRIC LIKELIHOOD;OBSERVATIONAL STUDY;Propensity Score;SENSITIVITY ANALYSIS},
 pages = {1619--1637},
 volume = {101},
 number = {476},
 journal = {Journal of the American Statistical Association},
 file = {A Distributional Approach for Causal Inference Using Propensity Scores:Attachments/A Distributional Approach for Causal Inference Using Propensity Scores.pdf:application/pdf}
}


@article{Tchetgen.2012,
 abstract = {Whilst estimation of the marginal (total) causal effect of a point exposure on an outcome is arguably the most common objective of experimental and observational studies in the health and social sciences, in recent years, investigators have also become increasingly interested in mediation analysis. Specifically, upon evaluating the total effect of the exposure, investigators routinely wish to make inferences about the direct or indirect pathways of the effect of the exposure not through or through a mediator variable that occurs subsequently to the exposure and prior to the outcome. Although powerful semiparametric methodologies have been developed to analyze observational studies, that produce double robust and highly efficient estimates of the marginal total causal effect, similar methods for mediation analysis are currently lacking. Thus, this paper develops a general semiparametric framework for obtaining inferences about so-called marginal natural direct and indirect causal effects, while appropriately accounting for a large number of pre-exposure confounding factors for the exposure and the mediator variables. Our analytic framework is particularly appealing, because it gives new insights on issues of efficiency and robustness in the context of mediation analysis. In particular, we propose new multiply robust locally efficient estimators of the marginal natural indirect and direct causal effects, and develop a novel double robust sensitivity analysis framework for the assumption of ignorability of the mediator variable.},
 author = {Tchetgen, Eric J. Tchetgen and Shpitser, Ilya},
 year = {2012},
 title = {Semiparametric theory for causal mediation analysis: Efficiency bounds, multiple robustness, and sensitivity analysis},
 pages = {1816--1845},
 volume = {40},
 number = {3},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 doi = {10.1214/12-AOS990},
 file = {nihms746449:Attachments/nihms746449.pdf:application/pdf}
}


@article{TchetgenTchetgen.2012,
 abstract = {Interference is said to be present when the exposure or treatment received by one individual may affect the outcomes of other individuals. Such interference can arise in settings in which the outcomes of the various individuals come about through social interactions. When interference is present, causal inference is rendered considerably more complex, and the literature on causal inference in the presence of interference has just recently begun to develop. In this article we summarise some of the concepts and results from the existing literature and extend that literature in considering new results for finite sample inference, new inverse probability weighting estimators in the presence of interference and new causal estimands of interest.},
 author = {{Tchetgen Tchetgen}, Eric J. and VanderWeele, Tyler J.},
 year = {2012},
 title = {On causal inference in the presence of interference},
 keywords = {Biomedical Research/statistics {\&} numerical data;Causality;Communicable Diseases/epidemiology;Data Interpretation, Statistical;Humans;Models, Statistical;Randomized Controlled Trials as Topic/statistics {\&} numerical data;Vaccination/statistics {\&} numerical data},
 pages = {55--75},
 volume = {21},
 number = {1},
 journal = {Statistical Methods in Medical Research},
 doi = {10.1177/0962280210386779},
 file = {nihms632455:Attachments/nihms632455.pdf:application/pdf}
}


@inproceedings{Thomas.2016,
 abstract = {Proceedings of the International Conference on Machine Learning 2016},
 author = {Thomas, Phillip S. and Brunskill, Emma},
 title = {Data-efficient off-Policy policy evaluation for reinforcement Learning},
 keywords = {complex return;MAGIC estimator;MMSE return;off-policy;policy evaluation;Reinforcement Learning},
 booktitle = {ICML},
 year = {2016},
 file = {thomasa16:Attachments/thomasa16.pdf:application/pdf}
}


@misc{Tigas.03.03.2022,
 abstract = {Causal discovery from observational and interventional data is challenging due to limited data and non-identifiability: factors that introduce uncertainty in estimating the underlying structural causal model (SCM). Selecting experiments (interventions) based on the uncertainty arising from both factors can expedite the identification of the SCM. Existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the SCM or select only the intervention target. This work incorporates recent advances in Bayesian causal discovery into the Bayesian optimal experimental design framework, allowing for active causal discovery of large, nonlinear SCMs while selecting both the interventional target and the value. We demonstrate the performance of the proposed method on synthetic graphs (Erdos-R\`enyi, Scale Free) for both linear and nonlinear SCMs as well as on the \emph{in-silico} single-cell gene regulatory network dataset, DREAM.},
 author = {Tigas, Panagiotis and Annadani, Yashas and Jesson, Andrew and Sch{\"o}lkopf, Bernhard and Gal, Yarin and Bauer, Stefan},
 date = {03.03.2022},
 title = {Interventions, Where and How? Experimental Design for Causal Models at  Scale},
 url = {http://arxiv.org/pdf/2203.02016v3},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Statistics - Machine Learning},
 file = {2203.02016:Attachments/2203.02016.pdf:application/pdf}
}


@article{Tomkins.2020,
 abstract = {In mobile health (mHealth), reinforcement learning algorithms that adapt to one's context without learning personalized policies might fail to distinguish between the needs of individuals. Yet the high amount of noise due to the in situ delivery of mHealth interventions can cripple the ability of an algorithm to learn when given access to only a single user's data, making personalization challenging. We present IntelligentPooling, which learns personalized policies via an adaptive, principled use of other users' data. We show that IntelligentPooling achieves an average of 26{\%} lower regret than state-of-the-art across all generative models. Additionally, we inspect the behavior of this approach in a live clinical trial, demonstrating its ability to learn from even a small group of users.},
 author = {Tomkins, Sabina and Liao, Peng and Klasnja, Predrag and Yeung, Serena and Murphy, Susan},
 title = {Rapidly personalizing mobile health treatment policies with limited data},
 url = {http://arxiv.org/pdf/2002.09971v1},
 keywords = {Computer Science - Computers and Society;Computer Science - Learning;Statistics - Machine Learning},
 journal = {arXiv preprint},
 file = {2002.09971:Attachments/2002.09971.pdf:application/pdf}
}


@article{Tran.2019,
 abstract = {A number of sophisticated estimators of longitudinal effects have been proposed for estimating the intervention-specific mean outcome. However, there is a relative paucity of research comparing these methods directly to one another. In this study, we compare various approaches to estimating a causal effect in a longitudinal treatment setting using both simulated data and data measured from a human immunodeficiency virus cohort. Six distinct estimators are considered: (i) an iterated conditional expectation representation, (ii) an inverse propensity weighted method, (iii) an augmented inverse propensity weighted method, (iv) a double robust iterated conditional expectation estimator, (v) a modified version of the double robust iterated conditional expectation estimator, and (vi) a targeted minimum loss-based estimator. The details of each estimator and its implementation are presented along with nuisance parameter estimation details, which include potentially pooling the observed data across all subjects regardless of treatment history and using data adaptive machine learning algorithms. Simulations are constructed over six time points, with each time point steadily increasing in positivity violations. Estimation is carried out for both the simulations and applied example using each of the six estimators under both stratified and pooled approaches of nuisance parameter estimation. Simulation results show that double robust estimators remained without meaningful bias as long as at least one of the two nuisance parameters were estimated with a correctly specified model. Under full misspecification, the bias of the double robust estimators remained better than that of the inverse propensity estimator under misspecification, but worse than the iterated conditional expectation estimator. Weighted estimators tended to show better performance than the covariate estimators. As positivity violations increased, the mean squared error and bias of all estimators considered became worse, with covariate-based double robust estimators especially susceptible. Applied analyses showed similar estimates at most time points, with the important exception of the inverse propensity estimator which deviated markedly as positivity violations increased. Given its efficiency, ability to respect the parameter space, and observed performance, we recommend the pooled and weighted targeted minimum loss-based estimator.},
 author = {Tran, Linh and Yiannoutsos, Constantin and Wools-Kaloustian, Kara and Siika, Abraham and {van der Laan}, Mark and Petersen, Maya},
 year = {2019},
 title = {Double robust efficient estimators of longitudinal treatment effects: Comparative performance in simulations and a case study},
 keywords = {Algorithms;Biostatistics/methods;Causality;Cohort Studies;Computer Simulation;Data Interpretation, Statistical;HIV Infections/mortality/therapy;Humans;Likelihood Functions;Longitudinal Studies;Machine learning;Models, Statistical;Probability;Propensity Score;Research Design;Software;Treatment Outcome},
 volume = {15},
 number = {2},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/ijb-2017-0054},
 file = {10.1515{\_}ijb-2017-0054:Attachments/10.1515{\_}ijb-2017-0054.pdf:application/pdf}
}


@inproceedings{Tschernutter.2022,
 abstract = {Personalized treatment decisions have become an integral part of modern medicine. Thereby, the aim is to make treatment decisions based on individual patient characteristics. Numerous methods have been developed for learning such policies from observational data that achieve the best outcome across a certain policy class. Yet these methods are rarely interpretable. However, interpretability is often a prerequisite for policy learning in clinical practice. In this paper, we propose an algorithm for interpretable off-policy learning via hyperbox search. In particular, our policies can be represented in disjunctive normal form (i.e., OR-of-ANDs) and are thus intelligible. We prove a universal approximation theorem that shows that our policy class is flexible enough to approximate any measurable function arbitrarily well. For optimization, we develop a tailored column generation procedure within a branch-and-bound framework. Using a simulation study, we demonstrate that our algorithm outperforms state-of-the-art methods from interpretable off-policy learning in terms of regret. Using real-word clinical data, we perform a user study with actual clinical experts, who rate our policies as highly interpretable.},
 author = {Tschernutter, Daniel and Hatt, Tobias and Feuerriegel, Stefan},
 title = {Interpretable off-policy learning via hyperbox search},
 url = {http://arxiv.org/pdf/2203.02473v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {ICML},
 year = {2022},
 file = {2203.02473:Attachments/2203.02473.pdf:application/pdf}
}


@inproceedings{Tu.2019,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Tu, Zhuozhuo and Zhang, Jingwei and Tao, Dacheng},
 title = {Theoretical analysis of adversarial learning: A minimax approach},
 booktitle = {NeurIPS},
 year = {2019},
 file = {6644-Supplementary Material:Attachments/6644-Supplementary Material.pdf:application/pdf;NeurIPS-2019-theoretical-analysis-of-adversarial-learning-a-minimax-approach-Paper:Attachments/NeurIPS-2019-theoretical-analysis-of-adversarial-learning-a-minimax-approach-Paper.pdf:application/pdf}
}


@proceedings{UAI.1994,
 year = {1994},
 title = {UAI}
}


@proceedings{UAI.2007,
 year = {2007},
 title = {UAI}
}


@proceedings{UAI.2009,
 year = {2009},
 title = {UAI}
}


@proceedings{UAI.2017,
 year = {2017},
 title = {UAI}
}


@proceedings{UAI.2019,
 year = {2019},
 title = {UAI}
}


@proceedings{UAI.2020,
 year = {2020},
 title = {UAI}
}


@proceedings{UAI.2021,
 year = {2021},
 title = {UAI}
}


@proceedings{UAI.2022,
 year = {2022},
 title = {UAI}
}


@proceedings{UAI.2023,
 year = {2023},
 title = {UAI}
}


@misc{UnitedNations.2015,
 author = {{United Nations}},
 year = {2015},
 title = {{S}ustainable {D}evelopment {G}oals: {T}he sustainable development agenda},
 url = {https://www.un.org/sustainabledevelopment/development-agenda/}
}


@article{vanderLaan.2006,
 abstract = {Suppose one observes a sample of independent and identically distributed observations from a particular data generating distribution. Suppose that one is concerned with estimation of a particular pathwise differentiable Euclidean parameter. A substitution estimator evaluating the parameter of a given likelihood based density estimator is typically too biased and might not even converge at the parametric rate: that is, the density estimator was targeted to be a good estimator of the density and might therefore result in a poor estimator of a particular smooth functional of the density. In this article we propose a one step (and, by iteration, k-th step) targeted maximum likelihood density estimator which involves 1) creating a hardest parametric submodel with parameter epsilon through the given density estimator with score equal to the efficient influence curve of the pathwise differentiable parameter at the density estimator, 2) estimating epsilon with the maximum likelihood estimator, and 3) defining a new density estimator as the corresponding update of the original density estimator. We show that iteration of this algorithm results in a targeted maximum likelihood density estimator which solves the efficient influence curve estimating equation and thereby yields a locally efficient estimator of the parameter of interest, under regularity conditions. In particular, we show that, if the parameter is linear and the model is convex, then the targeted maximum likelihood estimator is often achieved in the first step, and it results in a locally efficient estimator at an arbitrary (e.g., heavily misspecified) starting density.We also show that the targeted maximum likelihood estimators are now in full agreement with the locally efficient estimating function methodology as presented in Robins and Rotnitzky (1992) and van der Laan and Robins (2003), creating, in particular, algebraic equivalence between the double robust locally efficient estimators using the targeted maximum likelihood estimators as an estimate of its nuisance parameters, and targeted maximum likelihood estimators. In addition, it is argued that the targeted MLE has various advantages relative to the current estimating function based approach. We proceed by providing data driven methodologies to select the initial density estimator for the targeted MLE, thereby providing data adaptive targeted maximum likelihood estimation methodology. We illustrate the method with various worked out examples.},
 author = {{van der Laan}, Mark J. and Rubin, Donald B.},
 year = {2006},
 title = {Targeted maximum likelihood learning},
 volume = {2},
 number = {1},
 journal = {The International Journal of Biostatistics},
 file = {Daniel Rubin 1043 - Targeted Maximum Likelihood Learning:Attachments/Daniel Rubin 1043 - Targeted Maximum Likelihood Learning.pdf:application/pdf}
}


@article{vanderLaan.2007,
 abstract = {When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox. A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression. Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners. Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner. This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners. In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions. This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.},
 author = {{van der Laan}, Mark J. and Polley, Eric C. and Hubbard, Alan E.},
 year = {2007},
 title = {Super learner},
 pages = {1--23},
 volume = {6},
 journal = {Statistical Applications in Genetics and Molecular Biology},
 doi = {10.2202/1544-6115.1309}
}


@article{vanderLaan.2012,
 abstract = {We consider estimation of the effect of a multiple time point intervention on an outcome of interest, where the intervention nodes are subject to time-dependent confounding by intermediate covariates. In previous work van der Laan (2010) and Stitelman and van der Laan (2011a) developed and implemented a closed form targeted maximum likelihood estimator (TMLE) relying on the log-likelihood loss function, and demonstrated important gains relative to inverse probability of treatment weighted estimators and estimating equation based estimators. This TMLE relies on an initial estimator of the entire probability distribution of the longitudinal data structure. To enhance the finite sample performance of the TMLE of the target parameter it is of interest to select the smallest possible relevant part of the data generating distribution, which is estimated and updated by TMLE. Inspired by this goal, we develop a new closed form TMLE of an intervention specific mean outcome based on general longitudinal data structures. The target parameter is represented as an iterative sequence of conditional expectations of the outcome of interest. This collection of conditional means represents the relevant part, which is estimated and updated using the general TMLE algorithm. We also develop this new TMLE for other causal parameters, such as parameters defined by working marginal structural models. The theoretical properties of the TMLE are also practically demonstrated with a small scale simulation study.The proposed TMLE is building upon a previously proposed estimator Bang and Robins (2005) by integrating some of its key and innovative ideas into the TMLE framework.},
 author = {{van der Laan}, Mark J. and Gruber, Susan},
 year = {2012},
 title = {Targeted minimum loss based estimation of causal effects of multiple time point interventions},
 keywords = {Algorithms;Asymptotic linearity of an estimator;Bias;Biostatistics/methods;causal effect;Causality;confounding;efficient influence curve;G-computation formula;Humans;influence curve;Likelihood Functions;longitudinal data;Longitudinal Studies;loss function;marginal structural working model;Mathematical Concepts;Models, Statistical;Monte Carlo Method;nonparametric structural equation model;positivity a;Randomized Controlled Trials as Topic/statistics {\&} numerical data;Statistics, Nonparametric;Survival Analysis;Time Factors},
 volume = {8},
 number = {1},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/1557-4679.1370},
 file = {1557-4679.1370:Attachments/1557-4679.1370.pdf:application/pdf}
}


@book{vanderLaan.2018,
 author = {{van der Laan}, Mark J. and Rose, Sherri},
 year = {2018},
 title = {Targeted learning in data science},
 address = {Cham},
 publisher = {Springer},
 isbn = {978-3-319-65303-7},
 doi = {10.1007/978-3-319-65304-4},
 file = {Targeted Learning in Data Science by Mark J. van der Laan, Sherri Rose (z-lib.org):Attachments/Targeted Learning in Data Science by Mark J. van der Laan, Sherri Rose (z-lib.org).pdf:application/pdf}
}


@article{Vansteelandt.2006,
 author = {Vansteelandt, Stijn and Goetghebeur, Els and Kenward, Michael G. and Molenberghs, Geert},
 year = {2006},
 title = {Ignorance and uncertainty regions as inferential tools in a sensitivity analysis},
 pages = {953--979},
 volume = {16},
 journal = {Statistica Sinica},
 file = {A16n315:Attachments/A16n315.pdf:application/pdf}
}


@article{Vansteelandt.2016,
 author = {Vansteelandt, Stijn and Sjolander, Arvid},
 year = {2016},
 title = {Revisiting g-estimation of the effect of a time-varying exposure subject to time-varying confounding},
 volume = {5},
 number = {1},
 issn = {2194-9263},
 journal = {Epidemiologic Methods},
 doi = {10.1515/em-2015-0005},
 file = {revisiting{\_}gestimation:Attachments/revisiting{\_}gestimation.pdf:application/pdf}
}


@article{Varian.2016,
 abstract = {This is an elementary introduction to causal inference in economics written for readers familiar with machine learning methods. The critical step in any causal analysis is estimating the counterfactual-a prediction of what would have happened in the absence of the treatment. The powerful techniques used in machine learning may be useful for developing better estimates of the counterfactual, potentially improving causal inference.},
 author = {Varian, Hal R.},
 year = {2016},
 title = {Causal inference in economics and marketing},
 pages = {7310--7315},
 volume = {113},
 number = {27},
 journal = {Proceedings of the National Academy of Sciences (PNAS)},
 doi = {10.1073/pnas.1510479113},
 file = {Varian 2016 - Causal inference in economics:Attachments/Varian 2016 - Causal inference in economics.pdf:application/pdf}
}


@misc{Vergari.,
 author = {Vergari, Antonio and Choi, Yoojung and Peharz, Robert and {Van den Broeck}, Guy},
 title = {Probabilistic Circuits},
 file = {ECAI20:Attachments/ECAI20.pdf:application/pdf}
}


@article{Viviano.2023,
 abstract = {One of the major concerns of targeting interventions on individuals in social welfare programs is discrimination: individualized treatments may induce disparities across sensitive attributes such as age, gender, or race. This paper addresses the question of the design of fair and efficient treatment allocation rules. We adopt the non-maleficence perspective of first do no harm: we select the fairest allocation within the Pareto frontier. We cast the optimization into a mixed-integer linear program formulation, which can be solved using off-the-shelf algorithms. We derive regret bounds on the unfairness of the estimated policy function and small sample guarantees on the Pareto frontier under general notions of fairness. Finally, we illustrate our method using an application from education economics.},
 author = {Viviano, Davide and Bradic, Jelena},
 year = {2023},
 title = {Fair policy targeting},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Methodology;Statistics - Theory},
 journal = {Journal of the American Statistical Association},
 file = {2005.12395:Attachments/2005.12395.pdf:application/pdf}
}


@misc{vonLuxburg.2008,
 abstract = {Statistical learning theory provides the theoretical basis for many of today's machine learning algorithms. In this article we attempt to give a gentle, non-technical overview over the key ideas and insights of statistical learning theory. We target at a broad audience, not necessarily machine learning researchers. This paper can serve as a starting point for people who want to get an overview on the field before diving into technical details.},
 author = {Luxburg, Ulrike von and Schoelkopf, Bernhard},
 date = {2008},
 title = {Statistical Learning Theory: Models, concepts, and results},
 url = {http://arxiv.org/pdf/0810.4752v1},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 file = {0810.4752.pdf{\"i}{\textonequarter}‰...{\aa}¦{\glq}{\ae}{\v{z}}{\oe}{\ae}{\glq}¨{\ae}ƒ$^3${\`e}¦ {\"a}¸{\text\euro}{\"a}º{\frq}{\aa}$\ldots$¬{\aa}{\textonequarter} {\aa} $^3${\"a}{\textonehalf} {\c{c}}{\S}°{\"a}$^1${\flq}{\"a}¸º``{\"a}¸ {\ae}¸ {\`e}?`{\grq}''{\ae}ˆ-``{\"a}¸{\text\euro}{\`e}ˆ¬''{\"i}{\textonequarter}{\OE}{\"a}{\textonehalf} {\ae}{\oe}Ÿ{\ae}{\oe}{\frq}{\aa}{\circledR}ƒ{\ae}˜¯{\"a}{\frqq}{\text\euro}{\"a}$^1$ˆ{\"i}{\textonequarter}Ÿ:Attachments/0810.4752.pdf{\"i}{\textonequarter}‰...{\aa}¦{\glq}{\ae}{\v{z}}{\oe}{\ae}{\glq}¨{\ae}ƒ$^3${\`e}¦ {\"a}¸{\text\euro}{\"a}º{\frq}{\aa}$\ldots$¬{\aa}{\textonequarter} {\aa} $^3${\"a}{\textonehalf} {\c{c}}{\S}°{\"a}$^1${\flq}{\"a}¸º``{\"a}¸ {\ae}¸ {\`e}?`{\grq}''{\ae}ˆ-``{\"a}¸{\text\euro}{\`e}ˆ¬''{\"i}{\textonequarter}{\OE}{\"a}{\textonehalf} {\ae}{\oe}Ÿ{\ae}{\oe}{\frq}{\aa}{\circledR}ƒ{\ae}˜¯{\"a}{\frqq}{\text\euro}{\"a}$^1$ˆ{\"i}{\textonequarter}Ÿ.pdf:application/pdf}
}


@misc{Vowels.2022,
 abstract = {Parameter estimation in empirical fields is usually undertaken using parametric models, and such models readily facilitate statistical inference. Unfortunately, they are unlikely to be sufficiently flexible to be able to adequately model real-world phenomena, and may yield biased estimates. Conversely, non-parametric approaches are flexible but do not readily facilitate statistical inference and may still exhibit residual bias. We explore the potential for Influence Functions (IFs) to (a) improve initial estimators without needing more data (b) increase model robustness and (c) facilitate statistical inference. We begin with a broad introduction to IFs, and propose a neural network method 'MultiNet', which seeks the diversity of an ensemble using a single architecture. We also introduce variants on the IF update step which we call 'MultiStep', and provide a comprehensive evaluation of different approaches. The improvements are found to be dataset dependent, indicating an interaction between the methods used and nature of the data generating process. Our experiments highlight the need for practitioners to check the consistency of their findings, potentially by undertaking multiple analyses with different combinations of estimators. We also show that it is possible to improve existing neural networks for `free', without needing more data, and without needing to retrain them.},
 author = {Vowels, Matthew J. and Akbari, Sina and Camgoz, Necati Cihan and Bowden, Richard},
 date = {2022},
 title = {A free lunch with influence functions? Improving neural network  estimates with concepts from semiparametric statistics},
 url = {http://arxiv.org/pdf/2202.09096v2},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 file = {2202.09096 (1):Attachments/2202.09096 (1).pdf:application/pdf}
}


@misc{Wager.,
 author = {Wager, Stefan},
 title = {STATS 361: Causal Inference},
 file = {STATS 361 - Causal Inference:Attachments/STATS 361 - Causal Inference.pdf:application/pdf}
}


@article{Wager.2018,
 abstract = {Journal of the American Statistical Association, 2018. doi:10.1080/01621459.2017.1319839},
 author = {Wager, Stefan and Athey, Susan},
 year = {2018},
 title = {Estimation and inference of heterogeneous treatment effects using random forests},
 keywords = {Adaptive nearest neighbors matching;Asymptotic normality;Potential outcomes;Unconfoundedness},
 pages = {1228--1242},
 volume = {113},
 number = {523},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2017.1319839},
 file = {Estimation and Inference of Heterogeneous Treatment Effects using Random Forests:Attachments/Estimation and Inference of Heterogeneous Treatment Effects using Random Forests.pdf:application/pdf}
}


@article{Wald.1940,
 author = {Wald, Abraham},
 year = {1940},
 title = {The fitting of straight lines if both variables are subject to error},
 pages = {284--300},
 volume = {11},
 number = {3},
 journal = {Annals of Mathematical Statistics},
 file = {1177731868:Attachments/1177731868.pdf:application/pdf}
}


@article{Wang.2018,
 author = {Wang, Linbo and {Tchetgen Tchetgen}, Eric J.},
 year = {2018},
 title = {Bounded, efficient and multiply robust estimation of average treatment effects using instrumental variables},
 pages = {531--550},
 volume = {80},
 number = {3},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 file = {1611.09925:Attachments/1611.09925.pdf:application/pdf}
}


@article{Wang.2019,
 abstract = {Causal inference from observational data is a vital problem, but it comes with strong assumptions. Most methods assume that we observe all confounders, variables that affect both the causal variabl...},
 author = {Wang, Yixin and Blei, David M.},
 year = {2019},
 title = {The blessings of multiple causes},
 pages = {1574--1596},
 volume = {114},
 number = {528},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2019.1686987},
 file = {Wang, Blei 2019 - The Blessings of Multiple Causes:Attachments/Wang, Blei 2019 - The Blessings of Multiple Causes.pdf:application/pdf}
}


@inproceedings{Wang.2020,
 author = {Wang, Shirly and McDermott, Matthew B.A. and Chauhan, Geeticka and Ghassemi, Marzyeh and Hughes, Michael C. and Naumann, Tristan},
 title = {{MIMIC}-extract: A data extraction, preprocessing, and representation pipeline for {MIMIC-III}},
 booktitle = {CHIL},
 year = {2020},
 file = {MIMIC extract:Attachments/MIMIC extract.pdf:application/pdf}
}


@article{Wang.2021,
 abstract = {Management Science 0.0},
 author = {Wang, Guihua and Li, Jun and Hopp, Wallace J.},
 year = {2021},
 title = {An instrumental variable forest approach for detecting heterogeneous treatment effects in observational studies},
 keywords = {big data analytics;causal inference;heterogeneous treatment effects;Machine learning},
 issn = {0025-1909},
 journal = {Management Science},
 doi = {10.1287/mnsc.2021.4084},
 file = {mnsc.2021.4084:Attachments/mnsc.2021.4084.pdf:application/pdf}
}


@article{Wang.2021b,
 abstract = {Instrumental variables are widely used to deal with unmeasured confounding in observational studies and imperfect randomized controlled trials. In these studies, researchers often target the so-called local average treatment effect as it is identifiable under mild conditions. In this paper, we consider estimation of the local average treatment effect under the binary instrumental variable model. We discuss the challenges for causal estimation with a binary outcome, and show that surprisingly, it can be more difficult than the case with a continuous outcome. We propose novel modeling and estimating procedures that improve upon existing proposals in terms of model congeniality, interpretability, robustness or efficiency. Our approach is illustrated via simulation studies and a real data analysis.},
 author = {Wang, Linbo and Zhang, Yuexia and Richardson, Thomas S. and Robins, James M.},
 year = {2021},
 title = {Estimation of local treatment effects under the binary instrumental  variable model},
 url = {http://arxiv.org/pdf/2007.14458v2},
 keywords = {Statistics - Methodology},
 issn = {0006-3444},
 journal = {Biometrika},
 file = {2007.14458:Attachments/2007.14458.pdf:application/pdf}
}


@article{Winkler.2019,
 abstract = {Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.},
 author = {Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
 year = {2019},
 title = {Learning likelihoods with conditional normalizing flows},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Statistics - Machine Learning},
 volume = {arXiv:1912.00042},
 journal = {arXiv preprint},
 file = {1912.00042:Attachments/1912.00042.pdf:application/pdf}
}


@unpublished{Wolf.2018,
 author = {Wolf, Michael M.},
 year = {2018},
 title = {Mathematical foundations of supervised learning},
 file = {ML{\_}notes{\_}main:Attachments/ML{\_}notes{\_}main.pdf:application/pdf}
}


@inproceedings{Wong.2019,
 abstract = {Proceedings of the International Conference on Machine Learning 2019},
 author = {Wong, Eric and Schmidt, Frank R. and Kolter, Zico},
 title = {Wasserstein Adversarial Examples via Projected Sinkhorn Iterations},
 booktitle = {ICML},
 year = {2019},
 file = {wong19a:Attachments/wong19a.pdf:application/pdf}
}


@book{Wooldridge.2013,
 author = {Wooldridge, Jeffrey M.},
 year = {2013},
 title = {Introductory Econometrics: A modern approach},
 publisher = {Routledge},
 isbn = {9781136586101},
 doi = {10.4324/9780203157688},
 file = {Jeffrey{\_}M.{\_}Wooldridge{\_}Introductory{\_}Econometrics{\_}A{\_}Modern{\_}Approach{\_}{\_}2012:Attachments/Jeffrey{\_}M.{\_}Wooldridge{\_}Introductory{\_}Econometrics{\_}A{\_}Modern{\_}Approach{\_}{\_}2012.pdf:application/pdf}
}


@book{Wright.1928,
 author = {Wright, Phillip G.},
 year = {1928},
 title = {The tariff on animal and vegitable oils},
 address = {New York},
 publisher = {Macmillan}
}


@inproceedings{Wu.2019,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Wu, Yongkai and Zhang, Lu and Wu, Xintau and Tong, Hanghang},
 title = {{PC}-fairness: A unified framework for measuring causality-based fairness},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-pc-fairness-a-unified-framework-for-measuring-causality-based-fairness-Paper:Attachments/NeurIPS-2019-pc-fairness-a-unified-framework-for-measuring-causality-based-fairness-Paper.pdf:application/pdf}
}


@misc{Wu.2020,
 abstract = {The fundamental problem in treatment effect estimation from observational data is confounder identification and balancing. Most of the previous methods realized confounder balancing by treating all observed pre-treatment variables as confounders, ignoring further identifying confounders and non-confounders. In general, not all the observed pre-treatment variables are confounders that refer to the common causes of the treatment and the outcome, some variables only contribute to the treatment and some only contribute to the outcome. Balancing those non-confounders, including instrumental variables and adjustment variables, would generate additional bias for treatment effect estimation. By modeling the different causal relations among observed pre-treatment variables, treatment and outcome, we propose a synergistic learning framework to 1) identify confounders by learning decomposed representations of both confounders and non-confounders, 2) balance confounder with sample re-weighting technique, and simultaneously 3) estimate the treatment effect in observational studies via counterfactual inference. Empirical results on synthetic and real-world datasets demonstrate that the proposed method can precisely decompose confounders and achieve a more precise estimation of treatment effect than baselines.},
 author = {Wu, Anpeng and Kuang, Kun and Yuan, Junkun and Li, Bo and Wu, Runze and Zhu, Qiang and Zhuang, Yueting and Wu, Fei},
 date = {2020},
 title = {Learning decomposed representation for counterfactual inference},
 url = {http://arxiv.org/pdf/2006.07040v2},
 keywords = {Computer Science - Learning;Confounder Identification and Balancing;counterfactual inference;Decomposed Representation;Statistics - Machine Learning;Statistics - Methodology;Treatment Effect},
 file = {2006.07040:Attachments/2006.07040.pdf:application/pdf}
}


@inproceedings{Wu.2022,
 abstract = {Proceedings of the International Conference on Machine Learning 2022},
 author = {Wu, Anpeng and Kuang, Kun and Li, Bo and Wu, Fei},
 title = {Instrumental variable regression with confounder balancing},
 booktitle = {ICML},
 year = {2022},
 file = {wu22e:Attachments/wu22e.pdf:application/pdf}
}


@misc{Wu.2022b,
 abstract = {Estimating an individual's potential outcomes under counterfactual treatments is a challenging task for traditional causal inference and supervised learning approaches when the outcome is high-dimensional (e.g. gene expressions, impulse responses, human faces) and covariates are relatively limited. In this case, to construct one's outcome under a counterfactual treatment, it is crucial to leverage individual information contained in its observed factual outcome on top of the covariates. We propose a deep variational Bayesian framework that rigorously integrates two main sources of information for outcome construction under a counterfactual treatment: one source is the individual features embedded in the high-dimensional factual outcome; the other source is the response distribution of similar subjects (subjects with the same covariates) that factually received this treatment of interest.},
 author = {Wu, Yulun and Price, Layne C. and Wang, Zichen and Ioannidis, Vassilis N. and Karypis, George},
 date = {2022},
 title = {Variational causal inference},
 url = {http://arxiv.org/pdf/2209.05935v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Mathematics - Statistics;Quantitative Biology - Genomics;Statistics - Machine Learning;Statistics - Theory},
 file = {2209.05935:Attachments/2209.05935.pdf:application/pdf}
}


@inproceedings{Xia.2021,
 abstract = {One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.},
 author = {Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
 title = {The causal-neural connection: Expressiveness, learnability, and inference},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning},
 booktitle = {NeurIPS},
 year = {2021},
 file = {2107.00793:Attachments/2107.00793.pdf:application/pdf}
}


@inproceedings{Xia.2023,
 abstract = {Evaluating hypothetical statements about how the world would be had a different course of action been taken is arguably one key capability expected from modern AI systems. Counterfactual reasoning underpins discussions in fairness, the determination of blame and responsibility, credit assignment, and regret. In this paper, we study the evaluation of counterfactual statements through neural models. Specifically, we tackle two causal problems required to make such evaluations, i.e., counterfactual identification and estimation from an arbitrary combination of observational and experimental data. First, we show that neural causal models (NCMs) are expressive enough and encode the structural constraints necessary for performing counterfactual reasoning. Second, we develop an algorithm for simultaneously identifying and estimating counterfactual distributions. We show that this algorithm is sound and complete for deciding counterfactual identification in general settings. Third, considering the practical implications of these results, we introduce a new strategy for modeling NCMs using generative adversarial networks. Simulations corroborate with the proposed methodology.},
 author = {Xia, Kevin and Pan, Yushu and Bareinboim, Elias},
 title = {Neural causal models for counterfactual identification and estimation},
 keywords = {Computer Science - Learning},
 booktitle = {ICLR},
 year = {2023},
 file = {2210.00035:Attachments/2210.00035.pdf:application/pdf}
}


@inproceedings{Xu.2016,
 author = {Xu, Yanbo and Xu, Yanxun and Saria, Suchi},
 title = {A non-parametric bayesian approach for estimating treatment-response curves from sparse time series},
 booktitle = {ML4H},
 year = {2016}
}


@inproceedings{Xu.2021,
 abstract = {Instrumental variable (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by utilizing an instrumental variable, which affects the outcome only through the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to the outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally efficient manner. DFIV outperforms recent state-of-the-art methods on challenging IV benchmarks, including settings involving high dimensional image data. DFIV also exhibits competitive performance in off-policy policy evaluation for reinforcement learning, which can be understood as an IV regression task.},
 author = {Xu, Liyuan and Chen, Yutian and Srinivasan, Siddarth and Freitas, Nando de and Doucet, Arnaud and Gretton, Arthur},
 title = {Learning deep features in instrumental variable regression},
 url = {http://arxiv.org/pdf/2010.07154v3},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {ICLR},
 year = {2021},
 file = {learning{\_}deep{\_}features{\_}in{\_}inst:Attachments/learning{\_}deep{\_}features{\_}in{\_}inst.pdf:application/pdf}
}


@inproceedings{Xu.2021b,
 author = {Xu, Liyuan and Kanagawa, Heishiro and Gretton, Arthur},
 title = {Deep proxy causal learning and its application to confounded bandid policy evaluation},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-deep-proxy-causal-learning-and-its-application-to-confounded-bandit-policy-evaluation-Paper:Attachments/NeurIPS-2021-deep-proxy-causal-learning-and-its-application-to-confounded-bandit-policy-evaluation-Paper.pdf:application/pdf}
}


@article{Yang.2015,
 abstract = {The Annals of Statistics, 2015, Vol. 43, No. 2, 652-674},
 author = {Yang, Yun and Tokdar, Surya T.},
 year = {2015},
 title = {Minimax-optimal nonparametric regression in high dimensions},
 keywords = {60G15;62C20;62G08;Adaptive estimation;high-dimensional regression;minimax risk;model selection;nonparametric regression},
 pages = {652--674},
 volume = {43},
 number = {2},
 issn = {0090-5364},
 journal = {The Annals of Statistics},
 doi = {10.1214/14-AOS1289},
 file = {14-AOS1289:Attachments/14-AOS1289.pdf:application/pdf}
}


@article{Yang.2023,
 abstract = {Decision-makers often want to target interventions (e.g., marketing campaigns) so as to maximize an outcome that is observed only in the long-term. This typically requires delaying decisions until the outcome is observed or relying on simple short-term proxies for the long-term outcome. Here we build on the statistical surrogacy and off-policy learning literature to impute the missing long-term outcomes and then approximate the optimal targeting policy on the imputed outcomes via a doubly-robust approach. We apply our approach in large-scale proactive churn management experiments at The Boston Globe by targeting optimal discounts to its digital subscribers to maximize their long-term revenue. We first show that conditions for validity of average treatment effect estimation with imputed outcomes are also sufficient for valid policy evaluation and optimization; furthermore, these conditions can be somewhat relaxed for policy optimization. We then validate this approach empirically by comparing it with a policy learned on the ground truth long-term outcomes and show that they are statistically indistinguishable. Our approach also outperforms a policy learned on short-term proxies for the long-term outcome. In a second field experiment, we implement the optimal targeting policy with additional randomized exploration, which allows us to update the optimal policy for each new cohort of customers to account for potential non-stationarity. Over three years, our approach had a net-positive revenue impact in the range of {\$}4-5 million compared to The Boston Globe's current policies.},
 author = {Yang, Jeremy and Eckles, Dean and Dhillon, Paramveer and Aral, Sinan},
 year = {2023},
 title = {Targeting for long-term outcomes},
 url = {http://arxiv.org/pdf/2010.15835v1},
 keywords = {Computer Science - Learning;Statistics - Applications;Statistics - Machine Learning},
 issn = {0025-1909},
 journal = {Management Science},
 file = {2010.15835:Attachments/2010.15835.pdf:application/pdf}
}


@inproceedings{YaoweiHu.2021,
 abstract = {The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)},
 author = {Hu, Yaowei and Wu, Yongkai and Wu, Xintau},
 title = {A generative adversarial framework for bounding confounded causal effects},
 keywords = {Reasoning under Uncertainty: Causality},
 booktitle = {AAAI},
 year = {2021},
 file = {17437-Article Text-20931-1-2-20210518 (1):Attachments/17437-Article Text-20931-1-2-20210518 (1).pdf:application/pdf}
}


@article{Yazdani.2015,
 abstract = {Causal analyses and causal inference is a growing area of biostatics. In parallel, there is increasing focus on using genomic information to guide medical practice, i.e. personalized medicine or decision medicine. This perspective discusses causal inference in the context of personalized or decision medicine, including the assumptions and the concept that the task is different depending on whether the primary goal is the average response of treatment in the population or the ability to characterize the response for an individual or a subgroup. This perspective provides a tutorial of modern causal inference and then provides suggestions how application of specific kinds of causal inference would promote advances in translational sciences. The concept of the subpopulation causal effect is one path toward improved decision medicine. A dataset containing cardiovascular disease risk factor levels and genomic information is analyzed and different causal effects are estimated.},
 author = {Yazdani, Azam M. and Boerwinkle, Eric},
 year = {2015},
 title = {Causal inference in the age of decision medicine},
 volume = {6},
 number = {1},
 issn = {2153-0602},
 journal = {Journal of Data Mining in Genomics {\&} Proteomics},
 doi = {10.4172/2153-0602.1000163}
}


@article{Yin.2022,
 author = {Yin, Mingzhang and Shi, Claudia and Wang, Yixin and Blei, David M.},
 year = {2022},
 title = {Conformal Sensitivity Analysis for Individual Treatment Effects},
 keywords = {Distribution shift;Predictive inference;Uncertainty quantification;Unconfoundedness},
 pages = {1--14},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2022.2102503},
 file = {Conformal Sensitivity Analysis for Individual Treatment Effects:Attachments/Conformal Sensitivity Analysis for Individual Treatment Effects.pdf:application/pdf}
}


@article{Yoganarasimhan.2022,
 abstract = {Management Science 0.0},
 author = {Yoganarasimhan, Hema and Barzegary, Ebrahim and Pani, Abhishek},
 year = {2022},
 title = {Design and evaluation of optimal free trials},
 keywords = {digital marketing;field experiment;free trials;Machine learning;personalization;policy evaluation;Software as a Service;targeting},
 issn = {0025-1909},
 journal = {Management Science},
 file = {mnsc.2022.4507:Attachments/mnsc.2022.4507.pdf:application/pdf}
}


@inproceedings{Yoon.2018,
 author = {Yoon, Jinsung and Jordon, James and {van der Schaar}, Mihaela},
 title = {GANITE: Estimation of individualized treatment effects using generative adversarial nets},
 booktitle = {ICLR},
 year = {2018},
 file = {ganite{\_}estimation{\_}of{\_}individua:Attachments/ganite{\_}estimation{\_}of{\_}individua.pdf:application/pdf}
}


@inproceedings{Yu.2016,
 author = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S.},
 title = {Temporal regularized matrix factorization for high-dimensional time series prediction},
 booktitle = {NeurIPS},
 year = {2016},
 file = {Yu, Rao et al. 2016 - Temporal Regularized Matrix Factorization:Attachments/Yu, Rao et al. 2016 - Temporal Regularized Matrix Factorization.pdf:application/pdf}
}


@inproceedings{Yu.2022,
 abstract = {Long-term fairness is an important factor of consideration in designing and deploying learning-based decision systems in high-stake decision-making contexts. Recent work has proposed the use of Markov Decision Processes (MDPs) to formulate decision-making with long-term fairness requirements in dynamically changing environments, and demonstrated major challenges in directly deploying heuristic and rule-based policies that worked well in static environments. We show that policy optimization methods from deep reinforcement learning can be used to find strictly better decision policies that can often achieve both higher overall utility and less violation of the fairness requirements, compared to previously-known strategies. In particular, we propose new methods for imposing fairness requirements in policy optimization by regularizing the advantage evaluation of different actions. Our proposed methods make it easy to impose fairness constraints without reward engineering or sacrificing training efficiency. We perform detailed analyses in three established case studies, including attention allocation in incident monitoring, bank loan approval, and vaccine distribution in population networks.},
 author = {Yu, Eric Yang and Qin, Zhizhen and Lee, Min Kyung and Gao, Sicun},
 title = {Policy optimization with advantage regularization for long-term Fairness  in decision systems},
 url = {http://arxiv.org/pdf/2210.12546v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computers and Society;Computer Science - Learning},
 booktitle = {NeurIPS},
 year = {2022},
 file = {2210.12546:Attachments/2210.12546.pdf:application/pdf}
}


@article{Zehlike.2022,
 abstract = {In the past few years, there has been much work on incorporating fairness requirements into algorithmic rankers, with contributions coming from the data management, algorithms, information retrieval, and recommender systems communities. In this survey we give a systematic overview of this work, offering a broad perspective that connects formalizations and algorithmic approaches across subfields. An important contribution of our work is in developing a common narrative around the value frameworks that motivate specific fairness-enhancing interventions in ranking. This allows us to unify the presentation of mitigation objectives and of algorithmic techniques to help meet those objectives or identify trade-offs. In this survey, we describe four classification frameworks for fairness-enhancing interventions, along which we relate the technical methods surveyed in this paper, discuss evaluation datasets, and present technical work on fairness in score-based ranking. Then, we present methods that incorporate fairness in supervised learning, and also give representative examples of recent work on fairness in recommendation and matchmaking systems. We also discuss evaluation frameworks for fair score-based ranking and fair learning-to-rank, and draw a set of recommendations for the evaluation of fair ranking methods.},
 author = {Zehlike, Meike and Yang, Ke and Stoyanovich, Julia},
 year = {2022},
 title = {Fairness in ranking: A survey},
 keywords = {Computer Science - Databases;Computer Science - Information Retrieval;fairness;ranking;responsible data science;set selection;survey},
 volume = {arxiv:2103.14000},
 journal = {arXiv preprint},
 file = {2103.14000:Attachments/2103.14000.pdf:application/pdf}
}


@inproceedings{Zemel.2013,
 abstract = {Proceedings of the International Conference on Machine Learning 2013},
 author = {Zemel, Richard and Wu, Yu Ledell and Swersky, Kevin and Pitassi, Tohiann and Dwork, Cynthia},
 title = {Learning Fair Representations},
 keywords = {clustering;fairness;latent representations;privacy},
 booktitle = {ICML},
 year = {2013},
 file = {zemel13:Attachments/zemel13.pdf:application/pdf}
}


@article{Zeng.2023,
 abstract = {When estimating causal effects, it is important to assess external validity, i.e., determine how useful a given study is to inform a practical question for a specific target population. One challenge is that the covariate distribution in the population underlying a study may be different from that in the target population. If some covariates are effect modifiers, the average treatment effect (ATE) may not generalize to the target population. To tackle this problem, we propose new methods to generalize or transport the ATE from a source population to a target population, in the case where the source and target populations have different sets of covariates. When the ATE in the target population is identified, we propose new doubly robust estimators and establish their rates of convergence and limiting distributions. Under regularity conditions, the doubly robust estimators provably achieve the efficiency bound and are locally asymptotic minimax optimal. A sensitivity analysis is provided when the identification assumptions fail. Simulation studies show the advantages of the proposed doubly robust estimator over simple plug-in estimators. Importantly, we also provide minimax lower bounds and higher-order estimators of the target functionals. The proposed methods are applied in transporting causal effects of dietary intake on adverse pregnancy outcomes from an observational study to the whole U.S. female population.},
 author = {Zeng, Zhenghao and Kennedy, Edward H. and Bodnar, Lisa M. and Naimi, Ashley I.},
 year = {2023},
 title = {Efficient generalization and transportation},
 keywords = {Statistics - Methodology},
 volume = {arXiv:2302.00092},
 journal = {arXiv preprint},
 file = {2302.00092:Attachments/2302.00092.pdf:application/pdf}
}


@article{Zennaro.2022,
 abstract = {Structural causal models (SCMs) are a widespread formalism to deal with causal systems. A recent direction of research has considered the problem of relating formally SCMs at different levels of abstraction, by defining maps between SCMs and imposing a requirement of interventional consistency. This paper offers a review of the solutions proposed so far, focusing on the formal properties of a map between SCMs, and highlighting the different layers (structural, distributional) at which these properties may be enforced. This allows us to distinguish families of abstractions that may or may not be permitted by choosing to guarantee certain properties instead of others. Such an understanding not only allows to distinguish among proposal for causal abstraction with more awareness, but it also allows to tailor the definition of abstraction with respect to the forms of abstraction relevant to specific applications.},
 author = {Zennaro, Fabio Massimo},
 title = {Abstraction between structural causal models: A review of definitions  and properties},
 url = {http://arxiv.org/pdf/2207.08603v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning},
 file = {2207.08603:Attachments/2207.08603.pdf:application/pdf}
}


@inproceedings{Zhang.2020,
 abstract = {The choice of making an intervention depends on its potential benefit or harm in comparison to alternatives. Estimating the likely outcome of alternatives from observational data is a challenging problem as all outcomes are never observed, and selection bias precludes the direct comparison of differently intervened groups. Despite their empirical success, we show that algorithms that learn domain-invariant representations of inputs (on which to make predictions) are often inappropriate, and develop generalization bounds that demonstrate the dependence on domain overlap and highlight the need for invertible latent maps. Based on these results, we develop a deep kernel regression algorithm and posterior regularization framework that substantially outperforms the state-of-the-art on a variety of benchmarks data sets.},
 author = {Zhang, Yao and Bellot, Alexis and {van der Schaar}, Mihaela},
 title = {Learning overlapping representations for the estimation of  individualized treatment effects},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {AISTATS},
 year = {2020},
 file = {Learning Overlapping Representations:Attachments/Learning Overlapping Representations.pdf:application/pdf}
}


@inproceedings{Zhang.2022,
 author = {Zhang, Yao and Berrevoets, Jeroen and {van der Schaar}, Mihaela},
 title = {Identifiable energy-based representations: An application to estimating heterogeneous causal effects},
 booktitle = {AISTATS},
 year = {2022},
 file = {zhang22b:Attachments/zhang22b.pdf:application/pdf}
}


@article{Zhang.2022b,
 abstract = {Sensitivity analysis for the unconfoundedness assumption is a crucial component of observational studies. The marginal sensitivity model has become increasingly popular for this purpose due to its interpretability and mathematical properties. After reviewing the original marginal sensitivity model that imposes a {\$}L{\^{}}$\backslash$infty{\$}-constraint on the maximum logit difference between the observed and full data propensity scores, we introduce a more flexible {\$}L{\^{}}2{\$}-analysis framework; sensitivity value is interpreted as the {\textquotedbl}average{\textquotedbl} amount of unmeasured confounding in the analysis. We derive analytic solutions to the stochastic optimization problems under the {\$}L{\^{}}2{\$}-model, which can be used to bound the average treatment effect (ATE). We obtain the efficient influence functions for the optimal values and use them to develop efficient one-step estimators. We show that multiplier bootstrap can be applied to construct a simultaneous confidence band of the ATE. Our proposed methods are illustrated by simulation and real-data studies.},
 author = {Zhang, Yao and Zhao, Qingyuan},
 year = {2022},
 title = {Bounds and semiparametric inference in {\$}L{\^{}}$\backslash$infty{\$}- and {\$}L{\^{}}2{\$}-sensitivity  analysis for observational studies},
 url = {http://arxiv.org/pdf/2211.04697v1},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 volume = {arXiv:2211.04697},
 journal = {arXiv preprint},
 file = {2211.04697:Attachments/2211.04697.pdf:application/pdf}
}


@article{Zhao.2019,
 abstract = {To identify the estimand in missing data problems and observational studies, it is common to base the statistical estimation on the {\textquotedbl}missing at random{\textquotedbl} and {\textquotedbl}no unmeasured confounder{\textquotedbl} assumptions. However, these assumptions are unverifiable using empirical data and pose serious threats to the validity of the qualitative conclusions of the statistical inference. A sensitivity analysis asks how the conclusions may change if the unverifiable assumptions are violated to a certain degree. In this paper we consider a marginal sensitivity model which is a natural extension of Rosenbaum's sensitivity model that is widely used for matched observational studies. We aim to construct confidence intervals based on inverse probability weighting estimators, such that asymptotically the intervals have at least nominal coverage of the estimand whenever the data generating distribution is in the collection of marginal sensitivity models. We use a percentile bootstrap and a generalized minimax/maximin inequality to transform this intractable problem to a linear fractional programming problem, which can be solved very efficiently. We illustrate our method using a real dataset to estimate the causal effect of fish consumption on blood mercury level.},
 author = {Zhao, Qingyuan and Small, Dylan S. and Bhattacharya, Bhaswar B.},
 year = {2019},
 title = {Sensitivity analysis for inverse probability weighting estimators via  the percentile bootstrap},
 url = {http://arxiv.org/pdf/1711.11286v2},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 pages = {735--761},
 volume = {81},
 number = {4},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 file = {1711.11286:Attachments/1711.11286.pdf:application/pdf}
}


@misc{Zhu.01.06.2023,
 abstract = {Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.},
 author = {Zhu, Banghua and Ding, Mingyu and Jacobson, Philip and Wu, Ming and Zhan, Wei and Jordan, Michael and Jiao, Jiantao},
 date = {01.06.2023},
 title = {Doubly Robust Self-Training},
 url = {http://arxiv.org/pdf/2306.00265v2},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Statistics - Machine Learning},
 file = {2306.00265:Attachments/2306.00265.pdf:application/pdf}
}


@inproceedings{Zimmer.2021,
 abstract = {Proceedings of the International Conference on Machine Learning 2021},
 author = {Zimmer, Matthieu and Glanois, Clarie and Siddique, Umer and Weng, Paul},
 title = {Learning fair policies in decentralized cooperative  multi-agent reinforcement learning},
 keywords = {Actor-Critic;Decentralized;fairness;ICML;Multi-agent},
 booktitle = {ICML},
 year = {2021},
 file = {zimmer21a:Attachments/zimmer21a.pdf:application/pdf}
}


@inproceedings{Zou.2022,
 abstract = {Proceedings of the International Conference on Machine Learning 2022},
 author = {Zou, Hao and Li, Bo and Han, Jiangang and Chen, Shuiping and Ding, Xuetao and Cui, Peng},
 title = {Counterfactual prediction for outcome-oriented treatments},
 booktitle = {ICML},
 year = {2022},
 file = {zou22a:Attachments/zou22a.pdf:application/pdf}
}


