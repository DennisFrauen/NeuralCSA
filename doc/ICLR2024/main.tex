
\documentclass{article} % For LaTeX2e

\usepackage[dvipsnames, table]{xcolor}

\usepackage{iclr2024_conference}

\usepackage{times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{slashbox}
\usepackage{subcaption}
\usepackage{graphicx}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\usepackage{wrapfig}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\TODO}[1]{{\color{red}#1}}
\newcommand{\makeblue}[1]{{\color{blue}#1}}
\newcommand{\makered}[1]{{\color{red}#1}}
\usepackage{xspace}
  \newcommand{\eg}{e.\,g.\xspace}
  \newcommand{\ie}{i.\,e.\xspace}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{setspace}
\newcommand{\frameworkname}{\textsc{NeuralCSA}\xspace}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\hammer}{\includegraphics[scale=0.018]{figures/hammer.png}}
%\usepackage{xcolor,pifont}
%\newcommand*\colourcheck[1]{%
%  \expandafter\newcommand\csname #1check\endcsname{\textcolor{#1}{\ding{52}}}%
%}
%\colourcheck{green}
\usepackage{pifont}
\newcommand{\greencheck}{\textcolor{ForestGreen}{\checkmark}}
\newcommand{\redcross}{\textcolor{BrickRed}{\ding{55}}}
%\usepackage{tikz}
%\newcommand{\redcross}{%
%\tikz[scale=0.23] {
%    \draw[line width=0.7,line cap=round, color=red] (0,0) to [bend left=6] (1,1);
%    \draw[line width=0.7,line cap=round, color=red] (0.2,0.95) to [bend right=3] (0.8,0.05);
%}}
%\newcommand{\greencheck}{%
%\tikz[scale=0.23] {
%    \draw[line width=0.7,line cap=round, color=green] (0.25,0) to [bend left=10] (1,1);
%    \draw[line width=0.8,line cap=round, color=green] (0,0.35) to [bend right=1] (0.23,0);
%}}

%Theorems % Algorithms
\usepackage{amssymb,amsmath,amsthm}
\usepackage{algorithm2e}
\RestyleAlgo{ruled}
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}


\title{A Neural Framework for Generalized Causal Sensitivity Analysis}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Unobserved confounding is common in many applications, making causal inference from observational data challenging. As a remedy, \emph{causal sensitivity analysis} is an important tool to draw causal conclusions under unobserved confounding with mathematical guarantees. In this paper, we propose \frameworkname, a neural framework for \emph{generalized} causal sensitivity analysis. Unlike previous work, our framework is compatible with (i)~a large class of sensitivity models, including the marginal sensitivity model, $f$-sensitivity models, and Rosenbaum's sensitivity model; (ii)~different treatment types (i.e., binary and continuous); and (iii)~different causal queries, including (conditional) average treatment effects and simultaneous effects on multiple outcomes. The generality of \frameworkname is achieved by learning a latent distribution shift that corresponds to a treatment intervention using two conditional normalizing flows. We provide theoretical guarantees that \frameworkname is able to infer valid bounds on the causal query of interest and also demonstrate this empirically using both simulated and real-world data.
\end{abstract}
% To the best of our knowledge, \frameworkname is the first work on causal sensitivity analysis that addresses these settings in a single framework.
%Previous work on causal sensitivity analysis has focused on deriving closed-form bounds for specific causal queries (e.g., conditional average treatment effects for a single outcome) under specific assumptions on the data-generating process (sensitivity models)
%In the first stage, the first CNF is trained to learn the outcome distribution conditional on treatments and observed covariates. In the second stage, the second CNF is trained to learn a distribution in the latent space of the CNF that resembles a distribution shift due to the treatment intervention and respects a prespecified sensitivity model. 


\section{Introduction}

%Unobserved confounding
Causal inference from observational data is central to many fields such as medicine \citep{Glass.2013}, economics \citep{Imbens.1994}, or marketing \citep{Varian.2016}. However, the presence of unobserved confounding often renders causal inference challenging \citep{DAmour.2019}. As an example, consider an observational study examining the effect of smoking on lung cancer risk, where potential confounders, such as genetic factors influencing smoking behavior and cancer risk \citep{Erzurumluoglu.2020}, are not observed. Then, the causal relationship is not identifiable, and point identification without additional assumptions is impossible \citep{Pearl.2009}.

%What it is: Causal sensitivity analysis
Causal sensitivity analysis offers a remedy by moving from \emph{point} identification to \emph{partial} identification. To do so, approaches for causal sensitivity analysis first impose assumptions on the strength of unobserved confounding through so-called sensitivity models \citep{Rosenbaum.1987, Imbens.2003} and then obtain bounds on the causal query of interest. Such bounds often provide insights that the causal quantities can not reasonably be explained away by unobserved confounding, which is sufficient for consequential decision-making in many applications \citep{Kallus.2019}. In the above example, evidence supporting a causal effect of smoking on lung cancer even without information on genetic factors could serve as a basis for governments to implement anti-smoking policies. 

% Existing works

Existing works on causal sensitivity analysis can be loosely grouped by problem settings. These vary across (1)~sensitivity models, such as the marginal sensitivity model (MSM) \citep{Tan.2006}, $f$-sensitivity model \citep{Jin.2022}, and Rosenbaum's sensitivity model \citep{Rosenbaum.1987}; (2)~treatment type (i.e., binary and continuous); and (3)~causal query of interest. Causal queries may include (conditional) average treatment effects (CATE), but also distributional effects or simultaneous effects on multiple outcomes. Existing works typically focus on a specific sensitivity model, treatment type, and causal query (Table~\ref{tab:rw}). However, none is applicable to all settings within (1)--(3). 

%For example, we may be interested in the probability that both an individual's heart rate and blood pressure are below a certain threshold, if we intervene on that individual's smoking behavior.

%Limitations of previous work
%State-of-the-art work on causal sensitivity analysis focuses on deriving optimal bounds that can be estimated with flexible machine learning methods \citep{Jesson.2021, Jesson.2022, Dorn.2022, Dorn.2022b, Frauen.2023c, Oprescu.2023}. However, existing methods suffer from two major shortcomings: (i)~they are restricted to specific sensitivity models such as the marginal sensitivity model \citep{Tan.2006}, and (ii)~they provide bounds only for specific causal queries such as (conditional) average treatment effects for one-dimensional outcomes.

% Example of complex causal query-> multiple outcomes
%Point (ii) is especially restrictive in causal inference settings that study the simultaneous effect of an intervention on multiple outcomes. Consider an observational study where the causal relationship between smoking and the overall health of participants is of interest. Given the infeasibility of direct health measurements, multiple health indicators such as blood pressure and heart rate are recorded. Questions of interest may be of the following type: If we intervene on an individual's smoking behavior, what is the probability that both heart rate and blood pressure fall below a certain threshold? Current methods for causal sensitivity analysis are not applicable to such complex causal queries. This outlines the need for a more flexible approach, which we take in this paper.

% Our framework

To fill this gap, we propose \emph{\frameworkname}, a neural framework for causal sensitivity analysis that is applicable to numerous sensitivity models, treatment types, and causal queries, including multiple outcome settings. For this, we define a large class of sensitivity models, which we call \emph{generalized treatment sensitivity models} (GTSMs). GTSMs include common sensitivity models such as the MSM, $f$-sensitivity models, and Rosenbaum's sensitivity model. The intuition behind GTSMs is as follows: when intervening on the treatment $A$, the $U$--$A$ edge is removed in the corresponding causal graph, which leads to a distribution shift in the latent confounders $U$ (see Fig.~\ref{fig:dist_shift_sketch}). GTSMs then impose restrictions on this latent distribution shift, which corresponds to assumptions on the ``strength'' of unobserved confounding.

\begin{wrapfigure}{l}{0.4\textwidth}
  \centering
  \vspace{-0.5cm}
  \includegraphics[width=1\linewidth]{figures/dist_shift.pdf}
  \caption{Idea behind \frameworkname to learn the latent distribution shift due to treatment intervention (\hammer). Orange nodes denote observed (random) variables. Blue nodes denote unobserved variables pre-intervention. Green nodes indicate unobserved variables post-intervention under a GTSM $\mathcal{M}$. Observed confounders $X$ are omitted for simplicity.}
  \label{fig:dist_shift_sketch}
  \vspace{-0.5cm}
\end{wrapfigure}
\frameworkname is compatible with any sensitivity model that can be written as a GTSM. This is crucial in practical applications, where sensitivity models correspond to different assumptions on the data-generating process and may lead to different results \citep{Yin.2022}.
%and allows for various treatment types and causal queries, including queries involving multiple outcomes. 
To achieve this, \frameworkname \emph{learns the latent distribution shift in the unobserved confounders} from Fig.~\ref{fig:dist_shift_sketch} using two separately trained conditional normalizing flows (CNFs). This is different from previous works for causal sensitivity analysis, which do not provide a unified approach across numerous sensitivity models, treatment types, and causal queries. We provide theoretical guarantees that \frameworkname learns valid bounds on the causal query of interest and demonstrate this empirically.

% Contributions
Our \textbf{contributions}\insert\footins{\pdfrunninglinkoff}\footnote{Code is available at \href{https://anonymous.4open.science/r/NeuralCSA-DE7D}{https://anonymous.4open.science/r/NeuralCSA-DE7D}. (Link anonymized for peer review. Upon acceptance, the code will be moved to a public GitHub repository.}\insert\footins{\pdfrunninglinkon} are: (1)~We define a general class of sensitivity models, called GTSMs. (2)~We propose \frameworkname, a neural framework for causal sensitivity analysis under any GTSMs. \frameworkname is compatible with various sensitivity models, treatment types, and causal queries. (3)~We provide theoretical guarantees that \frameworkname learns valid bounds on the causal query of interest and demonstrate the effectiveness of our framework empirically. 

%(3)~We perform extensive computational experiments to confirm the effectiveness of our framework, using both simulated and real-world data.

\section{Related work}\label{sec:rw}

In the following, we provide an overview of related literature on partial identification and causal sensitivity analysis. A more detailed overview, including literature on point identification and estimation, can be found in Appendix~\ref{app:rw}.

%(für arXiv könnte man ja hiern och dein ICLR und Valetyns NeurIPS submission nehmen)
\textbf{Partial identification:} The aim of partial identification is to compute bounds on causal queries whenever point identification is not possible, such as under unobserved confounding \citep{Manski.1990}. There are several literature streams that impose different assumptions on the data-generating process in order to obtain informative bounds. One stream addresses partial identification for general causal graphs with discrete variables \citep{Duarte.2023}. Another stream assumes the existence of valid instrumental variables \citep{Gunsilius.2020, Kilbertus.2020}. Recently, there has been a growing interest in using neural networks for partial identification \citep{Xia.2021, Xia.2023, Padh.2023}. However, none of these methods allow for incorporating sensitivity models and sensitivity analysis.

\begin{wraptable}{r}{0.65\textwidth}
\vspace{-0.4cm}
\caption{Overview of key settings for causal sensitivity analyses and whether covered by existing literature (\greencheck) or not (\redcross). Treatments are either binary or continuous. Details are in Appendix~\ref{app:rw}. Our \frameworkname framework is \textbf{applicable in all settings} 
%(marked \colorbox{yellow}{yellow}).
}
\label{tab:rw}
\vspace{-0.3cm}
\resizebox{0.65\textwidth}{!}{
\begin{tabular}{lcccccc}
\noalign{\smallskip} \toprule \noalign{\smallskip}
\backslashbox{Causal query}{Sensitivity model}& \multicolumn{2}{c}{MSM} & \multicolumn{2}{c}{$f$-sensitivity} & \multicolumn{2}{c}{Rosenbaum}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} & Binary & Cont.\textsuperscript{$^\dagger$} & Binary & Cont. & Binary & Cont.\\
\midrule
CATE & \greencheck &  \greencheck &  \greencheck &  \redcross &  \greencheck & \redcross \\
Distributional effects &  \greencheck &  \greencheck &  \redcross &  \redcross &  \redcross& \redcross \\
Interventional density &  \greencheck&\greencheck & (\greencheck) & \redcross &  \redcross & \redcross \\
Multiple outcomes &  \redcross &  \redcross &  \redcross &  \redcross &  \redcross & \redcross \\
\bottomrule
\multicolumn{7}{p{0.7\textwidth}}{\scriptsize $^\dagger$ The MSM for continuous treatment is also called continuous MSM (CMSM) \citep{Jesson.2022}.}\\
\end{tabular}}
\vspace{-0.4cm}
\end{wraptable}

\textbf{Causal sensitivity analysis:} Causal sensitivity analysis addresses the partial identification of causal queries by imposing assumptions on the strength of unobserved confounding via sensitivity models. It dates back to \citet{Cornfield.1959}, who showed that unobserved confounding could not reasonably explain away the observed effect of smoking on lung cancer risk. 

Existing works can be grouped along three dimensions: (1)~the sensitivity model, (2)~the treatment type, and (3)~the causal query of interest (see Table~\ref{tab:rw}; details in Appendix~\ref{app:rw}). Popular sensitivity models include Rosenbaum's sensitivity model \citep{Rosenbaum.1987}, the marginal sensitivity model (MSM) \citep{Tan.2006}, and $f$-sensitivity models \citep{Jin.2022}. Here, most methods have been proposed for binary treatments and conditional average treatment effects \citep{Kallus.2019, Zhao.2019, Jesson.2021, Dorn.2022, Dorn.2022b, Oprescu.2023}. Extensions under the MSM have been proposed for continuous treatments \citep{Jesson.2022, Marmarelis.2023} and individual treatment effects \citep{Yin.2022, Jin.2023, Marmarelis.2023b}. However, approaches for many settings are still missing (shown by \redcross\xspace in Table~\ref{tab:rw}). In an attempt to generalize causal sensitivity analysis, \citet{Frauen.2023c} provided bounds for different treatment types (i.e., binary, continuous) and causal queries (e.g., CATE, distributional effects but not multiple outcomes). Yet, the results are limited to MSM-type sensitivity models. %A general approach to all settings in Table~\ref{tab:rw} is open to research. 

%Hence, many settings in Table~\ref{tab:rw} are not yet covered.

%\textbf{Research gap:}
To the best of our knowledge, no previous work proposes a unified solution for obtaining bounds under various sensitivity models (e.g., MSM, $f$-sensitivity, Rosenbaum's), treatment types (i.e., binary and continuous), and causal queries (e.g., CATE, distributional effects, interventional densities, and simultaneous effects on multiple outcomes). %This is our contribution.

%To the best of our knowledge, there exists no framework for causal sensitivity analysis that is compatible with various sensitivity models and causal queries. In particular, we are the first to propose solutions for the gaps in Table~\ref{tab:rw} (red crosses).


\section{Mathematical background}

%We now provide a brief mathematical background, introducing our problem setup and causal sensitivity analysis. For additional background on normalizing flows, we refer to Appendix~\ref{app:nf}.

\textbf{Notation:} We denote random variables $X$ as capital letters and their realizations $x$ in lowercase. We further write $\mathbb{P}(x)$ for the probability mass function if $X$ is discrete, and for the probability density function with respect to the Lebesque measure if $X$ is continuous. Conditional probability mass functions/ densities $\mathbb{P}(Y = y \mid X = x)$ are written as $\mathbb{P}(y \mid x)$. Finally, we denote the conditional distribution of $Y \mid X = x$ as $\mathbb{P}(Y \mid x)$ and its expectation as $\mathbb{E}[Y \mid x]$.

\subsection{Problem setup}

\textbf{Data generating process:} We consider the standard setting for (static) treatment effect estimation under unobserved confounding \citep{Dorn.2022}. That is, we have observed confounders $X \in \mathcal{X} \subseteq \R^{d_x}$, unobserved confounders $U \in \mathcal{U} \subseteq \R^{d_u}$, treatments $A \in \mathcal{A} \subseteq \R^{d_a}$, and outcomes $Y \in \mathcal{Y} \subseteq \R^{d_y}$. Note that we allow for (multiple) discrete or continuous treatments and multiple outcomes, i.e., $d_a, d_y \geq 1$. The underlying causal graph is shown in Fig.~\ref{fig:causal_graph}. We have access to an observational dataset $\mathcal{D} = (x_i, a_i, y_i)_{i=1}^n$ sampled i.i.d. from the observational distribution $(X, A, Y) \sim \mathbb{P}_\mathrm{obs}$. The full distribution $(X, U, A, Y) \sim \mathbb{P}$ is unknown.

\begin{wrapfigure}{r}{0.20\textwidth}
\vspace{-0.7cm}
\begin{center}
\includegraphics[width=0.20\textwidth]{figures/causal_graph.pdf}
\end{center}\vspace{-0.4cm}
\caption{Causal graph. Observed variables are colored orange and unobserved blue. We allow for arbitrary dependence between $X$ and $U$.}
\label{fig:causal_graph}
\vspace{-0.9cm}
\end{wrapfigure}

We use the potential outcomes framework to formalize the causal inference problem \citep{Rubin.1974} and denote $Y(a)$ as the potential outcome when intervening on the treatment and setting it to $A = a$. We impose the following standard assumptions \citep{Dorn.2022}.
\begin{assumption}\label{ass:causal}
\emph{We assume that for all $x \in \mathcal{X}$ and $a \in \mathcal{A}$ the following three conditions hold: (i)~$A=a$ implies $Y(a) = Y$(consistency); (ii)~$\mathbb{P}(a \mid x) > 0$ (positivity); and (iii)~$Y(a) \indep A \mid X, U$ (latent unconfoundedness).}
\end{assumption}

\textbf{Causal queries:} We are interested in a wide range of general causal queries. We formalize them as functionals $Q(x, a, \mathbb{P}) = \mathcal{F}(\mathbb{P}(Y(a) \mid x))$, where $\mathcal{F}$ is a functional that maps the potential outcome distribution $\mathbb{P}(Y(a) \mid x)$ to a real number \citep{Frauen.2023c}. Thereby, we cover various queries from the causal inference literature. For example, by setting $\mathcal{F} = \mathbb{E}[\cdot]$, we obtain the conditional expected potential outcomes/ dose-response curves $Q(x, a, \mathbb{P}) = \mathbb{E}[Y(a) \mid x]$. We can also obtain distributional versions of these queries by setting $\mathcal{F}$ to a quantile instead of the expectation. Furthermore, our methodology will also apply to queries that can be obtained by averaging or taking differences. For binary treatments $A \in \{0,1\}$, the query $\tau(x) = \mathbb{E}[Y(1) \mid x] - \mathbb{E}[Y(0) \mid x]$ is called the conditional average treatment effect (CATE), and its averaged version $\int \tau(x) \mathbb{P}(x) \diff x$ the average treatment effect (ATE).

Our formalization also covers simultaneous effects on multiple outcomes (i.e., $d_y \geq 2$). Consider query $Q(x, a, \mathbb{P}) = \mathbb{P}(Y(a) \in \mathcal{S} \mid x)$, which is the probability that the outcome $Y(a)$ is contained in some set $\mathcal{S} \subseteq \mathcal{Y}$ after intervening on the treatment. For example, consider two potential outcomes $Y_1(a)$ and $Y_2(a)$ denoting blood pressure and heart rate, respectively. We then might be interested in $\mathbb{P}(Y_1(a) \leq t_1, Y_2(a) \leq t_2 \mid x)$, where $t_1$ and $t_2$ are critical threshold values (see Sec.~\ref{sec:experiments}).

%We are also interested in averages of the form $Q(a, \mathbb{P}) = \int Q(x, a, \mathbb{P}) \mathbb{P}(x) \diff x$ and differences (effects) of the form $Q(x, a_1, a_2, \mathbb{P})$ = $Q(x, a_1, \mathbb{P}) - Q(x, a_2, \mathbb{P})$ for $a_1, a_2 \in \mathcal{A}$. Finally, we are interested in sensitivity analysis for the potential outcome distribution $\mathbb{P}(Y(a) \mid x)$ itself. 


\subsection{Causal sensitivity analysis}

Causal sensitivity analysis builds upon sensitivity models that restrict the possible strength of unobserved confounding \citep[e.g.,][]{Rosenbaum.1983}. Formally, we define a sensitivity model as a family of distributions of $(X, U, A, Y)$ that induce the observational distribution $\mathbb{P}_\mathrm{obs}$.

\begin{definition}
\emph{A sensitivity model $\mathcal{M}$ is a family of probability distributions $\mathbb{P}$ defined on $\mathcal{X} \times \mathcal{U} \times \mathcal{A} \times \mathcal{Y}$ for arbitrary finite-dimensional $\mathcal{U}$ so that $\int_\mathcal{U} \mathbb{P}(x, u, a, y) \diff u = \mathbb{P}_\mathrm{obs}(x, a, y)$ for all $\mathbb{P} \in \mathcal{M}$.}
\end{definition}

\textbf{Task:} Given a sensitivity model $\mathcal{M}$ and an observational distribution $\mathbb{P}_\mathrm{obs}$, the aim of causal sensitivity analysis is to solve the partial identification problem
\begin{equation}\label{eq:partial_identification}
    Q^+_\mathcal{M}(x, a) =  \sup_{\mathbb{P} \in \mathcal{M}} \, Q(x, a, \mathbb{P}) \quad \text{and} \quad Q^-_\mathcal{M}(x, a) =  \inf_{\mathbb{P} \in \mathcal{M}} \, Q(x, a, \mathbb{P}).
\end{equation}
By its definition, the interval $[Q^-_\mathcal{M}(x, a), Q^+_\mathcal{M}(x, a)]$ is the tightest interval that is guaranteed to contain the ground-truth causal query $Q(x, a, \mathbb{P})$ while satisfying the sensitivity constraints. We can also obtain bounds for averaged causal queries and differences via $\int Q^+_\mathcal{M}(x, a) \mathbb{P}(x) \diff x$ and $Q^+_\mathcal{M}(x, a_1) - Q^-_\mathcal{M}(x, a_2)$ (see Appendix~\ref{app:avg_diff} for details). %In some situations, we might also be interested in a distribution that maximizes/minimizes Eq.~\eqref{eq:partial_identification}. 

\textbf{Sensitivity models from the literature:} We now recap three types of prominent sensitivity models from the literature, namely, the MSM, $f$-sensitivity models, and Rosenbaum's sensitivity model. These are designed for binary treatments $A \in \{0, 1\}$. To formalize them, we first define the odds ratio $\mathrm{OR}(a, b) =  \frac{a}{(1 - a)} \frac{(1 - b)}{b}$, the observed propensity score $\pi(x) = \mathbb{P}(A=1 \mid x)$, and the full propensity score $\pi(x, u) = \mathbb{P}(A=1 \mid x, u)$.\footnote{Corresponding sensitivity models for continuous treatments can be defined by replacing the odds ratio with the density ratio $\mathrm{DR}(a, b) = \frac{a}{b}$ and the propensity scores with the densities $\mathbb{P}(a \mid x)$ and $\mathbb{P}(a \mid x, u)$ \citep{Bonvini.2022, Jesson.2022}. We refer to Appendix~\ref{app:sensitivity} for details and further examples of sensitivity models.} Then, the definitions are:
\begin{enumerate}[leftmargin=7.5mm]
    \item The \emph{marginal sensitivity model} (MSM) \citep{Tan.2006} is defined as the family of all $\mathbb{P}$ that satisfy
$\frac{1}{\Gamma} \leq \mathrm{OR}(\pi(x), \pi(x,u)) \leq \Gamma$ for all $x \in \mathcal{X}$ and $u \in \mathcal{U}$ and a sensitivity parameter $\Gamma \geq 1$.

\item \emph{$f$-sensitivity models} \citep{Jin.2022} build upon a given a convex function $f \colon \R_{>0} \to \R$ with $f(1) = 0$ and are defined via 
    $\max\big\{ \int_{\mathcal{U}} f\left(\mathrm{OR}(\pi(x), \pi(x,u))\right) \mathbb{P}(u \mid x, A=1) \diff u, \,  \int_{\mathcal{U}} f\left(\mathrm{OR}^{-1}(\pi(x), \pi(x,u))\right) \mathbb{P}(u \mid x, A=1) \diff u\big\} \leq \Gamma$
for all $x \in \mathcal{X}$.

\item \emph{Rosenbaum's sensitivity model} \citep{Rosenbaum.1987} is defined via $\frac{1}{\Gamma} \leq \mathrm{OR}(\pi(x, u_1), \pi(x,u_2)) \leq \Gamma$
for all $x \in \mathcal{X}$ and $u_1, u_2 \in \mathcal{U}$.
\end{enumerate}

\textbf{Interpretation and choice of $\Gamma$:} In the above sensitivity models, the sensitivity parameter $\Gamma$ controls the strength of unobserved confounding. Both MSM and Rosenbaum's sensitivity model bound on odds-ratio uniformly over all $u \in \mathcal{U}$, while the $f$-sensitivity model bounds an integral over $u$. We refer to Appendix~\ref{app:sensitivity} for further differences. Setting $\Gamma = 1$ in the above sensitivity models corresponds to unconfoundedness and thus point identification. For $\Gamma > 1$, point identification is not possible, and we need to solve the partial identification problem from Eq.~\eqref{eq:partial_identification} instead. 

In practice, one typically chooses $\Gamma$ by domain knowledge or data-driven heuristics \citep{Kallus.2019, Hatt.2022b}. For example, a common approach in practice is to determine the smallest $\Gamma$ so that the partially identified interval $[Q^-_\Gamma(x, a), Q^+_\Gamma(x, a)]$ includes $0$. Then, $\Gamma$ can be interpreted as a level of ``causal uncertainty'', quantifying the smallest violation of unconfoundedness that would explain away the causal effect \citep{Jesson.2021, Jin.2023}.



\section{The generalized treatment sensitivity model (GTSM)}\label{sec:general_sensitivity}

We now define our generalized treatment sensitivity model (GTSM). The GTSM subsumes a large class of sensitivity models and includes MSM, $f$-sensitivity, and Rosenbaum's sensitivity model).

\textbf{Motivation:} Intuitively, we define the GTSM so that it includes all sensitivity models that restrict the latent distribution shift in the confounding space due to the treatment intervention (see Fig.~\ref{fig:dist_shift_sketch}). To formalize this, we can write the observational outcome density under Assumption~\ref{ass:causal} as
\begin{equation}\label{eq:obs_dist}
    \mathbb{P}_\mathrm{obs}(y \mid x, a) = \int \mathbb{P}(y \mid x, u, a) \color{red} \, \mathbb{P}(u \mid x, a) \color{black} \diff u.
\end{equation}
When intervening on the treatment, we remove the $U$--$A$ edge in the corresponding causal graph (Fig.~\ref{fig:dist_shift_sketch}) and thus artificially remove dependence between $U$ and $A$. Formally, we can write the potential outcome density under Assumption~\ref{ass:causal} as
\begin{equation}\label{eq:int_dist}
    \mathbb{P}(Y(a) = y \mid x) = \int \mathbb{P}(Y(a) = y \mid x, u) \mathbb{P}(u \mid x) \diff u = \int \mathbb{P}(y \mid x, u, a) \color{orange} \mathbb{P}(u \mid x) \color{black} \diff u.
\end{equation}
Eq.~\eqref{eq:obs_dist} and~\eqref{eq:int_dist} imply that $\mathbb{P}_\mathrm{obs}(y \mid x, a)$ and $\mathbb{P}(Y(a) = y \mid x)$ only differ by the densities $\color{red} \mathbb{P}(u \mid x, a)$ and $\color{orange} \mathbb{P}(u \mid x)$ under the integrals (colored red and orange). If the distributions $\color{red}\mathbb{P}(U \mid x, a)$ and $\color{orange}\mathbb{P}(U \mid x)$ would coincide, it would hold that $\mathbb{P}(Y(a) = y \mid x) = \mathbb{P}_\mathrm{obs}(y \mid x, a)$ and the potential outcome distribution would be identified. This suggests that we should define sensitivity models by measuring deviations from unconfoundedness via the shift between  $\color{red}\mathbb{P}(U \mid x, a)$ and $\color{orange}\mathbb{P}(U \mid x)$.

\begin{definition}
\emph{A generalized treatment sensitivity model (GTSM) is a sensitivity model $\mathcal{M}$ that contains all probability distributions $\mathbb{P}$ that satisfy $\mathcal{D}_{x, a}\left(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a)\right) \leq \Gamma$ for a functional of distributions $\mathcal{D}_{x, a}$, a sensitivity parameter $\Gamma \in \R_{\geq 0}$, and all $x \in \mathcal{X}$ and $a \in \mathcal{A}$.}
\end{definition}
\begin{lemma}\label{lem:gtsm}
The MSM, the $f$-sensitivity model, and Rosenbaum's sensitivity model are GTSMs.
\end{lemma}
\vspace{-0.45cm}
\begin{proof}
    See Appendix~\ref{app:proofs}.
\end{proof}
The class of all GTSMs is still too large for meaningful sensitivity analysis. This is because the sensitivity constraint may not be invariant w.r.t. transformations (e.g., scaling) of the latent space $\mathcal{U}$.
\begin{definition}[Transformation-invariance]\label{def:transformation}
\emph{A GTSM $\mathcal{M}$ is transformation-invariant if it satisfies $\mathcal{D}_{x, a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a)) \geq \mathcal{D}_{x, a}(\mathbb{P}(t(U) \mid x), \mathbb{P}(t(U) \mid x, a))$ for any measurable function $t \colon \mathcal{U} \to \widetilde{\mathcal{U}}$ to another latent space $\widetilde{\mathcal{U}}$.}
\end{definition}

Transformation-invariance is necessary for meaningful sensitivity analysis because it implies that once we choose a latent space $\mathcal{U}$ and a sensitivity parameter $\Gamma$, we cannot find a transformation to another latent space $\widetilde{\mathcal{U}}$ so that the induced distribution on $\widetilde{\mathcal{U}}$ violates the sensitivity constraint. 
%Hence, it implies a type of invariance of the GTSM with respect to the latent space $\mathcal{U}$, its marginal distribution, and its dimensionality. 
All sensitivity models we consider in this paper are transformation-invariant, as stated below.

\begin{lemma}\label{lem:transformation}
The MSM, $f$-sensitivity models, and Rosenbaum's sensitivity model are transformation-invariant.
\end{lemma}
\vspace{-0.45cm}
\begin{proof}
    See Appendix~\ref{app:proofs}.
\end{proof}

\section{Neural causal sensitivity analysis}

We now introduce our neural approach to causal sensitivity analysis as follows. First, we simplify the partial identification problem from Eq.~\eqref{eq:partial_identification} under a GTSM and propose a (model-agnostic) two-stage procedure (Sec.~\ref{sec:two_stage}). Then, we provide theoretical guarantees for our two-stage procedure (Sec.~\ref{sec:theoretical_guaranteess}). Finally, we instantiate our neural framework called \frameworkname (Sec.~\ref{sec:neural}).


\subsection{Sensitivity analysis under a GTSM}\label{sec:two_stage}

\textbf{Motivation:} Recall that, by definition, a GTSM imposes constraints on the distribution shift in the latent confounders due to treatment intervention (Fig.~\ref{fig:dist_shift_sketch}). Our idea is to propose a two-stage procedure, where Stage~1 learns the observational distribution (Fig.~\ref{fig:dist_shift_sketch}, left), while Stage~2 learns the shifted distribution of $U$ after intervening on the treatment under a GTSM (Fig.~\ref{fig:dist_shift_sketch}, right). In Sec.~\ref{sec:theoretical_guaranteess}, we will see that, under weak assumptions, learning this distribution shift in separate stages is guaranteed to lead to the bounds $Q^+_\mathcal{M}(x, a)$ and $Q^-_\mathcal{M}(x, a)$. To formalize this, we start by simplifying the partial identification problem from Eq.~\eqref{eq:partial_identification} for a GTSM $\mathcal{M}$.
%\textbf{Partial identification problem:}

\textbf{Simplifying Eq.~\eqref{eq:partial_identification}:} We begin by rewriting Eq.~\eqref{eq:partial_identification} using the GTSM definition. Without loss of generality, we consider the upper bound $Q^+_\mathcal{M}(x, a)$. Recall that Eq.~\eqref{eq:partial_identification} seeks to maximize over all probability distributions that are compatible both with the observational data and with the sensitivity model. However, note that any GTSM only restricts the $U$--$A$ part of the distribution, not the $U$--$Y$ part. Hence, we can use Eq.~\eqref{eq:int_dist} and Eq.~\eqref{eq:obs_dist} to write the upper bound as
\begin{equation}\label{eq:alternating_opt}
Q^+_\mathcal{M}(x, a) = \hspace{-0.8cm} 
    \sup_{\color{black}\substack{\left\{\mathbb{P}(U \mid x, a^\prime)\right\}_{a^\prime \neq a} \color{black} \\ \text{s.t. }\mathcal{D}_{x, a}(\color{red}\mathbb{P}(U \mid x) \color{black}, \mathbb{P}(U \mid x, a)) \leq \Gamma \\ \text{and } \color{red}\mathbb{P}(u \mid x) \color{black}  = \int \mathbb{P}(u \mid x, a) \color{black} \mathbb{P}_\mathrm{obs}(a \mid x) \diff a}} 
    \hspace{-0.5cm}
    \color{black}\sup_{\substack{\mathbb{P}(U \mid x, a), \; \color{blue} \left\{\mathbb{P}(Y \mid x, u, a)\right\}_{u \in \mathcal{U}} \color{black} \\ \text{s.t. Eq.~\eqref{eq:obs_dist} holds}}}
    \hspace{-0.2cm}
    \mathcal{F} \left(\int \color{blue} \mathbb{P}(Y \mid x, u, a) \color{red} \mathbb{P}(u \mid x) \color{black} \diff u\right),
\end{equation}
where we maximize over (families of) probability distributions on $\mathcal{U}$ and $\mathcal{Y}$ (colored \color{red}red \color{black}and \color{blue}blue\color{black}). Here, the constraint in the right supremum ensures that the respective components of the full distribution $\mathbb{P}$ are compatible with the observational data, while the constraints in the left supremum ensure that the respective components are compatible with both observational data and the sensitivity model.

%Simplyfing the optimization
The partial identification problem from Eq.~\eqref{eq:alternating_opt} is still hard to solve as it involves two nested constrained optimization problems. However, it turns out that we can further simplify Eq.~\eqref{eq:alternating_opt}: We will show in Sec.~\ref{sec:theoretical_guaranteess} that we can replace the right supremum with fixed distributions $\mathbb{P}^\ast(U \mid x, a)$ and $\mathbb{P}^\ast(Y \mid x, a, u)$ for all $u \in \mathcal{U} \subseteq \R^{d_y}$ so that Eq.~\eqref{eq:obs_dist} holds. Then, Eq.~\eqref{eq:alternating_opt} reduces to a single constrained optimization problem (left supremum). Moreover, we will show that we can choose $\mathbb{P}^\ast(Y \mid x, a, u) = \delta(Y - f^\ast_{x, a}(u))$ as a delta-distribution induced by an invertible function $f^\ast_{x, a} \colon \mathcal{U} \to \mathcal{Y}$. The constraint in Eq.~\eqref{eq:obs_dist} that ensures compatibility with the observational data then reduces to $\mathbb{P}_\mathrm{obs}(Y \mid x, a) =  \mathbb{P}^\ast(f^\ast_{x, a}(U) \mid x, a)$. This motivates our following two-stage procedure.

%Two stage approach
\textbf{Two-stage procedure:} In \textbf{Stage~1}, we fix $\mathbb{P}^\ast(U \mid x, a)$ and fix an invertible function $\mathcal{U} \to \mathcal{Y}$ so that $\mathbb{P}_\mathrm{obs}(Y \mid x, a) =  \mathbb{P}^\ast(f^\ast_{x, a}(U) \mid x, a)$ holds. That is, the induced push-forward distribution of $\mathbb{P}^\ast(U \mid x, a)$ under $f^\ast_{x, a}$ must coincide with the observational distribution $\mathbb{P}_\mathrm{obs}(Y \mid x, a)$. The existence of such a function is always guaranteed \citep{Chen.2000}. In \textbf{Stage~2}, we then set $\mathbb{P}(U \mid x, a) = \mathbb{P}^\ast(U \mid x, a)$ and $\color{blue}\mathbb{P}(Y \mid x, a, u) \color{black}= \mathbb{P}^\ast(Y \mid x, a, u)$ in Eq.~\eqref{eq:alternating_opt} and only optimize over the left supremum. That is, we can write stage 2 for discrete treatments as
\begin{equation}
 \sup_{\color{black}\substack{\mathbb{P}(u \mid x, A \neq a) \\ \text{s.t. } \color{red}\mathbb{P}(u \mid x) \color{black}= \mathbb{P}^\ast(u \mid x, a) \mathbb{P}_\mathrm{obs}(a \mid x) + \mathbb{P}(u \mid x, A \neq a) \color{black} (1 - \mathbb{P}_\mathrm{obs}(a \mid x)) \\ \text{and }\mathcal{D}_{x, a}( \color{red}\mathbb{P}(U \mid x) \color{black}, \mathbb{P}^\ast(U \mid x, a)) \leq \Gamma }}\color{black}\mathcal{F} \left(\color{red}\mathbb{P}\color{black}(f^\ast_{x, a}(\color{red}U\color{black}) \color{red} \mid x) \color{black} \right).
\end{equation}
\vspace{-0.2cm}
%For continuous treatments, we can directly take the supremum over $\color{red}\mathbb{P}(u \mid x)$.


\subsection{Theoretical guarantees}
\label{sec:theoretical_guaranteess}

We now provide a formal result that our two-stage procedure returns valid solutions to the partial identification problem from Eq.~\eqref{eq:alternating_opt}. The following theorem states that Stage~2 of our procedure is able to attain the optimal upper bound $Q^+_\mathcal{M}(x, a)$ from Eq.~\eqref{eq:alternating_opt}, even after fixing the distributions $\mathbb{P}^\ast(U \mid x, a)$ and $\mathbb{P}^\ast(Y \mid x, a, u)$ as done in Stage~1.


\begin{theorem}[Sufficiency of two-stage procedure]\label{thrm:main}
Let $\mathcal{M}$ be a transformation-invariant GTSM. For fixed $x \in \mathcal{X}$ and $a \in \mathcal{A}$, let $\mathbb{P}^\ast(U \mid x, a)$ be a fixed distribution on $\mathcal{U} = \R^{d_y}$ and $f^\ast_{x, a} \colon \mathcal{U} \to \mathcal{Y}$ a fixed invertible function so that $\mathbb{P}_\mathrm{obs}(Y \mid x, a) =  \mathbb{P}^\ast(f^\ast_{x, a}(U) \mid x, a)$. Let $\mathcal{P}^\ast$ denote the space of all full probability distributions $\mathbb{P}^\ast$ that induce $\mathbb{P}^\ast(U \mid x, a)$ and $\mathbb{P}^\ast(Y \mid x, u, a) = \delta(Y - f^\ast_{x, a}(u))$ and that satisfy $\mathbb{P}^\ast \in \mathcal{M}$. Then, under Assumption~\ref{ass:causal}, it holds that $Q^+_\mathcal{M}(x, a) = \sup_{\mathbb{P}^\ast \in \mathcal{P}^\ast} Q(x, a, \mathbb{P}^\ast)$ and $Q^-_\mathcal{M}(x, a) = \inf_{\mathbb{P}^\ast \in \mathcal{P}^\ast} Q(x, a, \mathbb{P}^\ast)$.
\end{theorem}
\begin{proof}
    See Appendix~\ref{app:proofs}.
\end{proof}
\textbf{Intuition:} In Eq.~\eqref{eq:alternating_opt}, we optimize jointly over all components of the full distribution. This suggests that there are multiple solutions that differ only in the components of unobserved parts of $\mathbb{P}$ (i.e., in $\mathcal{U}$) but lead to the same potential outcome distribution and causal query. Theorem~\ref{thrm:main} states that we may restrict the space of possible solutions by fixing the components $\mathbb{P}^\ast(U \mid x, a)$ and $\mathbb{P}^\ast(Y \mid x, a, u)$, without loosing the ability to attain the optimal upper bound $Q^+_\mathcal{M}(x, a)$ from Eq.~\eqref{eq:alternating_opt}.

However, we cannot pick \emph{any} $\mathbb{P}^\ast(Y \mid x, a, u)$ that satisfies Eq.~\eqref{eq:obs_dist}. For example, any distribution that induces $Y \indep U \mid X, A$ would satisfy Eq.~\eqref{eq:obs_dist}, but implies unconfoundedness and would thus not lead to a valid upper bound $Q^+_\mathcal{M}(x, a)$. Intuitively, we have to choose a $\mathbb{P}(Y \mid x, a, u)$ that induces ``maximal dependence" between $U$ and $Y$ (conditioned on $X$ and $A$), because the GTSM does not restrict this part of the full probability distribution $\mathbb{P}$. Theorem~\ref{thrm:main} states that we may model $\mathbb{P}(Y \mid x, a, u) = \delta(Y - f^\ast_{x, a}(u))$ as a delta-distribution induced by an invertible function $f^\ast_{x, a} \colon \mathcal{U} \to \mathcal{Y}$.


\subsection{Neural instantiation: \frameworkname}\label{sec:neural}

We now provide a neural instantiation called \frameworkname for the above two-stage procedure using conditional normalizing flows (CNFs) \citep{Winkler.2019}. The architecture of \frameworkname is shown in Fig.~\ref{fig:architecture}. \frameworkname instantiates the two-step procedure as follows:
\begin{wrapfigure}{r}{0.5\textwidth}
 \centering
 \begin{center}
 \includegraphics[width=1\linewidth]{figures/architecture.pdf}
 \end{center}
 \vspace{-0.4cm}
\caption{Architecture of \frameworkname.}
\label{fig:architecture}
 \vspace{-0.8cm}
\end{wrapfigure}

\textbf{Stage 1:} We fix $\mathbb{P}^\ast(U \mid x, a)$ to the standard normal distribution on $\mathcal{U} = \R^{d_y}$. Our task is then to learn an invertible function $f^\ast_{x, a} \colon \mathcal{U} \to \mathcal{Y}$ that maps the standard Gaussian distribution on $\mathcal{U}$ to $\mathbb{P}_\mathrm{obs}(Y \mid x, a)$. We model $f^\ast_{x, a}$ as a CNF $f^\ast_{g^\ast_\theta(x, a)}$, where $f^\ast$ is a normalizing flow \citep{Rezende.2015}, for which the parameters are the output of a fully connected neural network $g^\ast_\theta$, which itself is parametrized by $\theta$ \citep{Winkler.2019}. We obtain $\theta$ by maximizing the empirical Stage~1 loss    $\mathcal{L}_1(\theta) =  \sum_{i=1}^n \log \mathbb{P} ({f^\ast_{g^\ast_\theta(x_i, a_i)}}(U) = y_i)$, where $U \sim \mathcal{N}(0_{d_y}, I_{d_y})$ is standard normally distributed. The stage 1 loss can be computed analytically via the change-of-variable formula (see Appendix~\ref{app:implementation}).

\textbf{Stage 2:} In Stage~2, we need to maximize over distributions on $U$ in the latent space $\mathcal{U}$ that maximize the causal query $\mathcal{F}(\mathbb{P}(f^\ast_{g^\ast_{\theta_\mathrm{opt}}(x, a)}(U) \mid x))$, where $\theta_\mathrm{opt}$ is a solution from maximizing $\mathcal{L}_1(\theta)$ in stage~1. We can do this by learning a {second CNF} $\widetilde{f}_{\widetilde{g}_\eta(x, a)}$, where $\widetilde{f} \colon \widetilde{\mathcal{U}} \to \mathcal{U}$ is a normalizing flow that maps a standard normally distributed auxiliary $\widetilde{U} \sim \mathcal{N}(0_{d_y}, I_{d_y})$ to the latent space $\mathcal{U}$, and whose parameters are the output of a fully connected neural network $\widetilde{g}_\eta$ parametrized by $\eta$. The CNF $\widetilde{f}_{\widetilde{g}_\eta(x, a)}$ from Stage~2 induces a new distribution on $U$, which mimics the shift due to unobserved confounding when intervening instead of conditioning (i.e., going from Eq.~\eqref{eq:obs_dist} to Eq.~\eqref{eq:int_dist}). We can compute the query under the shifted distribution by concatenating the Stage~2 CNF with the Stage~1 CNF and applying $\mathcal{F}$ to the shifted outcome distribution (see Fig.~\ref{fig:architecture}). More precisely, we optimize $\eta$ by maximizing or minimizing the empirical Stage~2 loss 
\vspace{-0.1cm}
\begin{equation}\label{eq:loss_stage2}
\resizebox{.7\hsize}{!}{
    $\mathcal{L}_2(\eta) =  \sum_{i=1}^n \mathcal{F} \left( \mathbb{P} \left({f^\ast_{g^\ast_{\theta_\mathrm{opt}}(x_i, a_i)}}\left( (1 - \xi_{x_i, a_i}) \widetilde{f}_{\widetilde{g}_\eta(x_i, a_i)}(\widetilde{U}) +  \xi_{x_i, a_i} \widetilde{U}\right) \right) \right)$}, 
\end{equation}
\vspace{-0.1cm}
\begin{wrapfigure}{L}{0.46\textwidth}
\begin{minipage}{0.46\textwidth}
\vspace{-0.5cm}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{\footnotesize Learning algorithm for Stage~2}
\label{alg:learning_stage2}
\scriptsize
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{~fitted CNF $f^\ast_{g^\ast_{\theta_\mathrm{opt}}(x, a)}$ from Stage~1, GTSM $\mathcal{M}$}
\Output{~fitted CNF $\widetilde{f}_{\widetilde{g}_{\eta_\mathrm{opt}}(x, a)}$ from Stage~2}
Initialize $\eta^{(1)}$, $\lambda^{(1)}$, and $\mu^{(1)}$\;
\For{$i \in \{1, \dots, n_1\}$}{
\For{$\ell \in \{1, \dots, n_2\}$}{
$\eta^{(i)}_{1} \gets \eta^{(i)}$\;
$(x, a) \gets$ batch of size $n_\mathrm{b}$\;
$\widetilde{u} \gets (\widetilde{u}^{(j)}_{x, a})_{j=1}^k \overset{i.i.d.}{\sim} \mathcal{N}(0_{d_y}, I_{d_y})$\;
$\xi \gets(\xi^{(j)}_{x, a})_{j=1}^k \overset{i.i.d.}{\sim} \mathrm{Bernoulli}(\mathbb{P}_\mathrm{obs}(a \mid x))$\;
$s_{x, a}(\eta) \gets \Gamma - \hat{\mathcal{D}}_{x, a}(\eta, \widetilde{u})$\;
$\mathcal{L}(\eta, \lambda, \mu) \gets \hat{\mathcal{L}}_2(\eta, \widetilde{u}, \xi) - \sum_{(x, a)} \lambda_{x, a} s_{x, a}(\eta) + \frac{\mu}{2} s_{x, a}^2(\eta) $\;
$\eta^{(i)}_{\ell+1} \gets$ optimization step of $\mathcal{L}(\eta^{(i)}_{\ell}, \lambda^{(i)}, \mu^{(i)})$ w.r.t. $\eta^{(i)}_{\ell}$\;
}
$\eta^{(i+1)} \gets \eta^{(i)}_{n_2}$\;
$\lambda^{(i+1)}_{x, a} \gets \max\left\{0, \lambda^{(i)}_{x, a} - \mu^{(i)} s_{x, a}(\eta^{(i+1)})\right\}$\;
$\mu^{(i+1)} \gets \alpha \mu^{(i)}$ for $\alpha > 1$
}
$\eta_\mathrm{opt} \gets \eta^{(n_1)}$\;
\end{algorithm}
\vspace{-0.9cm}
\end{minipage}
\end{wrapfigure}
where $\xi_{x_i, a_i} = \mathbb{P}_\mathrm{obs}(a_i \mid x_i))$, if $A$ is discrete, and $\xi_{x_i, a_i} = 0$, if $A$ is continuous. 

\textbf{Learning algorithm for stage 2:} There are two remaining challenges we need to address in Stage~2: (i)~optimizing Eq.~\eqref{eq:loss_stage2} does not ensure that the sensitivity constraints imposed by the GTSM $\mathcal{M}$ hold; and (ii)~computing the Stage~2 loss from Eq.~\eqref{eq:loss_stage2} may not be analytically tractable. For (i), we propose to incorporate the sensitivity constraints by using the augmented Lagrangian method \citep{Nocedal.2006}, which has already been successfully applied in the context of partial identification with neural networks \citep{Padh.2023}. For (ii), we propose to obtain samples $\widetilde{u} = (\widetilde{u}^{(j)}_{x, a})_{j=1}^k \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0_{d_y}, I_{d_y})$ and $\xi = (\xi^{(j)}_{x, a})_{j=1}^k \overset{\text{i.i.d.}}{\sim} \mathrm{Bernoulli}(\mathbb{P}_\mathrm{obs}(a \mid x))$ together with Monte Carlo estimators $\hat{\mathcal{L}}_2(\eta, \widetilde{u}, \xi)$ of the Stage~2 loss $\mathcal{L}_2(\eta)$ and $\hat{\mathcal{D}}_{x, a}(\eta, \widetilde{u})$ of the sensitivity constraint $\mathcal{D}_{x, a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a))$. A sketch of the learning algorithm is shown in Alg.~\ref{alg:learning_stage2}. We refer to Appendix~\ref{app:learning_alg} for details, including instantiations of our framework for numerous sensitivity models and causal queries.

\textbf{Analytical potential outcome density:} Once our model is trained, we can not only compute the bounds from Alg.~\ref{alg:learning_stage2} via sampling but also the analytical form (by using the density transformation formula) of the potential outcome density that gives rise to that bound. Fig.~\ref{fig:real}, shows an example. This makes it possible to perform sensitivity analysis for the potential outcome density itself, i.e., analyzing the ``distribution shift due to intervention".

%\textbf{Empirical uncertainty:} The aim of our method is to solve the partial identification problem from Eq.~\eqref{eq:partial_identification} in population, under access to the observed distribution $\mathbb{P}_\mathrm{obs}(x, a, y)$. In practice, we may also want to account for empirical uncertainty, i.e., uncertainty arising due to estimating $\mathbb{P}_\mathrm{obs}(x, a, y)$ from finite data. A straightforward way to do so would be to employ a percentile bootstrap approach to Stage~1 of our framework, similar to \citep{Jesson.2022}. Another direction for future work would be to extend our framework to a Bayesian framework, which has been shown to be able to effectively account for uncertainty due to lack of data and overlap violations in treatment effect settings \citep{Jesson.2020}.


\textbf{Implementation:} We use autoregressive neural spline flows \citep{Durkan.2019, Dolatabadi.2020}. For estimating propensity scores $\mathbb{P}_\mathrm{obs}(a \mid x)$, we use fully connected neural networks with softmax activation. We perform training using the Adam optimizer \citep{Kingma.2015}. Details are in Appendix~\ref{app:implementation}.

\section{Experiments}\label{sec:experiments}
 
\begin{wrapfigure}{r}{0.6\textwidth}
\vspace{-0.4cm}
 \centering
\begin{subfigure}{0.30\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_binary_1D.pdf}
\end{subfigure}%
\begin{subfigure}{0.30\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_continuous_1D_a05.pdf}
\end{subfigure}
\vspace{-0.8cm}
\caption{Validating the correctness of \frameworkname (ours) by comparing with optimal closed-form solutions (CF) for the MSM on simulated data. \emph{Left:} Dataset 1, binary treatment. \emph{Right:} Dataset 2, continuous treatment. Reported: mean $\pm$ standard deviation over 5 runs.}
\label{fig:simulation_msm}
\vspace{-0.2cm}
\end{wrapfigure}
We now demonstrate the effectiveness of \frameworkname for causal sensitivity analysis empirically.
As is common in the causal inference literature, we use synthetic and semi-synthetic data with known causal ground truth to evaluate \frameworkname \citep{Kallus.2019, Jesson.2022}. We proceed as follows: (i)~We use synthetic data to show the validity of bounds from \frameworkname under multiple sensitivity models, treatment types, and causal queries. We also show that for the MSM, the \frameworkname bounds coincide with known optimal solutions. (ii)~We show the validity of the \frameworkname bounds using a semi-synthetic dataset. (iii)~We show the applicability of \frameworkname in a case study using a real-world dataset with multiple outcomes, which cannot be handled by previous approaches. We refer to Appendix~\ref{app:avg_diff} for details regarding datasets and experimental evaluation, and to Appendix~\ref{app:more_exp} for additional experiments.



\textbf{(i)~Synthetic data:} We consider two synthetic datasets of sample size $n=10000$ inspired from previous work on sensitivity analysis: Dataset~1 is adapted from \citet{Kallus.2019} and has a binary treatment $A \in \{0,1\}$. The data-generating process follows an MSM with oracle sensitivity parameter $\Gamma^\ast = 2$. We are interested in the CATE $\tau(x) = \E[Y(1) - Y(0) \mid x]$. Dataset~2 is adapted from \citet{Jesson.2022} and has a continuous treatment $A \in [0,1]$. Here, we are interested in the dose-response function $\mu(x, a) = \E[Y(a) \mid x]$, where we choose $a = 0.5$. We report results for further treatment values in Appendix~\ref{app:more_exp}.

\begin{wrapfigure}{l}{0.6\textwidth}
\vspace{-0.5cm}
 \centering
\begin{subfigure}{0.30\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_binary_1D_f.pdf}
\end{subfigure}%
\begin{subfigure}{0.30\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_continuous_1D_f_a05.pdf}
\end{subfigure}
\vspace{-0.7cm}
\caption{Confirming the validity of our \frameworkname bounds for various sensitivity models. \emph{Left:} Dataset 1, binary treatment. \emph{Right:} Dataset 2, continuous treatment. Reported: mean $\pm$ standard deviation over 5 runs.}
\label{fig:simulation_f}
\vspace{-0.3cm}
\end{wrapfigure}

We first compare our \frameworkname bounds with existing results closed-form bounds (CF) for the MSM \citep{Dorn.2022, Frauen.2023c}, which have been proven to be optimal. We plot both \frameworkname and the CF for both datasets and three choices of sensitivity parameter $\Gamma \in \{2, 4, 10\}$ (Fig.~\ref{fig:simulation_msm}). Our bounds almost coincide with the optimal CF solutions, which confirms that \frameworkname learns optimal bounds under the MSM.

We also show the validity of our \frameworkname bounds for Rosenbaum's sensitivity model and the following $f$-sensitivity models: Kullbach-Leibler (KL, $f(x) = x \log(x)$), Total Variation (TV, $f(x) = 0.5 |x-1|$), Hellinger (HE, $f(x) = (\sqrt{x} - 1)^2$), and Chi-squared ($\chi^2$, $f(x) = (x-1)^2$). To do so, we choose the ground-truth sensitivity parameter $\Gamma^\ast$ for each sensitivity model that satisfies the respective sensitivity constraint (see Appendix~\ref{app:details_exp} for details). The results are in Fig.~\ref{fig:simulation_f}. We make the following observations: (i)~all bounds cover the causal query on both datasets, thus confirming the validity of \frameworkname. (ii)~For Dataset~1, the MSM returns the tightest bounds because our simulation follows an MSM.

\textbf{(ii)~Semi-synthetic data:}
We create a semi-synthetic dataset using MIMIC-III \citep{Johnson.2016}, which includes electronic health records from patients admitted to intensive care units. We extract $8$ confounders and a binary treatment (mechanical ventilation). Then, we augment the data with a synthetic unobserved confounder and outcome. We obtain $n=14719$ patients and split the data into train (80\%), val (10\%), and test (10\%). For details, see Appendix~\ref{app:details_exp}.

We verify the validity of our \frameworkname bounds for CATE in the following way: For each sensitivity model, we obtain the smallest oracle sensitivity parameter $\Gamma^\ast$ that guarantees coverage (i.e., satisfies the respective sensitivity constraint) for 50\% of the test samples. Then, we plot the coverage and median interval length of the \frameworkname bounds over the test set. The results are in Table~\ref{tab:semi_synthetic}.
\begin{wrapfigure}{l}{0.25\textwidth}
\vspace{-0.4cm}
 \centering
 \begin{center}
 \includegraphics[width=1\linewidth]{figures/plt_densities.pdf}
 \end{center}
 \vspace{-0.5cm}
\caption{Analytic stage 2 densities for MSM and KL-sensitivity model (upper bounds).}
\label{fig:semi_synthetic}
\vspace{-0.3cm}
\end{wrapfigure}
We observe that (i)~all bounds achieve at least 50\% coverage, thus confirming the validity of the bounds, and (ii)~some sensitivity models (e.g., the MSM) are conservative, i.e., achieve much higher coverage and interval length than needed. This is because the sensitivity constraints of these models do not adapt well to the data-generating process, thus the need for choosing a large $\Gamma^\ast$ to guarantee coverage. This highlights the importance of choosing a sensitivity model that captures the data-generating process well. For further details, we refer to \citep{Jin.2022}.
\begin{wraptable}{r}{0.4\textwidth}
\vspace{-0.4cm}
\caption{Results for semi-synthetic data}
\vspace{-0.4cm}
\label{tab:semi_synthetic}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{lcc}
\noalign{\smallskip} \toprule \noalign{\smallskip}
{Sensitivity model}& Coverage & Interval length\\
\midrule
MSM $\Gamma^\ast = 5.48$ & $0.91 \pm 0.03$ & $0.77 \pm 0.03$ \\
KL $\Gamma^\ast = 0.25$ & $0.54 \pm 0.07$ & $0.31 \pm 0.01$ \\
TV $\Gamma^\ast = 0.38$ & $0.86 \pm 0.09$ & $0.83 \pm 0.14$ \\
HE $\Gamma^\ast = 0.18$ & $0.83 \pm 0.06$ & $0.63 \pm 0.03$ \\
$\chi^2$ $\Gamma^\ast = 0.68$ & $0.67 \pm 0.07$ & $0.41 \pm 0.01$ \\
RB $\Gamma^\ast = 14.42$ & $0.79 \pm 0.07$ & $0.56 \pm 0.03$ \\
\bottomrule
\multicolumn{3}{l}{Reported: mean $\pm$ standard deviation ($5$ runs).}
\end{tabular}}
\vspace{-0.3cm}
\end{wraptable}
We also provide further insights into the difference between two exemplary sensitivity models: the MSM and the KL-sensitivity model. To do so, we plot the observational distribution from stage 1 together with the shifted distributions from stage 2 that lead to the respective upper bound for a fixed test patient (Fig.~\ref{fig:semi_synthetic}). The distribution shift corresponding to the MSM is a step function, which is consistent with results from established literature \citep{Jin.2023}. This is in contrast to the smooth distribution shift obtained by the KL-sensitivity model. In addition, this example illustrates the possibility of using \frameworkname for sensitivity analysis on the \emph{entire interventional density}.

\textbf{(iii)~Case study using real-world data:} We now demonstrate an application of \frameworkname to perform causal sensitivity analysis for an interventional distribution on multiple outcomes. To do so, we use the same MIMIC-III data from our semi-synthetic experiments but add two outcomes: heart rate ($Y_1$) and blood pressure ($Y_2$). We consider the causal query $\mathbb{P}(Y_1(1) \geq 115, Y_2(1) \geq 90 \mid X = x)$, i.e., the joint probability of achieving a heart rate higher than $115$ and a blood pressure higher than $90$ under treatment intervention (``danger area''). We consider an MSM and train \frameworkname with sensitivity parameters $\Gamma \in \{2, 4\}$. Then, we plot the stage 1 distribution together with both stage 2 distributions for a fixed, untreated patient from the test set in Fig.~\ref{fig:real}.

\begin{wrapfigure}{r}{0.76\textwidth}
\vspace{-0.1cm}
 \centering
\begin{subfigure}{0.25\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_real_stage1.pdf}
\end{subfigure}%
\begin{subfigure}{0.25\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_real_stage2_upper_gamma2.pdf}
\end{subfigure}
\begin{subfigure}{0.25\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_real_stage2_upper_gamma4.pdf}
\end{subfigure}
\caption{Contour plots of 2D densities obtained by \frameworkname under an MSM. Here, we aim to learn an upper bound of the causal query $\mathbb{P}(Y_1(1) \geq 115, Y_2(1) \geq 90 \mid X = x_0)$ for a test patient $x_0$. \emph{Left:} Stage 1/ observational distribution. \emph{Middle:} Stage 2, $\Gamma = 2$. \emph{Right:} Stage 2, $\Gamma = 4$.}
\label{fig:real}
\vspace{-0.2cm}
\end{wrapfigure}
As expected, increasing $\Gamma$ leads to a distribution shift in the direction of the ``danger area'', i.e., high heart rate and high blood pressure. For $\Gamma = 2$, there is only a moderate fraction of probability mass inside the danger area, while, for $\Gamma = 4$, this fraction is much larger. A practitioner may potentially decide against treatment if there are other unknown factors (e.g., undetected comorbidity) that could result in a confounding strength of $\Gamma = 4$.

%if a confounding strength of $\Gamma = 4$ can not be excluded by domain knowledge.

\section{Conclusion}

From a methodological perspective, \frameworkname offers new ideas to causal sensitivity analysis and partial identification: In contrast to previous methods, \frameworkname explicitly learns a latent distribution shift due to treatment intervention. We refer to Appendix~\ref{app:discussion} for a discussion on limitations and future work. From an applied perspective, \frameworkname enables practitioners to perform causal sensitivity analysis in numerous settings, including multiple outcomes. Furthermore, it allows for choosing from a wide variety of sensitivity models, which may be crucial to effectively incorporate domain knowledge about the data-generating process.


%\section{Discussion}

%\textbf{Limitations:} \frameworkname is a versatile framework that can approximate the bounds of causal effects in various settings. However, there are a few settings where (optimal) closed-form bounds exist (e.g., CATE for binary treatments under the MSM), which should be preferred when available. Instead, our \frameworkname offers a unified framework for causal sensitivity analysis under various sensitivity models, treatment types, and causal queries, and can be applied in settings where closed-form solutions have not been derived or do not exist (Table~\ref{tab:rw}).

%\textbf{Future work:} Our research hints at the broad applicability of \frameworkname beyond the three sensitivity models that we discussed above (see also Appendix~\ref{app:sensitivity}). For future work, it would be interesting to conduct a comprehensive comparison of sensitivity models and provide practical recommendations for their usage. Future work may further consider incorporating techniques from semiparametric statistical theory in order to obtain estimation guarantees, robustness properties, and confidence intervals.



\clearpage

\bibliography{bibliography.bib}
\bibliographystyle{iclr2024_conference}

\clearpage

\appendix
\section{Extended related work}\label{app:rw}

In the following, we provide an extended related work. Specifically, we elaborate on (1)~a systematic overview of causal sensitivity analysis, (2)~its application in settings beyond partial identification of interventional causal queries, and (3)~point identification and estimation of causal queries.

\subsection{A systematic overview on causal sensitivity analysis}

In Table~\ref{tab:app_rw}, we provide a systematic overview of existing works for causal sensitivity analysis, which we group by the underlying sensitivity model, the treatment type, and the causal query. As such, Table~\ref{tab:app_rw} extends Table~\ref{tab:rw} in that we follow the same categorization but now point to key references from the literature.

\begin{table}[ht]
\caption{Overview of key works for causal sensitivity analyses under the MSM, $f$-sensitivity models, or Rosenbaum's sensitivity model. Settings with no existing literature are indicated with a red cross (\redcross). Treatments are either binary or continuous. \frameworkname framework is applicable in all settings.
}
\label{tab:app_rw}
\vspace{-0.3cm}
\resizebox{1\textwidth}{!}{
\begin{tabular}{lcccccc}
\noalign{\smallskip} \toprule \noalign{\smallskip}
\backslashbox{Causal query}{Sensitivity model}& \multicolumn{2}{c}{MSM} & \multicolumn{2}{c}{$f$-sensitivity} & \multicolumn{2}{c}{Rosenbaum}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} & Binary & Cont.\textsuperscript{($^\dagger$)} & Binary & Cont. & Binary & Cont.\\
\midrule
CATE & \cite{Tan.2006} & \cite{Bonvini.2022}  & \cite{Jin.2022} &  \redcross & \cite{Rosenbaum.1983b}  & \redcross  \\
 & \cite{Kallus.2019} & \cite{Jesson.2022}  &  &  & \cite{Rosenbaum.1987} &   \\
& \cite{Zhao.2019} & \cite{Frauen.2023c}  &  &  & \cite{Heng.2021}  &  \\
  & \cite{Jesson.2021} &   &  &  &  &  \\
  & \cite{Dorn.2022} &   &  &  &  &  \\
  & \cite{Dorn.2022b} &   &  &  &  &  \\
  & \cite{Oprescu.2023} &   &  &  &  &  \\
    & \cite{Soriano.2023} &   &  &  &  &  \\
\midrule
Distributional effects & \cite{Frauen.2023c} & \citep{Frauen.2023c} & \redcross& \redcross & \redcross& \redcross\\
\midrule
Interventional density & \cite{Jin.2023} & \cite{Frauen.2023c}  & \cite{Jin.2022}  & \redcross & \redcross  & \redcross  \\
 & \cite{Yin.2022} &  & &  & & \\
  & \cite{Marmarelis.2023b} &  & &  & & \\
 & \cite{Frauen.2023c} &  & &  & & \\
\midrule
Multiple outcomes & \redcross & \redcross & \redcross & \redcross &\redcross & \redcross  \\
\bottomrule
\multicolumn{7}{p{1\textwidth}}{\tiny ($^\dagger$) The MSM for continuous treatment is also called continuous MSM (CMSM) \citep{Jesson.2022}.}\\
\end{tabular}}
\end{table}

Evidently, most work has focussed on sensitivity analysis for CATE in binary treatment settings. For many settings, such as $f$-sensitiivity and Rosenbaum's sensitivity model with continuous treatments or multiple outcomes, no previous work exists. Here, \frameworkname is the first work that allows for computing bounds in these settings.

\subsection{Sensitivity analysis in other causal settings}

Causal sensitivity analysis has found applicability not only in addressing the partial identification problem, as discussed in Eq.~\eqref{eq:partial_identification} but also in various domains of machine learning and causal inference. We briefly highlight some notable instances where ideas from causal sensitivity analysis have made substantial contributions.

One such stream of literature is off-policy learning, where sensitivity models have been leveraged to account for unobserved confounding or distribution shifts \citep{Kallus.2018c, Hatt.2022b}. Here, sensitivity analysis enables robust policy learning. Another example is algorithmic fairness, where sensitivity analysis has been used to study causal fairness notions (e.g., counterfactual fairness) under unobserved confounding \citep{Kilbertus.2019}. Finally have been used to study the partial identification of counterfactual queries \citep{Melnychuk.2023b}


\subsection{Point identification and estimation}

If we replace the latent unconfoundedness assumption in Assumtion~\ref{ass:causal} with (non-latent) unconfoundedness, that is,
\begin{equation}
    Y(a) \indep A \mid X \quad \text{for all} \quad a \in \mathcal{A},
\end{equation}
we can point-identify the distribution of the potential outcomes via
\begin{equation}
    \mathbb{P}(Y(a) = y \mid x) = \mathbb{P}(Y(a) = y \mid x, a)  = \mathbb{P}_\mathrm{obs}(y \mid x, a).
\end{equation}
Hence, inferring the causal query $Q(x, a, \mathbb{P})$ reduces to a purely statistical inference problem, i.e., estimating $\mathcal{F}(\mathbb{P}_\mathrm{obs}(Y \mid x, a))$ from finite data.

Various methods for estimating point-identified causal effects under unconfoundedness have been proposed. In recent years, a particular emphasis was on methods for estimating (conditional) average treatment effects (CATEs) that make use of machine learning to model flexible non-linear relationships within the data. Examples include forest-based methods \citep{Wager.2018} and deep learning \citep{Johansson.2016, Shalit.2017, Yoon.2018, Shi.2019}. Another stream of literature incorporates theory from semiparametric statistics and provides robustness and efficiency guarantees \citep{vanderLaan.2006, Chernozhukov.2018, Kunzel.2019, Curth.2021, Kennedy.2022d}. Beyond CATE, methods have also been proposed for estimating distributional effects or potential outcome densities \citep{Chernozhukov.2013, Muandet.2021, Kennedy.2023, Melnychuk.2023}. We emphasize that all these methods focus on \emph{estimation} of \emph{point-identified} causal queries, while we are interested in \emph{causal sensitivity analysis} and thus \emph{partial identification} under violations of the unconfoundedness assumption.

\clearpage

\section{Proofs}\label{app:proofs}

\subsection{Proof of Lemma~\ref{lem:gtsm}}

We provide a proof for the following more detailed version of Lemma~\ref{lem:gtsm}.

\begin{lemma}\label{lem:app_gtsm}
The MSM, the $f$-sensitivity model, and Rosenbaum's sensitivity model are GTSMs with sensitivity parameter $\Gamma$. Let $\rho(x, u, a) = \frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{\mathbb{P}(u \mid x)}{\mathbb{P}(u \mid x, a)} - \mathbb{P}(a \mid x) \right)$ and $\rho(x, u_1, u_2, a) = \frac{\mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x) - \mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x, a) \mathbb{P}(a \mid x)}{\mathbb{P}(u_2 \mid x, a) \mathbb{P}(u_1 \mid x) - \mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x, a) \mathbb{P}(a \mid x)}$. For the MSM, we have
\begin{equation}
\mathcal{D}_{x,a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a)) = \max\left\{\sup_{u \in \mathcal{U}} \rho(x, u, a), \, \sup_{u \in \mathcal{U}} \rho(x, u, a)^{-1} \right\}.
\end{equation}
For $f$-sensitivity models, we have 
\begin{equation}
\mathcal{D}_{x, a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a)) = \max\left\{\int_{\mathcal{U}} f(\rho(x, u, a)) \mathbb{P}(u \mid x, a) \diff u, \, \int_{\mathcal{U}} f(\rho(x, u, a)^{-1}) \mathbb{P}(u \mid x, a) \diff u\right\}.
\end{equation}
For Rosenbaum's sensitivity model, we have
\begin{equation}
\mathcal{D}_{x, a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a)) = \max\left\{\sup_{u_1, u_2 \in \mathcal{U}} \rho(x, u_1, u_2, a), \, \sup_{u_1, u_2 \in \mathcal{U}} \rho(x, u_1, u_2, a)^{-1} \right\}.   
\end{equation}
\end{lemma}

\begin{proof}
We show that all three sensitivity models (MSM, $f$-sensitivity models, and Rosenbaum's sensitivity model) are GTSMs. Recall that the odds ratio is defined as $\mathrm{OR}(a, b) =  \frac{a}{(1 - a)} \frac{(1 - b)}{b}$.

\textbf{MSM:} Using Bayes' Theorem, we obtain $\mathbb{P}(u \mid x, a) = \frac{\mathbb{P}(a \mid x, u) \mathbb{P}(u \mid x)}{\mathbb{P}(a \mid x)}$ and therefore
\begin{align}
\rho(x, u, a) &=  \frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{\mathbb{P}(u \mid x)}{\mathbb{P}(u \mid x, a)} - \mathbb{P}(a \mid x) \right) \\
&= \frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{\mathbb{P}(a \mid x)}{\mathbb{P}(a \mid x, u)} - \mathbb{P}(a \mid x) \right)\\
&= \frac{\mathbb{P}(a \mid x)}{1 - \mathbb{P}(a \mid x)} \left(\frac{1 - \mathbb{P}(a \mid x, u)}{\mathbb{P}(a \mid x, u)}  \right) \\
&= \mathrm{OR}\left(\mathbb{P}(a \mid x), \mathbb{P}(a \mid x, u)\right)
\end{align}

Hence, $\max\left\{\sup_{u \in \mathcal{U}} \rho(x, u, a), \, \sup_{u \in \mathcal{U}} \rho(x, u, a)^{-1} \right\} \leq \Gamma$ is equivalent to 
\begin{equation}
    \frac{1}{\Gamma} \leq \mathrm{OR}\left(\mathbb{P}(a \mid x), \mathbb{P}(a \mid x, u)\right) \leq \Gamma
\end{equation}
for all $u \in \mathcal{U}$, which reduces to the original MSM defintion for $a=1$.

\textbf{$f$-sensitivity models:} Follows immediately from $\rho(x, u, a) = \mathrm{OR}\left(\mathbb{P}(a \mid x), \mathbb{P}(a \mid x, u)\right)$.

\textbf{Rosenbaum's sensitivity model:} 
We can write 
\begin{align}
\rho(x, u_1, u_2, a) &= \frac{\mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x, a) \mathbb{P}(a \mid x) - \mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x)}{\mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x, a) \mathbb{P}(a \mid x) - \mathbb{P}(u_2 \mid x, a) \mathbb{P}(u_1 \mid x)}\\
&= \frac{\mathbb{P}(a \mid x, u_1) \mathbb{P}(u_1 \mid x)}{\mathbb{P}(a \mid x, u_2) \mathbb{P}(u_2 \mid x)} \left(\frac{\mathbb{P}(a \mid x, u_2) \mathbb{P}(u_2 \mid x) - \mathbb{P}(u_2 \mid x)}{\mathbb{P}(a \mid x, u_1) \mathbb{P}(u_1 \mid x) - \mathbb{P}(u_1 \mid x)} \right) \\
&= \frac{\mathbb{P}(a \mid x, u_1)}{\mathbb{P}(a \mid x, u_2)} \left(\frac{\mathbb{P}(a \mid x, u_2)  - 1}{\mathbb{P}(a \mid x, u_1) - 1} \right) \\
&= \mathrm{OR}\left(\mathbb{P}(a \mid x, u_1), \mathbb{P}(a \mid x, u_2)\right)
\end{align}

Hence, $\max\left\{\sup_{u_1, u_2 \in \mathcal{U}} \rho(x, u_1, u_2, a), \, \sup_{u_1, u_2 \in \mathcal{U}} \rho(x, u_1, u_2, a)^{-1} \right\} \leq \Gamma$ is equivalent to 
\begin{equation}
    \frac{1}{\Gamma} \leq \mathrm{OR}\left(\mathbb{P}(a \mid x, u_1), \mathbb{P}(a \mid x, u_2)\right)\leq \Gamma
\end{equation}
for all $u_1, u_2 \in \mathcal{U}$, which reduces to the original definition of Rosenbaum's sensitivity model for $a=1$.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:transformation}}

\begin{proof}
We show transformation-invariance separately for all three sensitivity models (MSM, $f$-sensitivity models, and Rosenbaum's sensitivity model).

\textbf{MSM:} Let $\mathcal{D}_{x, a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a)) = \Gamma$, which implies that implies $\frac{1}{\Gamma} \leq \rho(x, u, a) \leq \Gamma$ for all $u \in \mathcal{U}$. By rearranging terms, we obtain
\begin{equation}\label{eq:proof_lem2_msm_upper}
\mathbb{P}(u \mid x)  \leq \left( \Gamma (1 - \mathbb{P}(a \mid x)) + \mathbb{P}(a \mid x) \right) \, \mathbb{P}(u \mid x, a)
\end{equation}
and
\begin{equation}\label{eq:proof_lem2_msm_lower}
\mathbb{P}(u \mid x)  \geq \left( \frac{1}{\Gamma} (1 - \mathbb{P}(a \mid x)) + \mathbb{P}(a \mid x) \right) \, \mathbb{P}(u \mid x, a).
\end{equation}
Let $t \colon \mathcal{U} \to \widetilde{\mathcal{U}}$ be a transformation of the unobserved confounder. By using Eq.~\eqref{eq:proof_lem2_msm_upper} we can write
\begin{align}
\rho(x, t(u), a) &=\frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{\mathbb{P}(t(u) \mid x)}{\mathbb{P}(t(u) \mid x, a)} - \mathbb{P}(a \mid x) \right) \\
&= \frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{ \int \delta(t(u) - t(u^\prime))\mathbb{P}(u^\prime \mid x) \diff u^\prime}{\int \delta(t(u) - t(u^\prime))\mathbb{P}(u^\prime \mid x, a) \diff u^\prime} - \mathbb{P}(a \mid x) \right) \\
& \leq \frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{ \int \delta(t(u) - t(u^\prime))\mathbb{P}(u^\prime \mid x, a) \diff u^\prime}{\int \delta(t(u) - t(u^\prime))\mathbb{P}(u^\prime \mid x, a) \diff u^\prime} \right) 
 \, \Gamma \\
&= \Gamma
\end{align}
for all $u \in \mathcal{U}$. Similarly, we can use Eq.~\eqref{eq:proof_lem2_msm_lower} to obtain
\begin{align}
\rho(x, t(u), a)
&= \frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{ \int \delta(t(u) - t(u^\prime))\mathbb{P}(u^\prime \mid x) \diff u^\prime}{\int \delta(t(u) - t(u^\prime))\mathbb{P}(u^\prime \mid x, a) \diff u^\prime} - \mathbb{P}(a \mid x) \right) \\
& \geq \frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{ \int \delta(t(u) - t(u^\prime))\mathbb{P}(u^\prime \mid x, a) \diff u^\prime}{\int \delta(t(u) - t(u^\prime))\mathbb{P}(u^\prime \mid x, a) \diff u^\prime} \right) 
 \, \frac{1}{\Gamma} \\
&= \frac{1}{\Gamma}
\end{align}
for all $u \in \mathcal{U}$. Hence, 
\begin{equation}
\mathcal{D}_{x,a}(\mathbb{P}(t(U) \mid x), \mathbb{P}(t(U) \mid x, a)) = \max\left\{\sup_{u \in \mathcal{U}} \rho(x, t(u), a), \, \sup_{u \in \mathcal{U}} \rho(x, t(u), a)^{-1} \right\} \leq \Gamma.
\end{equation}

\textbf{f-sensitivity models:} This follows from the data compression theorem for $f$-divergences. We refer to \citet{Polyanskiy.2022} for details.

\textbf{Rosenbaum's sensitivity model}: We begin by rewriting
\begin{align}
    \rho(x, u_1, u_2, a) &= \frac{\mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x, a) \mathbb{P}(a \mid x) - \mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x)}{\mathbb{P}(u_1 \mid x, a) \mathbb{P}(u_2 \mid x, a) \mathbb{P}(a \mid x) - \mathbb{P}(u_2 \mid x, a) \mathbb{P}(u_1 \mid x)}\\
    &= \left(\frac{1}{\frac{\mathbb{P}(u_1 \mid x)}{\mathbb{P}(u_1 \mid x, a)} - \mathbb{P}(a \mid x)} \right) \left( \frac{\mathbb{P}(u_2 \mid x)}{\mathbb{P}(u_2 \mid x, a)} - \mathbb{P}(a \mid x)\right)
\end{align}
as a function of density ratios on $\mathcal{U}$. Let now $\mathcal{D}_{x, a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a)) = \Gamma$. This implies

\begin{equation}\label{eq:proof_lem2_rb_upper}
 \mathbb{P}(u_1 \mid x) \leq \left(\Gamma \left( \frac{ \mathbb{P}(u_2 \mid x)}{ \mathbb{P}(u_2 \mid x, a)}  - \mathbb{P}(a \mid x)\right) + \mathbb{P}(a \mid x)\right) \mathbb{P}(u_1 \mid x, a)  
\end{equation}
and 
\begin{equation}\label{eq:proof_lem2_rb_lower}
 \mathbb{P}(u_1 \mid x) \geq \left(\frac{1}{\Gamma} \left( \frac{ \mathbb{P}(u_2 \mid x)}{ \mathbb{P}(u_2 \mid x, a)}  - \mathbb{P}(a \mid x)\right) + \mathbb{P}(a \mid x)\right) \mathbb{P}(u_1 \mid x, a)
\end{equation}
for all $u_1, u_2 \in \mathcal{U}$. Let $t \colon \mathcal{U} \to \widetilde{\mathcal{U}}$ be a transformation. By using Eq.~\eqref{eq:proof_lem2_rb_upper} and Eq.~\eqref{eq:proof_lem2_rb_lower} we obtain
\begin{align}
\rho(x, t(u_1), t(u_1), a)
&= \left( \frac{1}{\frac{\int \delta(t(u_1) - t(u^\prime_1))\mathbb{P}(u^\prime_1 \mid x) \diff u^\prime_1}{\int \delta(t(u_1) - t(u^\prime_1))\mathbb{P}(u^\prime_1 \mid x, a) \diff u^\prime_1} - \mathbb{P}(a \mid x)} \right) \\
& \quad \quad \left( \frac{\int \delta(t(u_2) - t(u^\prime_2))\mathbb{P}(u^\prime_2 \mid x) \diff u^\prime_2}{\int \delta(t(u_2) - t(u^\prime_2))\mathbb{P}(u^\prime_2 \mid x, a) \diff u^\prime_2} - \mathbb{P}(a \mid x)\right) \\
&\leq \left( \frac{1}{\frac{1}{\Gamma} \left(\frac{\mathbb{P}(u_2 \mid x)}{\mathbb{P}(u_2 \mid x, a) } - \mathbb{P}(a \mid x) \right)} \right) \Gamma \left( \frac{\mathbb{P}(u_1 \mid x)}{\mathbb{P}(u_1 \mid x, a)} - \mathbb{P}(a \mid x) \right) \\
 &= \frac{\Gamma^2}{\rho(x, u_1, u_1, a)}
\end{align}
for all $u_1, u_2 \in \mathcal{U}$. Hence, 
\begin{equation}
\inf_{u_1, u_2} \rho(x, t(u_1), t(u_1), a) \leq \Gamma.     
\end{equation}
By using analogous arguments, we can also show that 
\begin{equation}
\sup_{u_1, u_2} \rho(x, t(u_1), t(u_1), a) \leq \Gamma,     
\end{equation}
which implies
\begin{equation}
    \mathcal{D}_{x,a}(\mathbb{P}(t(U) \mid x), \mathbb{P}(t(U) \mid x, a)) \leq \Gamma.
\end{equation}
\end{proof}

\subsection{Proof of Theorem~\ref{thrm:main}}

\begin{proof}
Without loss of generality, we provide a proof for the upper bound $Q^+_\mathcal{M}(x, a)$. Our arguments work analogously for the lower bound $Q^-_\mathcal{M}(x, a)$. Furthermore, we only show the inequality 
\begin{equation}
    Q^+_\mathcal{M}(x, a) \leq \sup_{\mathbb{P}^\ast \in \mathcal{P}^\ast} Q(x, a, \mathbb{P}^\ast),
\end{equation}
because the other direction (``$\geq$'') holds by definition of $Q^+_\mathcal{M}(x, a)$. Hence, it is enough to show the existence of a sequence of full distributions $(\mathbb{P}_\ell^\ast)_{\ell \in \N}$ with $\mathbb{P}_\ell^\ast \in \mathcal{P}^\ast$ for all $\ell \in \N$ that satisfies $\lim_{{\ell \to \infty}} Q(x, a, \mathbb{P}_\ell^\ast) = Q^+_\mathcal{M}(x, a)$.

To do so, we proceed in three steps: In step 1, we construct a sequence $(\mathbb{P}_\ell^\ast)_{\ell \in \N}$ of full distributions that induce $\mathbb{P}_\ell^\ast(U \mid x, a) = \mathbb{P}^\ast(U \mid x, a)$ and $\mathbb{P}_\ell^\ast(Y \mid x, u, a) = \mathbb{P}^\ast(Y \mid x, u, a) = \delta(Y - f^\ast_{x, a}(u))$ for every $\ell \in \N$. In step 2, we show compatibility with the sensitivity model, i.e., $\mathbb{P}_\ell^\ast \in \mathcal{M}$ for all $\ell \in \N$. Finally, in step 3 we show that $\lim_{{\ell \to \infty}} Q(x, a, \mathbb{P}_\ell^\ast) = Q^+_\mathcal{M}(x, a)$.

\textbf{Step 1:} Let $\widetilde{\mathbb{P}}$ be a full distribution on $\mathcal{X} \times \widetilde{\mathcal{U}} \times \mathcal{A} \times \mathcal{Y}$ for some latent space $\widetilde{\mathcal{U}}$ that is the solution to Eq.~\eqref{eq:partial_identification}. By definition, there exists a sequence $(\widetilde{\mathbb{P}}_\ell)_{\ell \in \N}$ with $\widetilde{\mathbb{P}}_\ell \in \mathcal{M}$ and
$\lim_{{\ell \to \infty}} Q(x, a, \widetilde{\mathbb{P}}_\ell) = Q^+_\mathcal{M}(x, a)$. Let $\widetilde{\mathbb{P}}_\ell(\widetilde{U} \mid x)$ and $\widetilde{\mathbb{P}}_\ell(Y \mid x, u, a)$ be corresponding induced distributions (for fixed $x$, $a$). Without loss of generality, we can assume that $\widetilde{\mathbb{P}}_\ell$ is induced by a structural causal model \citep{Pearl.2009}, so that we can write the conditional outcome distribution with a (not necessarily invertible) functional assignment $Y = \widetilde{f}_{X, A, \ell}(U)$ as a point distribution $\widetilde{\mathbb{P}}_\ell(Y \mid x, u, a) = \delta(Y - \widetilde{f}_{x, a, l}(u))$. Note that we do not explicitly consider exogenous noise because we can always consider this part of the latent space $\widetilde{\mathcal{U}}$. By Eq.~\eqref{eq:int_dist} and Eq.~\eqref{eq:obs_dist} we can write the observed conditional outcome distribution as
\begin{equation}\label{eq:proof_tilde_obs}
\mathbb{P}_\mathrm{obs}(Y \mid x, a) = \widetilde{\mathbb{P}}_\ell(\widetilde{f}_{x, a, \ell}(\widetilde{U}) \mid x, a),  
\end{equation}
and the potential outcome distribution conditioned on $x$ as 
\begin{equation}
\widetilde{\mathbb{P}}_\ell(Y(a) \mid x) = \widetilde{\mathbb{P}}_\ell(\widetilde{f}_{x, a, \ell}(\widetilde{U}) \mid x).
\end{equation}

We now define the sequence $(\mathbb{P}_\ell^\ast)_{\ell \in \N}$. First we define a distribution on $\mathcal{U} \subseteq \R^{d_y}$ via
\begin{equation}\label{eq:proof_defp}
\mathbb{P}_\ell^\ast(U \mid x) = \widetilde{\mathbb{P}}_\ell({f^\ast_{x, a}}^{-1} \left(Y(a) \right)\mid x) = \widetilde{\mathbb{P}}_\ell({f^\ast_{x, a}}^{-1} \left(\widetilde{f}_{x, a, \ell}(\widetilde{U}) \right)\mid x).
\end{equation}

We then define full probability distribution $\mathbb{P}_\ell^\ast$ for the fixed $x$ and $a$ and all $u\in \mathcal{U}$, $y \in \mathcal{Y}$ as
\begin{equation}
    \mathbb{P}_\ell^\ast(x, u, a, y) = \delta \left(f^\ast_{x, a}(u) - y \right) \mathbb{P}^\ast(u \mid x, a) \mathbb{P}_\mathrm{obs}(x, a).
\end{equation}
Finally, we can choose a family of distributions $(\mathbb{P}_\ell^\ast(U \mid x, a^\prime))_{a^\prime \neq a}$ so that $\mathbb{P}_\ell^\ast(U \mid x) = \int \mathbb{P}_\ell^\ast(U \mid x, a) \mathbb{P}_\mathrm{obs}(a \mid x) \diff a$ and define
\begin{equation}
    \mathbb{P}_\ell^\ast(x, u, a^\prime, y) = \delta\left(f^\ast_{x, a^\prime}(u) - y\right) \mathbb{P}_\ell^\ast(u \mid x, a^\prime) \mathbb{P}_\mathrm{obs}(x, a^\prime).
\end{equation}

By definition, $\mathbb{P}_\ell^\ast$ induces the fixed components $\mathbb{P}^\ast(U \mid x, a)$ and $\mathbb{P}^\ast(Y \mid x, u, a) = \delta(Y - f^\ast_{x, a}(u))$, as well as $\mathbb{P}_\ell^\ast(U \mid x)$ from Eq.~\eqref{eq:proof_defp} and the observational data distribution $\mathbb{P}_\mathrm{obs}(X, A, Y)$.

\textbf{Step 2:} We now show that $\mathbb{P}_\ell^\ast$ respects the sensitivity constraints, i.e., satisfies $\mathbb{P}_\ell^\ast \in \mathcal{M}$. It holds that
\begin{align}\label{eq:proof_transform_xa}
\mathbb{P}_\ell^\ast(U \mid x, a) &= {\mathbb{P}}_\ell^\ast({f^\ast_{x, a}}^{-1} \left(f^\ast_{x, a}(U) \right)\mid x, a) \\ & \overset{(1)}{=}{\mathbb{P}}_\mathrm{obs}({f^\ast_{x, a}}^{-1} \left(Y\right)\mid x, a) \\ & \overset{(2)}{=} \widetilde{\mathbb{P}}_\ell({f^\ast_{x, a}}^{-1} \left(\widetilde{f}_{x, a, \ell}(\widetilde{U}) \right)\mid x, a),
\end{align}
where (1) holds due to the data-compatibility assumption on $f^\ast_{x, a}$ and (2) holds due to Eq.~\eqref{eq:proof_tilde_obs}.

We now define a transformation $t \colon \widetilde{\mathcal{U}} \to \mathcal{U}$ via $t = {f^\ast_{x, a}}^{-1} \circ \widetilde{f}_{x, a, \ell}$. We obtain
\begin{align}
  \mathcal{D}_{x, a}(\mathbb{P}_\ell^\ast(U \mid x), \mathbb{P}_\ell^\ast(U \mid x, a)) &\overset{(1)}{=} \mathcal{D}_{x, a}(\widetilde{\mathbb{P}}_\ell(t(\widetilde{U}) \mid x), \widetilde{\mathbb{P}}_\ell^\ast(t(\widetilde{U}) \mid x, a)) \\ & \overset{(2)}{\leq} \mathcal{D}_{x, a}(\widetilde{\mathbb{P}}_\ell(\widetilde{U} \mid x), \widetilde{\mathbb{P}}_\ell^\ast(\widetilde{U} \mid x, a))\\ & \overset{(3)}{\leq} \Gamma,
\end{align}
where (1) holds due to Eq.~\eqref{eq:proof_defp} and Eq.~\eqref{eq:proof_transform_xa}, (2) holds due to the tranformation-invariance property of $\mathcal{M}$, and (3) holds because $\widetilde{\mathbb{P}}_\ell \in \mathcal{M}$.
Hence, $\mathbb{P}_\ell^\ast \in \mathcal{M}$.

\textbf{Step 3:} We show now that $\lim_{{\ell \to \infty}} Q(x, a, \mathbb{P}_\ell^\ast) = Q^+_\mathcal{M}(x, a)$, which completes our proof.
By Eq.~\eqref{eq:proof_defp} it holds that
\begin{equation}
\mathbb{P}^\ast_\ell(Y(a) \mid x) = \mathbb{P}_\ell^\ast(f^\ast_{x, a}(U) \mid x)  = \widetilde{\mathbb{P}}_\ell(Y(a) \mid x),
\end{equation}
which means that potential outcome distributions conditioned on $x$ coincide for $\mathbb{P}^\ast_\ell$ and $\widetilde{\mathbb{P}}_\ell$. It follows that
\begin{equation}
    Q(x, a, \mathbb{P}_\ell^\ast) = \mathcal{F}\left( \mathbb{P}^\ast_\ell(Y(a) \mid x)\right) = \mathcal{F}\left( \widetilde{\mathbb{P}}_\ell(Y(a) \mid x)\right) \xrightarrow[\ell \to \infty]{}  Q^+_\mathcal{M}(x, a).
\end{equation}

\end{proof}


\clearpage

\section{Further sensitivity models}\label{app:sensitivity}

In the following, we list additional sensitivity models that can be written as GTSMs, and thus can be used with \frameworkname.

\textbf{Continuous marginal sensitivity model (CMSM)}: The CMSM has been proposed by \citet{Jesson.2022} and \citet{Bonvini.2022}. It is defined via
\begin{equation}
\frac{1}{\Gamma} \leq  \frac{\mathbb{P}(a\mid x)}{\mathbb{P}(a \mid x, u)} \leq \Gamma
\end{equation}
for all $x \in \mathcal{X}$, $u \in \mathcal{U}$, and $a \in \mathcal{A}$. The CMSM can be written as a CMSM with sensitivity parameter $\Gamma$ by defining 
\begin{equation}
\mathcal{D}_{x,a}(\mathbb{P}(u \mid x), \mathbb{P}(u \mid x, a)) = \max\left\{\sup_{u \in \mathcal{U}} \frac{\mathbb{P}(u \mid x)}{\mathbb{P}(u \mid x, a)}, \, \sup_{u \in \mathcal{U}} \frac{\mathbb{P}(u \mid x, a)}{\mathbb{P}(u \mid x)} \right\}.
\end{equation}
This directly follows by applying Bayes' Theorem to $\mathbb{P}(u \mid x, a) = \frac{\mathbb{P}(a \mid x, u) \mathbb{P}(u \mid x)}{\mathbb{P}(a \mid x)}$.

\textbf{Continuous $f$-sensitivity models}: Motivated by the CMSM, we can define $f$-sensitivity models for continuous treatments via
\begin{equation}
    \max\left\{ \int_{\mathcal{U}} f\left(\frac{\mathbb{P}(a\mid x)}{\mathbb{P}(a \mid x, u)}\right) \mathbb{P}(u \mid x, a) \diff u, \,  \int_{\mathcal{U}} f\left(\frac{\mathbb{P}(a\mid x, u)}{\mathbb{P}(a \mid x)}\right) \mathbb{P}(u \mid x, a) \diff u\right\} \leq \Gamma
\end{equation}
for all $x \in \mathcal{X}$ and $a \in \mathcal{A}$.
By using Bayes' Theorem, we can write any continuous $f$-sensitivity model as a GTSM by defining
\begin{align}
\mathcal{D}_{x, a}(\mathbb{P}(u \mid x), \mathbb{P}(u \mid x, a)) &= \max\left\{\int_{\mathcal{U}} f\left(\frac{\mathbb{P}(u \mid x)}{\mathbb{P}(u \mid x, a)}\right) \mathbb{P}(u \mid x, a) \diff u, 
 \right. \\ & \quad \quad \quad \quad \left.\int_{\mathcal{U}} f\left(\frac{\mathbb{P}(u \mid x, a)}{\mathbb{P}(u \mid x)}\right) \mathbb{P}(u \mid x, a) \diff u\right\}.
\end{align}

\textbf{Weighted marginal sensitivity models:} \citet{Frauen.2023c} proposed a weighted version of the MSM, defined via
\begin{equation}
    \frac{1}{(1 - \Gamma) q(x, a) + \Gamma} \leq \frac{\mathbb{P}(u \mid x, a)}{\mathbb{P}(u \mid x)}  \leq \frac{1}{(1 - \Gamma^{-1}) q(x, a) + \Gamma^{-1}},
\end{equation}
where $q(x, a)$ is a weighting function that incorporates domain knowledge about the strength of unobserved confounding. By using similar arguments than in the proof of Lemma~\ref{lem:gtsm}, we can write the weighted MSM as a GTSM by defining
\begin{equation}
\mathcal{D}_{x,a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a)) = \max\left\{\sup_{u \in \mathcal{U}} \rho(x, u, a), \, \sup_{u \in \mathcal{U}} \rho(x, u, a)^{-1} \right\},
\end{equation}
where 
\begin{equation}
\rho(x, u, a) = \frac{1}{1 - q(x, a)} \left(\frac{\mathbb{P}(u \mid x)}{\mathbb{P}(u \mid x, a)} - q(x, a) \right).    
\end{equation}

\clearpage

\section{Query averages and differences}\label{app:avg_diff}

Here, we show that we can use our bounds $Q^+_\mathcal{M}(x, a)$ and $Q^-_\mathcal{M}(x, a)$ to obtain sharp bounds for averages and differences of causal queries. We follow established literature on causal sensitivity analysis \citep{Dorn.2022, Dorn.2022b, Frauen.2023c}.

\textbf{Averages:} We are interested in the sharp upper bound for the \emph{average causal query}
\begin{equation}
\Bar{Q}_\mathcal{M}(a, \mathbb{P}) = \int_\mathcal{X} Q(x, a, \mathbb{P}) \, \mathbb{P}_\mathrm{obs}(x) \diff x.
\end{equation}
An example is the average potential outcome $\E[Y(a)]$, which can be obtained by averaging conditional potential outcomes via $\E[Y(a)] = \int \E(Y(a) \mid x) \, \mathbb{P}_\mathrm{obs}(x) \diff x$. We can obtain upper bounds via
\begin{equation}\label{eq:bounds_avg}
\Bar{Q}^+_\mathcal{M}(a) = \sup_{\mathbb{P} \in \mathcal{M}} \int_\mathcal{X} Q(x, a, \mathbb{P}) \, \mathbb{P}_\mathrm{obs}(x) \diff x = \int_\mathcal{X} Q^+_\mathcal{M}(x, a) \, \mathbb{P}_\mathrm{obs}(x) \diff x\quad,
\end{equation}
and lower bounds via
\begin{equation}
 \Bar{Q}^-_\mathcal{M}(a) = \inf_{\mathbb{P} \in \mathcal{M}} \int_\mathcal{X} Q(x, a, \mathbb{P}) \, \mathbb{P}_\mathrm{obs}(x) \diff x  =  \int_\mathcal{X} Q^-_\mathcal{M}(x, a) \, \mathbb{P}_\mathrm{obs}(x) \diff x,
\end{equation}
whenever we can interchange the supremum/ infimum and the integral. That is, bounding the averaged causal query $\Bar{Q}_\mathcal{M}(a, \mathbb{P})$ reduces to averaging the bounds $Q^+_\mathcal{M}(x, a)$ and $Q^-_\mathcal{M}(x, a)$.

\textbf{Differences:} For two different treatment values $a_1, a_2 \in \mathcal{A}$, we are interested in the \emph{difference of causal queries}
\begin{equation}
    Q(x, a_1, \mathbb{P}) - Q(x, a_2, \mathbb{P}).
\end{equation}
An example is the conditional average treatment effect $\E[Y(1) \mid x] - \E[Y(0) \mid x]$. We can obtain an upper bound via
\begin{align}\label{eq:bounds_diff}
Q^+_\mathcal{M}(x, a_1, a_2) &=\sup_{\mathbb{P} \in \mathcal{M}} \left(  Q(x, a_1, \mathbb{P}) - Q(x, a_2, \mathbb{P}) \right) \\
& \leq \sup_{\mathbb{P} \in \mathcal{M}} Q(x, a_1, \mathbb{P})  \inf_{\mathbb{P} \in \mathcal{M}} Q(x, a_2, \mathbb{P}) \\
&= Q^+_\mathcal{M}(x, a_1)  - Q^-_\mathcal{M}(x, a_2).
\end{align}
Similarly, a lower bound is given by
\begin{equation}
Q^-_\mathcal{M}(x, a_1, a_2) \geq  Q^-_\mathcal{M}(x, a_1)  - Q^+_\mathcal{M}(x, a_2).
\end{equation}
It has been shown that these bounds are even sharp for some sensitivity models such as the MSM, i.e., attain equality \citep{Dorn.2022}.


\clearpage

\section{\frameworkname training details }\label{app:learning_alg}

In this section, we provide details regarding the training of \frameworkname, in particular, the Monte-Carlo estimates for Stage~2 and the full learning algorithm.

\subsection{Monte-Carlo estimates of stage 2 losses and sensitivity constraints}

In the following, we assume that we obtained samples $\widetilde{u} = (\widetilde{u}^{(j)}_{x, a})_{j=1}^k \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0_{d_y}, I_{d_y})$ and $\xi = (\xi^{(j)}_{x, a})_{j=1}^k \overset{\text{i.i.d.}}{\sim} \mathrm{Bernoulli}(\mathbb{P}_\mathrm{obs}(a \mid x))$.  

\subsubsection{Stage 2 losses}

Here, we provide our estimators $\hat{\mathcal{L}}_2(\eta, \widetilde{u}, \xi)$ of the Stage~2 loss $\mathcal{L}_2(\eta)$. We consider three different causal queries: (i)~Expectations, (ii)~Set probabilities, and (iii)~Quantiles.

\textbf{Expectations:} Expectations correspond to setting $\mathcal{F}(\mathbb{P}) = \mathbb{E}[X]$. Then, we can estimate our Stage~2 loss via the empirical mean, i.e.,
\begin{equation}
    \hat{\mathcal{L}}_2(\eta, \widetilde{u}, \xi) = \frac{1}{k}  \sum_{i=1}^n \sum_{j=1}^k f^\ast_{g^\ast_{\theta_\mathrm{opt}}(x_i, a_i)}\left( (1 - \xi_{x_i, a_i}^{(j)}) \widetilde{f}_{\widetilde{g}_\eta(x_i, a_i)}(\widetilde{u}_{x_i, a_i}^{(j)}) +  \xi_{x_i, a_i}^{(j)} \widetilde{u}_{x_i, a_i}^{(j)}\right).
\end{equation}

\textbf{Set probabilities:} Here we consider queries of the form $\mathcal{F}(\mathbb{P}) = \mathbb{P}(X \in \mathcal{S})$ for some set $S \subseteq \mathcal{Y}$. We first define the log-likelihood
\begin{align}
\ell\left(\eta, \widetilde{u}_{x_i, a_i}^{(j)}, \xi_{x_i, a_i}^{(j)}\right) &= \mathbb{P} \left({f^\ast_{g^\ast_{\theta_\mathrm{opt}}(x_i, a_i)}}\left( (1 - \xi_{x_i, a_i}) \widetilde{f}_{\widetilde{g}_\eta(x_i, a_i)}(\widetilde{U}) +  \xi_{x_i, a_i} \widetilde{U}\right) \right. = \\ 
& \quad \quad \quad \left. {f^\ast_{g^\ast_{\theta_\mathrm{opt}}(x_i, a_i)}}\left((1 - \xi_{x_i, a_i}^{(j)}) \widetilde{f}_{\widetilde{g}_\eta(x_i, a_i)}(\widetilde{u}_{x_i, a_i}^{(j)}) +  \xi_{x_i, a_i}^{(j)} \widetilde{u}_{x_i, a_i}^{(j)} \right)\right),   
\end{align}
which corresponds to the log-likelihood of the shifted distribution (under stage 2) at the point that is obtained from plugging the Monte-Carlo samples $\widetilde{u}_{x_i, a_i}^{(j)}$ and $\xi_{x_i, a_i}^{(j)}$ into the CNFs. We then optimize Stage~2 by maximizing this log-likelihood only at points in $\mathcal{S}$. That is, the corresponding Monte-Carlo estimator of the Stage~2 loss is
\begin{align}\label{eq_app:monte_carlo_set}
    \hat{\mathcal{L}}_2(\eta, \widetilde{u}, \xi) &= \sum_{i=1}^n \sum_{j=1}^k \ell\left(\eta, \widetilde{u}_{x_i, a_i}^{(j)}, \xi_{x_i, a_i}^{(j)}\right) \\
    & \quad \mathbbm{1} \left\{{f^\ast_{g^\ast_{\theta_\mathrm{opt}}(x_i, a_i)}}\left((1 - \xi_{x_i, a_i}^{(j)}) \widetilde{f}_{\widetilde{g}_\eta(x_i, a_i)}(\widetilde{u}_{x_i, a_i}^{(j)}) +  \xi_{x_i, a_i}^{(j)} \widetilde{u}_{x_i, a_i}^{(j)}\right) \in \mathcal{S} \right\}
\end{align}
Here, we only backpropagate through the log-likelihood in order to obtain informative (non-zero) gradients.

\textbf{Quantiles:} We consider quantiles of the form $\mathcal{F}(\mathbb{P}) = F_X^{-1}(q)$, where $F_X$ is the c.d.f. corresponding to $\mathbb{P}$ and $q \in (0, 1)$. For this, we can use the same Stage~2 loss as in Eq.~\eqref{eq_app:monte_carlo_set} by defining the set $\mathcal{S} = \left\{y \in \mathcal{Y} \mid y \leq \hat{F}_{ij}^{-1}(q)\right\}$ where $\hat{F}_{ij}^{-1}$ is empirical c.d.f. corresponding to $\left\{ {f^\ast_{g^\ast_{\theta_\mathrm{opt}}(x_i, a_i)}}\left((1 - \xi_{x_i, a_i}^{(j)}) \widetilde{f}_{\widetilde{g}_\eta(x_i, a_i)}(\widetilde{u}_{x_i, a_i}^{(j)}) +  \xi_{x_i, a_i}^{(j)} \widetilde{u}_{x_i, a_i}^{(j)} \right)\right\}_{j=1}^k$.

\subsubsection{Sensitivity constraints}

Here, we provide our estimators $\hat{\mathcal{D}}_{x, a}(\eta, \widetilde{u})$ of the sensitivity constraint $\mathcal{D}_{x, a}(\mathbb{P}(U \mid x), \mathbb{P}(U \mid x, a))$. We consider the three sensitivity models from the main paper: (i)~MSM, (ii)~$f$-sensitivity models, and (iii)~Rosenbaum's sensitivity model.

\textbf{MSM:} We define
\begin{equation}
    \hat{\rho}(x, u, a, \eta) = \frac{1}{1 - \mathbb{P}(a \mid x)} \left(\frac{ \mathbb{P} \left((1 - \xi_{x, a}) \widetilde{f}_{\widetilde{g}_\eta(x, a)}(\widetilde{U}) +  \xi_{x, a} \widetilde{U} = u \right)}{\mathbb{P}(\widetilde{U} = u)} - \mathbb{P}(a \mid x) \right).
\end{equation}
Then, our estimator for the MSM constraint is
\begin{equation}
   \hat{\mathcal{D}}_{x, a}(\eta, \widetilde{u}) =  \max\left\{\max_{u \in \widetilde{u}} \hat{\rho}(x, u, a, \eta), \, \max_{u \in \widetilde{u}} \hat{\rho}(x, u, a, \eta)^{-1} \right\}.
\end{equation}

\textbf{$f$-sensitivity models:} Our estimator for the $f$-sensitivity constraint is
\begin{equation}
\hat{\mathcal{D}}_{x, a}(\eta, \widetilde{u}) = \max\left\{\frac{1}{k}\sum_{j=1}^k f(\hat{\rho}(x, \widetilde{u}^{(j)}_{x, a}, a, \eta)),  \, \frac{1}{k}\sum_{j=1}^k f(\hat{\rho}(x, \widetilde{u}^{(j)}_{x, a}, a, \eta)^{-1}) \right\}.
\end{equation}

\textbf{Rosenbaum's sensitivity model:} We define
\begin{align}
    \hat{\rho}(x, u_1, u_2, a, \eta) &= \frac{\mathbb{P}(\widetilde{U} = u_1)}{\mathbb{P}(\widetilde{U} = u_2)}\left(\frac{\mathbb{P}(\widetilde{U} = u_2) \mathbb{P}(a \mid x) - \mathbb{P} \left((1 - \xi_{x, a}) \widetilde{f}_{\widetilde{g}_\eta(x, a)}(\widetilde{U}) +  \xi_{x, a} \widetilde{U} = u_2 \right)}{\mathbb{P}(\widetilde{U} = u_1) \mathbb{P}(a \mid x) - \mathbb{P} \left((1 - \xi_{x, a}) \widetilde{f}_{\widetilde{g}_\eta(x, a)}(\widetilde{U}) +  \xi_{x, a} \widetilde{U} = u_1 \right)} \right).
\end{align}
Then, our estimator is
\begin{equation}
   \hat{\mathcal{D}}_{x, a}(\eta, \widetilde{u}) =  \max\left\{\max_{u_1, u_2 \in \widetilde{u}} \hat{\rho}(x, u_1, u_2, a, \eta), \, \max_{u_1, u_2 \in \widetilde{u}} \hat{\rho}(x, u_1, u_2, a, \eta)^{-1} \right\}.
\end{equation}


\subsection{Full learning algorithm}

Our full learning algorithm for Stage~1 and Stage~2 is shown in Algorithm~\ref{alg:learning_full}. For Stage~2, we use our Monte-Carlo estimators described in the previous section in combination with the augmented lagrangian method to incorporate the sensitivity constraints. For details regarding the augmented lagrangian method, we refer to \citet{Nocedal.2006}, chapter 17.

\textbf{Reusability:} Using CNFs instead of unconditional normalizing flows allows us to compute bounds $Q^+_\mathcal{M}(x, a)$ and $Q^-_\mathcal{M}(x, a)$ without the need to retrain our model for different $x \in \mathcal{X}$ and $a \in \mathcal{A}$. In particular, we can simultaneously compute bounds for averaged queries or differences \emph{without} retraining (see Appendix~\ref{app:avg_diff}). Furthermore, Stage~1 is independent of the sensitivity model, which means that we can reuse our fitted Stage~1 CNF for different sensitivity models and sensitivity parameters $\Gamma$, and only need to retrain the Stage~2 CNF. 


\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Full learning algorithm for \frameworkname}
\label{alg:learning_full}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{~causal query $Q(x, a)$, GTSM $\mathcal{M}$, and obs. dataset $\mathcal{D}$ $\mathbb{P}_\mathrm{obs}$ \\ epoch numbers $n_0$, $n_1$, $n_2$; batch size $n_b$; learning rates $\gamma_0$, $\gamma_1$, and $\alpha > 1$ }
\Output{learned parameters $\theta_\mathrm{opt}$ and $\eta_\mathrm{opt}$ of Stage~1 and Stage~2}
\tcp{Stage 1}
$\mathbb{P}^\ast(U \mid x, a) \gets$ fixed probability distribution on $\mathcal{U} \subseteq \R^{d_u = d_y}$\;
Initialize $\theta^{(1)}$ and $U \sim \mathcal{N}(0_{d_y}, I_{d_y})$\;
\For{$i \in \{1, \dots, n_0\}$}{
$(x, a, y) \gets$ batch of size $n_\mathrm{b}$\;
$\mathcal{L}_1(\theta) \gets  \sum_{(x, a, y)} \log \mathbb{P} ({f^\ast_{g^\ast_\theta(x, a)}}(U) = y)$\;
$\theta^{(i+1)} \gets$ optimization step of $\mathcal{L}_1(\theta^{(i)})$ w.r.t. $\theta^{(i)}$ with learning rate $\gamma_0$\;
}
$\theta_\mathrm{opt} \gets \theta^{(n_0)}$\;
\tcp{Stage 2}
Initialize $\eta^{(1)}$, $\lambda^{(1)}$, and $\mu^{(1)}$\;
\For{$i \in \{1, \dots, n_1\}$}{
\For{$\ell \in \{1, \dots, n_2\}$}{
$\eta^{(i)}_{1} \gets \eta^{(i)}$\;
$(x, a) \gets$ batch of size $n_\mathrm{b}$\;
$\widetilde{u} \gets (\widetilde{u}^{(j)}_{x, a})_{j=1}^k \overset{i.i.d.}{\sim} \mathcal{N}(0_{d_y}, I_{d_y})$\;
$\xi \gets(\xi^{(j)}_{x, a})_{j=1}^k \overset{i.i.d.}{\sim} \mathrm{Bernoulli}(\mathbb{P}_\mathrm{obs}(a \mid x))$\;
$s_{x, a}(\eta) \gets \Gamma - \hat{\mathcal{D}}_{x, a}(\eta, \widetilde{u})$\;
$\mathcal{L}(\eta, \lambda, \mu) \gets \hat{\mathcal{L}}_2(\eta, \widetilde{u}, \xi) - \sum_{(x, a)} \lambda_{x, a} s_{x, a}(\eta) + \frac{\mu}{2} s_{x, a}^2(\eta) $\;
$\eta^{(i)}_{\ell+1} \gets$ optimization step of $\mathcal{L}(\eta^{(i)}_{\ell}, \lambda^{(i)}, \mu^{(i)})$ w.r.t. $\eta^{(i)}_{\ell}$ with learning rate $\gamma_1$\;
}
$\eta^{(i+1)} \gets \eta^{(i)}_{n_2}$\;
$\lambda^{(i+1)}_{x, a} \gets \max\left\{0, \lambda^{(i)}_{x, a} - \mu^{(i)} s_{x, a}(\eta^{(i+1)})\right\}$\;
$\mu^{(i+1)} \gets \alpha \mu^{(i)}$
}
$\eta_\mathrm{opt} \gets \eta^{(n_1)}$\;
\end{algorithm}


\clearpage

\section{Implementation and hyperparameter tuning details}\label{app:implementation}

\textbf{Stage 1 CNF}: We use a conditional normalizing flows (CNF) \cite{Winkler.2019} for stage 1 of \frameworkname. Normalizing flows (NFs) model a distribution $\mathbb{P}(Y)$ of a target variable $Y$ by transforming a simple base distribution $\mathbb{P}(U)$ (e.g., standard normal) of a latent variable $U$ through an invertible transformation $Y = f_{\hat{\theta}}(U)$, where $\hat{\theta}$ denotes parameters \cite{Rezende.2015}. In order to estimate the  \emph{conditional} distributions $\mathbb{P}(Y \mid x, a)$, CNFs define the parameters $\hat{\theta}$ as an output of a \emph{hyper network} $\hat{\theta} = g_\theta(x, a)$ with learnable parameters $\theta$. The conditional log-likelihood can be written analytically as
\begin{equation}
 \log \left( \mathbb{P} (f_{g_\theta(x, a)}(U) = y) \right) 
 \overset{(\ast)}{=} \log \left(\mathbb{P} \left(U = f_{g_\theta(x, a)}^{-1}(y)\right) \right) + \log\left(\mathrm{det} \left( \frac{\mathrm{d}}{\mathrm{d} y} f_{g_\theta(x, a)}^{-1}(y)\right)\right),
\end{equation}
where $(\ast)$ follows from the change-of-variables theorem for invertible transformations.

\textbf{Stage 2 CNF}: As in stage 1, we use a CNF that transforms $U = \widetilde{f}_{\hat{\eta}}(\widetilde{U})$, where $\hat{\eta} = \widetilde{g}_\eta(x, a)$ with learnable parameters $\eta$. The conditional log-likelihood can be expressed analytically via
\begin{equation}
 \log \left( \mathbb{P} (\widetilde{f}_{\widetilde{g}_\theta(x, a)}(\widetilde{U}) = u) \right) = \log \left(\mathbb{P} \left(\widetilde{U} = \widetilde{f}_{\widetilde{g}_\theta(x, a)}^{-1}(u)\right) \right) + \log\left(\mathrm{det} \left( \frac{\mathrm{d}}{\mathrm{d} u} \widetilde{f}_{\widetilde{g}_\theta(x, a)}^{-1}(u)\right)\right).   
\end{equation}

In our implementation, we use autoregressive neural spline flows. That is, we model the invertible transformation $f_{\theta}$ via a spline flow as described in \cite{Dolatabadi.2020}. We use an autoregressive neural network for the hypernetwork $g_\eta(\mathbf{x}, \mathbf{m}, \mathbf{a})$ with 2 hidden layers, ReLU activation functions, and linear output. For training, we use the Adam optimizer \cite{Kingma.2015}.

\textbf{Propensity scores:} The estimation of the propensity scores $\mathbb{P}(a \mid \mathbf{x})$ is a standard binary classification problem. We use feed-forward neural networks with 3 hidden layers, ReLU activation functions, and softmax output. For training, we minimize the standard cross-entropy loss by using the Adam optimizer \cite{Kingma.2015}.

\textbf{Hyperparameter tuning:}
We perform hyperparameter tuning for our propensity score models and stage 1 CNFs. The tunable parameters and search ranges are shown in Table~\ref{tab:hyper}. Then, we use the same optimally trained propensity score models and stage 1 CNFs networks for all stage 2 models and closed-form solutions (in Fig.~\ref{fig:simulation_msm}). For the stage 2 CNFs, we choose hyperparameters that lead to a stable convergence of Alg.~\ref{alg:learning_stage2}, while ensuring that the sensitivity constraints are satisfied. For reproducibility purposes, we report the selected hyperparameters as \emph{.yaml} files.\footnote{Code is available at \href{https://anonymous.4open.science/r/NeuralCSA-DE7D}{https://anonymous.4open.science/r/NeuralCSA-DE7D}.}

\begin{table}[h]
\caption{Hyperparameter tuning details.}
\centering
\label{tab:hyper}
\footnotesize
\begin{tabular}{lll}
\toprule
\textsc{Model} & \textsc{Tunable parameters} & \textsc{Search range} \\
\midrule
Stage 1 CNF &Epochs & $50$ \\
&Batch size &$32$, $64$, $128$  \\
 & Learning rate & $0.0005$, $0.001$, $0.005$\\
 & Hidden layer size (hyper network) & $5$, $10$, $20$, $30$\\
  & Number of spline bins & $2$, $4$, $8$ \\
 
\midrule
Propensity network & Epochs & $30$ \\
&Batch size &$32$, $64$, $128$  \\
& Learning rate & $0.0005$, $0.001$, $0.005$ \\
 & Hidden layer size  & $5$, $10$, $20$, $30$ \\
  & Dropout probability & $0$, $0.1$\\
\bottomrule
\end{tabular}
\end{table}

\clearpage

\section{Details regarding datasets and experiments}\label{app:details_exp}

We provide details regarding the datasets we use in our experimental evaluation in Sec.~\ref{sec:experiments}.

\subsection{Synthetic data}

\textbf{Binary treatment:} We simulate an observed confounder $X \sim \mathrm{Uniform}[-1, 1]$ and define the observed propensity score as $\pi(x) = \mathbb{P}_\mathrm{obs}(A=1 \mid x) = 0.25 + 0.5 \, \sigma(3x)$, where $\sigma(\cdot)$ denotes the sigmoid function. Then, we simulate an unobserved confounder
\begin{equation}\label{eq:app_sim_datacomp}
U \mid X = x \sim \textrm{Bernoulli}\left(p=\frac{(\Gamma - 1) \pi(x) + 1}{\Gamma + 1} \right) 
\end{equation}
and a binary treatment
\begin{equation}
A= 1 \mid X = x, U = u \sim \textrm{Bernoulli}\left(p=u \pi(x) s^{+}(x, a) + (1 - u) \pi(x) s^{-}(x, a) \right),
\end{equation}
where $s^{+}(x, a) = \frac{1}{(1 - \Gamma^{-1}) \pi(x) + \Gamma^{-1}}$ and $s^{-}(x, a) = \frac{1}{(1 - \Gamma) \pi(x) + \Gamma}$.

Finally, we simulate a continuous outcome 
\begin{equation}
Y = (2  A - 1) X + (2 A - 1) - 2  \sin(2  (2  A - 1)  X) - 2  (2  U - 1)  (1 + 0.5  X) + \varepsilon,
\end{equation}
where $\varepsilon \sim \mathcal{N}(0, 1)$.

The data-generating process is constructed so that $\frac{\mathbb{P}(A = 1 \mid x, u)}{\pi(x)} = u s^{+}(x, a) + (1-u)s^{-}(x, a)$ or equivalent that $\mathrm{OR}(x, u) = u \Gamma + (1 - u) \Gamma^{-1}$. Hence, the full distribution follows an MSM with sensitivity parameter $\Gamma$. Furthermore, by Eq.~\eqref{eq:app_sim_datacomp} we have 
\begin{equation}
  \mathbb{P}(A = 1 \mid x, 1) \mathbb{P}(U = 1 \mid x)  +  \mathbb{P}(A = 1 \mid x, 0)  \mathbb{P}(U = 0 \mid x) = \pi(x) = \mathbb{P}_\mathrm{obs}(A=1 \mid x)
\end{equation}
so that $\mathbb{P}$ induces $\mathbb{P}_\mathrm{obs}(A=1 \mid x)$.

\textbf{Continuous treatment:} We simulate an observed confounder $X \sim \mathrm{Uniform}[-1, 1]$, and independently a binary unobserved confounder $U \sim \textrm{Bernoulli}(p=0.5)$. Then, we simulate a continuous treatment via
\begin{equation}
     A \mid X = x, U = u \sim \mathrm{Beta}(\alpha, \beta) \text{ with } \alpha = \beta = 2 + x + \gamma (u - 0.5),
\end{equation}
where $\gamma$ is a parameter that controls the strength of unobserved confounding. In our experiments, we chose $\gamma = 2$. Finally, we simulate an outcome via
\begin{equation}
    Y = A + X  \exp(-X A) - 0.5  (U - 0.5)  X + (0.5  X + 1) + \varepsilon,
\end{equation}
where $\varepsilon \sim \mathcal{N}(0, 1)$. Note that the data-generating process does \emph{not} follow a (continuous) MSM. 

\textbf{Oracle sensitivity parameters $\Gamma^\ast$:} We can obtain Oracle sensitivity parameters $\Gamma^\ast(x, a)$ for each sample with $X=x$ and $A=a$ by simulating from our previously defined data-generating process to estimate the densities $\mathbb{P}(u \mid x, a)$ and $\mathbb{P}(u \mid x)$ and subsequently solve for $\Gamma^\ast(x, a)$ in the GTSM equations from Lemma~\ref{lem:app_gtsm}. By definition, $\Gamma^\ast(x, a)$ is the smallest sensitivity parameter such that the corresponding sensitivity model is guaranteed to produce bounds that cover the ground-truth causal query. For binary treatments, it holds $\Gamma^\ast(x, a) = \Gamma^\ast$, i.e., the oracle sensitivity parameter does not depend on $x$ and $a$. For continuous treatments, we choose $\Gamma^\ast = \Gamma^\ast(a) = \int \Gamma^\ast(x, a) \mathbb{P}(x) \diff x$ in Fig.~\ref{fig:simulation_f}.

\textbf{Semi-synthetic data:} We obtain covariates $X$ and treatments $A$ from MIMIC-III as described in the paragraph regarding real-world data below. Then we learn the observed propensity score $\hat{\pi}(x) = \hat{\mathbb{P}}(A = 1 \mid x)$ using a feed-forward neural network with three hidden layers and ReLU activation function. We simulate a uniform unobserved confounder $U \sim \mathcal{U}[0, 1]$. Then, we define a weight
\begin{align}
    w(X, U) &= \mathbbm{1}\left\{\gamma \geq 2 - \frac{1}{\hat{\pi}(X)} \right\} \left( \gamma + 2 U (1 - \gamma)\right) + \\ & \quad \quad \mathbbm{1}\left\{\gamma < 2 -  \frac{1}{\hat{\pi}(X)} \right\} \left(2 - \frac{1}{\hat{\pi}(X)} +  2 U \left(\frac{1}{\hat{\pi}(X)} - 1\right)\right)
\end{align}
and simulate synthetic binary treatments via
\begin{equation}
A= 1 \mid X = x, U = u \sim \textrm{Bernoulli}\left(p=w(x,u) \hat{\pi}(x)  \right).   
\end{equation}
Here, $\gamma$ is a parameter controlling the strength of unobserved confounding, which we set to $0.25$. The data-generating process is constructed in a way such that the full propensity $\mathbb{P}(A= 1 \mid X = x, U = u)$ induces the (estimated) observed propensity $\hat{\pi}$ from the real-world data. We then simulate synthetic outcomes via
\begin{equation}
    Y = (2 A - 1) \left(\frac{1}{d_x + 1} \left(\left(\sum_{i=1}^{d_x} X_i \right) + U\right) \right) + \varepsilon,
\end{equation}
where $\varepsilon \sim \mathcal{N}(0, 0.1)$.
In our experiments, we use \% of the data for training and validation, and \% of the data for evaluating test performance. From our test set, we filter out all samples that satisfy either $\mathbb{P}(A=1 \mid x) < 0.3$  or $\mathbb{P}(A=1 \mid x) > 0.7$. This is because these samples are associated with large empirical uncertainty (low sample sizes). In our experiments, we only demonstrate the validity of our \frameworkname bounds in settings with low empirical uncertainty.

\textbf{Oracle sensitivity parameters $\Gamma^\ast$:} Similar to our fully synthetic experiments, we first obtain the Oracle sensitivity parameters $\Gamma^\ast(x, a)$ for each test sample with confounders $x$ and treatment $a$. We then take the median overall $\Gamma^\ast(x, a)$ of the test sample. By definition, \frameworkname should then cover at least 50\% of all test query values (see Table~\ref{tab:semi_synthetic}).  

\textbf{Real-world data:} We use the MIMIC-III dataset \cite{Johnson.2016}, which includes electronic health records from patients admitted to intensive care units. We use a preprocessing pipeline \cite{Wang.2020} to extract patient trajectories with 8 hourly recorded patient characteristics (heart rate, sodium blood pressure, glucose, hematocrit, respiratory rate, age, gender) and a binary treatment indicator (mechanical ventilation). We then sample random time points for each patient trajectory and define the covariates $X \in \R^8$ as the past patient characteristics averaged over the previous 10 hours. Our treatment $A \in \{0, 1\}$ is an indicator of whether mechanical ventilation was done in the subsequent 10-hour time. Finally, we consider the final heart rate and blood pressure averaged over 5 hours as outcomes. After removing patients with missing values and outliers (defined by covariate values smaller than the corresponding 0.1th percentile or larger than the corresponding 99.9th percentile), we obtain a dataset of size $n=14719$ patients. We split the data into train (80\%), val (10\%), and test (10\%).

\clearpage

\section{Additional experiments}\label{app:more_exp}

\subsection{Additional treatment combinations for synthetic data}

Here we provide results for additional treatment values $a \in \{0.1, 0.9\}$ for the synthetic experiments with continuous treatment in Sec.~\ref{sec:experiments}. Fig.~\ref{fig:app_treat} shows the results for our experiment where we compare the \frameworkname bounds under the MSM with (optimal) closed-form solutions. Fig.~\ref{fig:app_treat_f} shows the results of our experiment where we compare the bounds of different sensitivity models. The results are consistent with our observations from the main paper and show the validity of the bounds obtained by \frameworkname,

\begin{figure}[ht]
 \centering
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_continuous_1D_a01.pdf}
\end{subfigure}%
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_continuous_1D_a09.pdf}
\end{subfigure}
\caption{Validating the correctness of \frameworkname (ours) by comparing with optimal closed-form solutions (CF) for the MSM in the synthetic continuous treatment setting. \emph{Left:} $a=0.1$. \emph{Right:} $a=0.9$. Reported: mean $\pm$ standard deviation over 5 runs.}
\label{fig:app_treat}
\end{figure}

\begin{figure}[ht]
 \centering
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_continuous_1D_f_a01.pdf}
\end{subfigure}%
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_continuous_1D_f_a09.pdf}
\end{subfigure}
\caption{Confirming the validity of \frameworkname bounds for various sensitivity models in the synthetic continuous treatment setting. \emph{Left:} $a=0.1$. \emph{Right:} $a=0.9$. Reported: mean $\pm$ standard deviation over 5 runs.}
\label{fig:app_treat_f}
\end{figure}

\subsection{Densities for lower bounds on real-world data}

Here we provide the distribution shifts for our real-world case study (Sec.~\ref{sec:experiments}), but for the lower bounds instead of the upper. The results are shown in Fig.~\ref{fig:app_real}. In contrast to the shifts for the upper bounds, increasing $\Gamma$ leads to a distribution shift \emph{away} from the direction of the danger area, i.e., high heart rate and blood pressure. 

\begin{figure}[ht]
 \centering
\begin{subfigure}{0.30\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_real_stage1.pdf}
\end{subfigure}%
\begin{subfigure}{0.30\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_real_stage2_lower_gamma2.pdf}
\end{subfigure}
\begin{subfigure}{0.30\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_real_stage2_lower_gamma4.pdf}
\end{subfigure}
\caption{Contour plots of 2D densities obtained by \frameworkname under an MSM. Here, we aim to learn a lower bound of the causal query $\mathbb{P}(Y_1(1) \geq 115, Y_2(1) \geq 90 \mid X = x_0)$ for a test patient $x_0$. \emph{Left:} Stage 1/ observational distribution. \emph{Middle:} Stage 2, $\Gamma = 2$. \emph{Right:} Stage 2, $\Gamma = 4$.}
\label{fig:app_real}
\vspace{-0.4cm}
\end{figure}

\clearpage

\section{Discussion on limitations and future work}\label{app:discussion}

\textbf{Limitations:} \frameworkname is a versatile framework that can approximate the bounds of causal effects in various settings. However, there are a few settings where (optimal) closed-form bounds exist (e.g., CATE for binary treatments under the MSM), which should be preferred when available. Instead, \frameworkname offers a unified framework for causal sensitivity analysis under various sensitivity models, treatment types, and causal queries, and can be applied in settings where closed-form solutions have not been derived or do not exist (Table~\ref{tab:rw}).

\textbf{Future work:} Our research hints at the broad applicability of \frameworkname beyond the three sensitivity models that we discussed above (see also Appendix~\ref{app:sensitivity}). For future work, it would be interesting to conduct a comprehensive comparison of sensitivity models and provide practical recommendations for their usage. Future work may further consider incorporating techniques from semiparametric statistical theory in order to obtain estimation guarantees, robustness properties, and confidence intervals. Finally, we only provided identifiability results that hold in the limit of infinite data. It would be interesting to provide rigorous empirical uncertainty quantification for \frameworkname, e.g., via a Bayesian approach. While in principle the bootstrapping approach from \citep{Jesson.2022} could be applied in our setting, this could be computationally infeasible for large datasets.


\end{document}
